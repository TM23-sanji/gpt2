{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"94a01212bd544d73a12671c6c8d9356a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d54426a55d944573aad832b4e1783815","IPY_MODEL_4a937d9c91e84fc098eda43a36609f9a","IPY_MODEL_6a7e7662587e49e6855027811c5017a4"],"layout":"IPY_MODEL_4d202abc354c4436847a92412cf1d718"}},"d54426a55d944573aad832b4e1783815":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2bb85598eafb40618c5046bad7f586f4","placeholder":"​","style":"IPY_MODEL_535ac665ba81433791f9136fb7f56fea","value":"README.md: "}},"4a937d9c91e84fc098eda43a36609f9a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0e15b15f2c049b888f84d310fa330a8","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d85f1b4970d441d6a2d0c6c2dcb39c14","value":1}},"6a7e7662587e49e6855027811c5017a4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_65517edb617a436babdf73a0ed32f8bc","placeholder":"​","style":"IPY_MODEL_fa05c01170b94541a7f701e710e96093","value":" 44.3k/? [00:00&lt;00:00, 4.20MB/s]"}},"4d202abc354c4436847a92412cf1d718":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2bb85598eafb40618c5046bad7f586f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"535ac665ba81433791f9136fb7f56fea":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c0e15b15f2c049b888f84d310fa330a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"d85f1b4970d441d6a2d0c6c2dcb39c14":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"65517edb617a436babdf73a0ed32f8bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa05c01170b94541a7f701e710e96093":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d6b32663fa644b038fbd29dadb8dce16":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d6da71c134844efba11e3abe0ee157d4","IPY_MODEL_ef39fd50d8054b26a912cbb3e77866b6","IPY_MODEL_2a3686c94db24023bd9ea054df9f00a9"],"layout":"IPY_MODEL_0529533f85974ebeb89b9ea4d9c6f2c6"}},"d6da71c134844efba11e3abe0ee157d4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd9ebe1a76fe4c06b810f56cafe2dabe","placeholder":"​","style":"IPY_MODEL_15a67395f476467a9c707ac0c1355192","value":"Resolving data files: 100%"}},"ef39fd50d8054b26a912cbb3e77866b6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_14c66ff0e8c243beb0923dfedea610b8","max":27468,"min":0,"orientation":"horizontal","style":"IPY_MODEL_81202529d822402496b73c078fe53904","value":27468}},"2a3686c94db24023bd9ea054df9f00a9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_44e680b963f44c6598b67b134bd03bcb","placeholder":"​","style":"IPY_MODEL_0c2af3ed5f9745b2a878fafd055de98a","value":" 27468/27468 [00:02&lt;00:00, 11247.70it/s]"}},"0529533f85974ebeb89b9ea4d9c6f2c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd9ebe1a76fe4c06b810f56cafe2dabe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15a67395f476467a9c707ac0c1355192":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"14c66ff0e8c243beb0923dfedea610b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81202529d822402496b73c078fe53904":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"44e680b963f44c6598b67b134bd03bcb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c2af3ed5f9745b2a878fafd055de98a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0f7324186c6249b3badf9b5eb287bc71":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ffc9791af8644fc69c9f802469b400d7","IPY_MODEL_ab85091d34454135801f9d648cca6dea","IPY_MODEL_e578317531a24046914f220ed8cccfef"],"layout":"IPY_MODEL_0f1b4f99b0564a4eb85a19f87ce1ae62"}},"ffc9791af8644fc69c9f802469b400d7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9ca47321b334a5db059aa4d7a29090e","placeholder":"​","style":"IPY_MODEL_036bc17617cd4186bc8d0d5c8ab5207f","value":"Resolving data files: 100%"}},"ab85091d34454135801f9d648cca6dea":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3828b15119ca4570b54cdfaefe6a5f59","max":27468,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b18e588ad72d4655bffcd88125bb7caa","value":27468}},"e578317531a24046914f220ed8cccfef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0d9670c6e7574eba92f2fbf96b2eb3a3","placeholder":"​","style":"IPY_MODEL_91b01c66c9b94a12bbe054dc3503398e","value":" 27468/27468 [00:00&lt;00:00, 323992.50it/s]"}},"0f1b4f99b0564a4eb85a19f87ce1ae62":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9ca47321b334a5db059aa4d7a29090e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"036bc17617cd4186bc8d0d5c8ab5207f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3828b15119ca4570b54cdfaefe6a5f59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b18e588ad72d4655bffcd88125bb7caa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0d9670c6e7574eba92f2fbf96b2eb3a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91b01c66c9b94a12bbe054dc3503398e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nwandb_token = user_secrets.get_secret(\"wandb\")\n\nimport os\nos.environ[\"HF_TOKEN\"] = hf_token\nos.environ[\"WANDB_API_KEY\"] = wandb_token\nprint(\"HF token loaded—rate limit bypassed!\")  # Optional confirm","metadata":{"id":"dzNdkipKsiVJ","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:41:11.281265Z","iopub.execute_input":"2025-11-13T08:41:11.281548Z","iopub.status.idle":"2025-11-13T08:41:11.754484Z","shell.execute_reply.started":"2025-11-13T08:41:11.281524Z","shell.execute_reply":"2025-11-13T08:41:11.753717Z"}},"outputs":[{"name":"stdout","text":"HF token loaded—rate limit bypassed!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!uv pip install -q wandb transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:41:11.755274Z","iopub.execute_input":"2025-11-13T08:41:11.755552Z","iopub.status.idle":"2025-11-13T08:41:13.055894Z","shell.execute_reply.started":"2025-11-13T08:41:11.755527Z","shell.execute_reply":"2025-11-13T08:41:13.054908Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch, random, os, math, time, wandb\nimport numpy as np\n# import tiktoken\n# from datatrove.pipeline.readers import ParquetReader\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\n# from itertools import cycle\nfrom torch.utils.data import DataLoader, IterableDataset\nimport torch.nn as nn\nimport torch.nn.functional as F  # For scaled_dot_product_attention\nimport torch.optim as optim\nfrom torch.amp import autocast, GradScaler\nimport matplotlib.pyplot as plt\n# from torch.profiler import profile, record_function, ProfilerActivity\n\nwandb.login(key=wandb_token) ","metadata":{"id":"1yrMCzDPstK7","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:41:13.058146Z","iopub.execute_input":"2025-11-13T08:41:13.058384Z","iopub.status.idle":"2025-11-13T08:41:34.501385Z","shell.execute_reply.started":"2025-11-13T08:41:13.058363Z","shell.execute_reply":"2025-11-13T08:41:34.500708Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtusharmishra802\u001b[0m (\u001b[33mtusharmishra802-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('huggyllama/llama-30b')\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:41:34.502249Z","iopub.execute_input":"2025-11-13T08:41:34.502794Z","iopub.status.idle":"2025-11-13T08:41:37.727852Z","shell.execute_reply.started":"2025-11-13T08:41:34.502769Z","shell.execute_reply":"2025-11-13T08:41:37.727244Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/700 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"651bec2c1a514078adaa332114220f4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2527c389e6374319b84bc3e43f85c003"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb83cf6a3d7946fdbc0f325009a073e8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bee9370789524973ba1d8d7584ce7b66"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)","metadata":{"id":"ZjYaNXk3s8v0","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:41:37.728701Z","iopub.execute_input":"2025-11-13T08:41:37.728968Z","iopub.status.idle":"2025-11-13T08:41:37.739572Z","shell.execute_reply.started":"2025-11-13T08:41:37.728946Z","shell.execute_reply":"2025-11-13T08:41:37.738847Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"CFG = {\n    \"seed\": 42,\n    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n\n    # Model\n    \"vocab_size\": 32000,          # llama vocab\n    \"emb_dim\": 128,               # 4096 \n    \"context_length\": 8,          # 4096\n    \"n_heads\": 4,                 # 32\n    \"num_kv_heads\": 2,\n    \"n_layers\": 1,                # 32\n    \"drop_rate\": 0.1,\n    \"qkv_bias\": False,\n    'base': 10000,\n    \"intermediate_size\": int(8/3 * 4096),  # ~11008 for full\n\n    # Data\n    \"max_tokens\": 500_000,        # STOP after this many tokens\n    \"warmup_tokens\": 10_000,      # linear warm-up\n    \"batch_size\": 32,\n    \"shuffle_buffer\": 5_000,\n\n    # Optimiser\n    \"optimizer\": \"adamw\",\n    \"lr\": 3e-4,\n    \"final_lr\": 3e-5,\n    \"weight_decay\": 0.1,\n    \"betas\": (0.9, 0.95),\n\n    # SwiGLU\n    'beta' : 1,\n\n    # Misc\n    \"log_interval\": 20,           # steps\n    \"wandb_project\": \"llama-demo\",\n    \"wandb_run_name\": None,       # auto-generated\n}\n","metadata":{"id":"zF2gSbvhPfeA","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:41:37.740334Z","iopub.execute_input":"2025-11-13T08:41:37.740649Z","iopub.status.idle":"2025-11-13T08:41:37.823511Z","shell.execute_reply.started":"2025-11-13T08:41:37.740630Z","shell.execute_reply":"2025-11-13T08:41:37.822782Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"torch.manual_seed(CFG[\"seed\"])\nif CFG[\"device\"] == \"cuda\":\n    torch.cuda.manual_seed_all(CFG[\"seed\"])\n\n# ------------------- 2. WANDB INIT -------------------\nwandb.init(\n    project=CFG[\"wandb_project\"],\n    name=CFG[\"wandb_run_name\"],\n    config=CFG,\n    mode=\"online\",   # set \"offline\" if you have no internet\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:41:37.824288Z","iopub.execute_input":"2025-11-13T08:41:37.824506Z","iopub.status.idle":"2025-11-13T08:41:45.562146Z","shell.execute_reply.started":"2025-11-13T08:41:37.824489Z","shell.execute_reply":"2025-11-13T08:41:45.561534Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251113_084137-fsbp9mvr</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/tusharmishra802-/llama-demo/runs/fsbp9mvr' target=\"_blank\">swift-leaf-10</a></strong> to <a href='https://wandb.ai/tusharmishra802-/llama-demo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/tusharmishra802-/llama-demo' target=\"_blank\">https://wandb.ai/tusharmishra802-/llama-demo</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/tusharmishra802-/llama-demo/runs/fsbp9mvr' target=\"_blank\">https://wandb.ai/tusharmishra802-/llama-demo/runs/fsbp9mvr</a>"},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/tusharmishra802-/llama-demo/runs/fsbp9mvr?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7ed44559f190>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"class RMSNorm(nn.Module):\n    def __init__(self, emb_dim, eps=1e-6):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(emb_dim))\n\n    def forward(self, x):\n        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n        x_norm = x / rms\n        return x_norm * self.weight\n\nclass SiLU(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return x * F.sigmoid(CFG[\"beta\"] * x)\n\nclass FeedForward(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        intermediate_size = cfg.get(\"intermediate_size\", int(8/3 * cfg[\"emb_dim\"]))\n        self.gate_proj = nn.Linear(cfg[\"emb_dim\"], intermediate_size, bias=False)\n        self.up_proj = nn.Linear(cfg[\"emb_dim\"], intermediate_size, bias=False)\n        self.down_proj = nn.Linear(intermediate_size, cfg[\"emb_dim\"], bias=False)\n        self.act_fn = SiLU()\n\n    def forward(self, x):\n        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n","metadata":{"id":"TBX6i-FuPqYp","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:41:45.562901Z","iopub.execute_input":"2025-11-13T08:41:45.563431Z","iopub.status.idle":"2025-11-13T08:41:45.570664Z","shell.execute_reply.started":"2025-11-13T08:41:45.563402Z","shell.execute_reply":"2025-11-13T08:41:45.569987Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class RotaryEmbeddings(nn.Module):\n    def __init__(self, dim: int, max_position_embeddings: int = 2048, base: int = 10000, device=CFG[\"device\"]):\n        super().__init__()\n        self.dim = dim\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.float32, device=device) / self.dim))\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n        \n        t = torch.arange(self.max_position_embeddings, dtype=torch.float32, device=device)\n        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)  # (T, D/2)\n        self.register_buffer(\"cos_cached\", freqs.cos(), persistent=False)\n        self.register_buffer(\"sin_cached\", freqs.sin(), persistent=False)\n\n    def forward(self, x, seq_len=None):\n        if seq_len is None:\n            seq_len = x.shape[-2]\n        cos = self.cos_cached[:seq_len, ...].unsqueeze(0).unsqueeze(0)  # (1,1,T,D//2)\n        sin = self.sin_cached[:seq_len, ...].unsqueeze(0).unsqueeze(0)\n        \n        x1 = x[..., : self.dim : 2]  # (..., D//2)\n        x2 = x[..., 1 : self.dim : 2]\n        \n        rotated_x1 = x1 * cos - x2 * sin\n        rotated_x2 = x1 * sin + x2 * cos\n        \n        return torch.cat((rotated_x1, rotated_x2), dim=-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:41:45.572950Z","iopub.execute_input":"2025-11-13T08:41:45.573149Z","iopub.status.idle":"2025-11-13T08:41:45.590593Z","shell.execute_reply.started":"2025-11-13T08:41:45.573133Z","shell.execute_reply":"2025-11-13T08:41:45.589918Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"ds = load_dataset(\"HuggingFaceFW/fineweb\", split=\"train\", streaming=True)\n\ndef tokenize_function(examples):\n    texts = examples['text']\n    tokenized = tokenizer(texts, truncation=False, add_special_tokens=False)  # Batched for speed\n    tokenized['input_ids'] = [ids + [tokenizer.eos_token_id] for ids in tokenized['input_ids']]\n    return tokenized\n\nds = ds.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['text'])  # Drops raw text, keeps input_ids\nds = ds.shuffle(buffer_size=CFG['shuffle_buffer'])","metadata":{"id":"FaZbm8Fcs9tu","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:41:45.591198Z","iopub.execute_input":"2025-11-13T08:41:45.591440Z","iopub.status.idle":"2025-11-13T08:42:18.011979Z","shell.execute_reply.started":"2025-11-13T08:41:45.591423Z","shell.execute_reply":"2025-11-13T08:42:18.011384Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"391b64b9da9e49a58755e2dba802ea48"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/27468 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52f0d7caee684a5e8212d8eb432f4433"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/27468 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b996128ae342479ca2f1b5188ff46744"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"class SlidingWindowDataset(IterableDataset):\n    def __init__(self, ds, tokenizer, context_len, stride, target_tokens):\n        self.ds = ds  # HF streaming iterable\n        self.tokenizer = tokenizer\n        self.context_len = context_len\n        self.stride = stride\n        self.max_tokens = target_tokens\n        self.pad_id = tokenizer.pad_token_id\n\n    def __iter__(self):\n        buffer = []\n        token_count = 0\n        for example in self.ds:  # Streams tokenized input_ids\n            toks = example['input_ids']\n            if not toks:\n                continue\n            buffer.extend(toks)\n\n            # Fixed buffer logic\n            while len(buffer) > self.context_len:\n                x = buffer[:self.context_len]\n                y = buffer[1:self.context_len + 1]\n                # Pad if needed (rare post-fix)\n                if len(y) < self.context_len:\n                    y += [self.pad_id] * (self.context_len - len(y))\n                yield {'input_ids': torch.tensor(x, dtype=torch.long),\n                       'labels': torch.tensor(y, dtype=torch.long)}  # Dict for HF Trainer\n                buffer = buffer[self.stride:]\n                token_count += self.context_len\n                if token_count >= self.max_tokens:\n                    return\n\n            # Cap buffer to prevent OOM\n            if len(buffer) > 2 * self.context_len:\n                buffer = buffer[-self.context_len:]\n\n        # Remnant with padding\n        if len(buffer) >= 128:  # Min threshold\n            x = buffer[:self.context_len]\n            y = buffer[1:min(self.context_len + 1, len(buffer) + 1)]\n            if len(y) < self.context_len:\n                y += [self.pad_id] * (self.context_len - len(y))\n            yield {'input_ids': torch.tensor(x, dtype=torch.long),\n                   'labels': torch.tensor(y, dtype=torch.long)}\n\n# Usage\ndataset = SlidingWindowDataset(ds, tokenizer, context_len=CFG['context_length'], stride=CFG[\"context_length\"] // 2, target_tokens=CFG['max_tokens'])\ndataloader = DataLoader(dataset, batch_size=32, num_workers=2, pin_memory=True, prefetch_factor=2, collate_fn=lambda b: {k: torch.stack([d[k] for d in b]) for k in b[0]})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:42:18.012743Z","iopub.execute_input":"2025-11-13T08:42:18.012996Z","iopub.status.idle":"2025-11-13T08:42:18.022618Z","shell.execute_reply.started":"2025-11-13T08:42:18.012971Z","shell.execute_reply":"2025-11-13T08:42:18.021898Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"if torch.cuda.is_available():\n    try:\n        torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=True, enable_math=False)\n        print(\"PyTorch SDP kernels enabled (mem_efficient & not math for speed on T4/P100)!\")\n    except Exception as e:\n        print(f\"Could not enable SDP kernels: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:42:18.023424Z","iopub.execute_input":"2025-11-13T08:42:18.024117Z","iopub.status.idle":"2025-11-13T08:42:18.045508Z","shell.execute_reply.started":"2025-11-13T08:42:18.024092Z","shell.execute_reply":"2025-11-13T08:42:18.044654Z"}},"outputs":[{"name":"stdout","text":"PyTorch SDP kernels enabled (mem_efficient & not math for speed on T4/P100)!\n","output_type":"stream"},{"name":"stderr","text":"/usr/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n  self.gen = func(*args, **kwds)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_in, d_out, dropout, num_heads, qkv_bias=False, device=CFG[\"device\"]):\n        super().__init__()\n        assert d_out % num_heads == 0\n        self.d_out = d_out\n        self.num_heads = num_heads\n        self.head_dim = d_out // num_heads\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.out_proj = nn.Linear(d_out, d_out, bias=False)\n        self.rope = RotaryEmbeddings(self.head_dim, device=device)\n        self.dropout = dropout\n        print(f\"MHA: {num_heads} heads, head_dim={self.head_dim}\")\n\n    def forward(self, x):\n        b, t, _ = x.shape\n        q = self.W_query(x).reshape(b, t, self.num_heads, self.head_dim).transpose(1, 2)\n        k = self.W_key(x).reshape(b, t, self.num_heads, self.head_dim).transpose(1, 2)\n        v = self.W_value(x).reshape(b, t, self.num_heads, self.head_dim).transpose(1, 2)\n        q = self.rope(q)\n        k = self.rope(k)\n        attn = F.scaled_dot_product_attention(q, k, v, dropout_p=self.dropout if self.training else 0.0, is_causal=True)\n        attn = attn.transpose(1, 2).reshape(b, t, self.d_out)\n        return self.out_proj(attn)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3aw80MvytyCg","outputId":"6aaf875e-5eed-4dbd-dd32-7960836c6db9","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:42:18.046295Z","iopub.execute_input":"2025-11-13T08:42:18.046919Z","iopub.status.idle":"2025-11-13T08:42:18.056496Z","shell.execute_reply.started":"2025-11-13T08:42:18.046900Z","shell.execute_reply":"2025-11-13T08:42:18.055753Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"class GroupedQueryAttention(nn.Module):\n    def __init__(self, d_in, d_out, dropout, num_heads, num_kv_heads=None, qkv_bias=False, device=CFG[\"device\"]):\n        super().__init__()\n        assert d_out % num_heads == 0\n        self.d_out = d_out\n        self.num_heads = num_heads\n        self.head_dim = d_out // num_heads\n        self.num_kv_heads = num_kv_heads or num_heads\n        assert self.num_kv_heads <= self.num_heads, \"num_kv_heads must <= num_heads\"\n        assert d_out % self.num_kv_heads == 0 or self.num_kv_heads == num_heads, \"Inconsistent head dims for GQA\"\n        \n        # Projections\n        self.W_query = nn.Linear(d_in, self.num_heads * self.head_dim, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, self.num_kv_heads * self.head_dim, bias=qkv_bias)  # Renamed for clarity\n        self.W_value = nn.Linear(d_in, self.num_kv_heads * self.head_dim, bias=qkv_bias)\n        self.out_proj = nn.Linear(self.num_heads * self.head_dim, d_out, bias=False)\n        \n        # RoPE with device\n        self.rope = RotaryEmbeddings(self.head_dim, device=device)\n        self.dropout = dropout\n\n    def forward(self, x):\n        b, t, d_in = x.shape\n        \n        # Project Q/K/V\n        q = self.W_query(x).reshape(b, t, self.num_heads, self.head_dim).transpose(1, 2)\n        k = self.W_key(x).reshape(b, t, self.num_kv_heads, self.head_dim).transpose(1, 2)\n        v = self.W_value(x).reshape(b, t, self.num_kv_heads, self.head_dim).transpose(1, 2)\n\n        # GQA Repeat\n        if self.num_kv_heads != self.num_heads:\n            repeat_factor = self.num_heads // self.num_kv_heads\n            k = k.repeat_interleave(repeat_factor, dim=1)\n            v = v.repeat_interleave(repeat_factor, dim=1)\n\n        q = self.rope(q)\n        k = self.rope(k)\n\n        # SDPA (use full k/v seq_len for mask/attn)\n        attn_output = F.scaled_dot_product_attention(\n            q, k, v, dropout_p=self.dropout if self.training else 0.0, is_causal=True, attn_mask=None\n        )\n        \n        # Merge\n        attn_output = attn_output.transpose(1, 2).contiguous().reshape(b, t, self.d_out)\n        output = self.out_proj(attn_output)\n                \n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:42:18.057141Z","iopub.execute_input":"2025-11-13T08:42:18.057365Z","iopub.status.idle":"2025-11-13T08:42:18.072127Z","shell.execute_reply.started":"2025-11-13T08:42:18.057346Z","shell.execute_reply":"2025-11-13T08:42:18.071380Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self, cfg, device=CFG[\"device\"]):\n        super().__init__()\n        self.att = GroupedQueryAttention(cfg[\"emb_dim\"], cfg[\"emb_dim\"], cfg[\"drop_rate\"], cfg[\"n_heads\"], cfg['num_kv_heads'], cfg[\"qkv_bias\"], device)\n        self.ff = FeedForward(cfg)\n        self.norm1 = RMSNorm(cfg[\"emb_dim\"])\n        self.norm2 = RMSNorm(cfg[\"emb_dim\"])\n\n    def forward(self, x):\n        x = x + self.att(self.norm1(x))\n        x = x + self.ff(self.norm2(x))\n        return x","metadata":{"id":"_iCWz1QSNMBP","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:42:18.072919Z","iopub.execute_input":"2025-11-13T08:42:18.073158Z","iopub.status.idle":"2025-11-13T08:42:18.089884Z","shell.execute_reply.started":"2025-11-13T08:42:18.073137Z","shell.execute_reply":"2025-11-13T08:42:18.089068Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class LlamaModel(nn.Module):\n    def __init__(self, cfg, device=CFG[\"device\"]):\n        super().__init__()\n        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg, device) for _ in range(cfg[\"n_layers\"])])\n        self.final_norm = RMSNorm(cfg[\"emb_dim\"])\n        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n        self.out_head.weight = self.tok_emb.weight\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx):\n        x = self.tok_emb(idx)\n        x = self.trf_blocks(x)\n        x = self.final_norm(x)\n        return self.out_head(x)","metadata":{"id":"zpDx0t1QP00N","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:42:18.090600Z","iopub.execute_input":"2025-11-13T08:42:18.090788Z","iopub.status.idle":"2025-11-13T08:42:18.104897Z","shell.execute_reply.started":"2025-11-13T08:42:18.090775Z","shell.execute_reply":"2025-11-13T08:42:18.104185Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"model_cfg = {\n    \"vocab_size\": CFG[\"vocab_size\"],\n    \"context_length\": CFG[\"context_length\"],\n    \"emb_dim\": CFG[\"emb_dim\"],\n    \"n_heads\": CFG[\"n_heads\"],\n    \"num_kv_heads\": CFG[\"num_kv_heads\"],\n    \"n_layers\": CFG[\"n_layers\"],\n    \"drop_rate\": CFG[\"drop_rate\"],\n    \"qkv_bias\": CFG[\"qkv_bias\"],\n}\nmodel = LlamaModel(model_cfg, device=CFG[\"device\"])  # Pass device here!\n\n# 2. Move to GPU + FP16\nmodel = model.to(CFG[\"device\"]).half()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mxa4M1AYP4fH","outputId":"077af1ed-bbb6-45c3-9af3-1a6f1a7fadf9","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:42:18.105537Z","iopub.execute_input":"2025-11-13T08:42:18.105767Z","iopub.status.idle":"2025-11-13T08:42:18.691787Z","shell.execute_reply.started":"2025-11-13T08:42:18.105748Z","shell.execute_reply":"2025-11-13T08:42:18.690975Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def count_params(model):\n    return sum(p.numel() for p in model.parameters())\n\nprint(f\"Total params: {count_params(model):,}\")\nprint(\"Data equal:\", torch.equal(model.out_head.weight, model.tok_emb.weight))\n\nmodel.tok_emb.weight.data[0, 0] = 999.0  # Modify embedding\nprint(\"Shared? out_head[0,0] after change:\", model.out_head.weight.data[0, 0])  # Should be 999.0\nprint(\"Total params after mod:\", count_params(model))  # Still ~124M—no extra","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UZHVivGMQOia","outputId":"309a609b-5957-4a8f-bb63-1ba73525ec23","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:42:18.692677Z","iopub.execute_input":"2025-11-13T08:42:18.692945Z","iopub.status.idle":"2025-11-13T08:42:18.929368Z","shell.execute_reply.started":"2025-11-13T08:42:18.692921Z","shell.execute_reply":"2025-11-13T08:42:18.928629Z"}},"outputs":[{"name":"stdout","text":"Total params: 4,276,480\nData equal: True\nShared? out_head[0,0] after change: tensor(999., device='cuda:0', dtype=torch.float16)\nTotal params after mod: 4276480\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"# ------------------- 7. OPTIMISER + SCALER -------------------\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=CFG[\"lr\"],\n    betas=CFG[\"betas\"],\n    weight_decay=CFG[\"weight_decay\"],\n    fused=True,                          # works on P100\n)\n\nscaler = GradScaler()\n\n# ------------------- 8. LR SCHEDULER -------------------\ntotal_train_tokens = CFG[\"max_tokens\"]\nwarmup_tokens      = CFG[\"warmup_tokens\"]\nbase_lr            = CFG[\"lr\"]\nfinal_lr           = CFG[\"final_lr\"]\n\ndef lr_lambda(tokens_seen):\n    if tokens_seen <= warmup_tokens:\n        return tokens_seen / max(1, warmup_tokens)               # linear warm-up\n    progress = (tokens_seen - warmup_tokens) / max(1, total_train_tokens - warmup_tokens)\n    cosine = 0.5 * (1.0 + math.cos(math.pi * min(1.0, progress)))\n    return (final_lr / base_lr) + (1.0 - final_lr / base_lr) * cosine\n\nscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\nprint(f\"LR Scheduler: Warmup {warmup_tokens:,} → Cosine decay to {final_lr:.1e}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":651},"id":"EIpJL94CQVNP","outputId":"52b805a7-53da-439f-8ede-36c5ff8dfc6b","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:42:18.930256Z","iopub.execute_input":"2025-11-13T08:42:18.930536Z","iopub.status.idle":"2025-11-13T08:42:18.939945Z","shell.execute_reply.started":"2025-11-13T08:42:18.930507Z","shell.execute_reply":"2025-11-13T08:42:18.939220Z"}},"outputs":[{"name":"stdout","text":"LR Scheduler: Warmup 10,000 → Cosine decay to 3.0e-05\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"def validate(model, loader):\n    model.eval()\n    total_loss = 0.0\n    total_tokens = 0\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(CFG[\"device\"], non_blocking=True) for k, v in batch.items()}\n            with autocast(device_type=CFG['device'], dtype=torch.bfloat16, enabled=True):\n                logits = model(batch[\"input_ids\"])\n                loss = criterion(\n                    logits.view(-1, logits.size(-1)),\n                    batch[\"labels\"].view(-1),\n                )\n            num_tokens = (batch[\"labels\"] != -100).sum().item()\n            total_loss += loss.item() * num_tokens\n            total_tokens += num_tokens\n    model.train()\n    return total_loss / total_tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:42:18.940777Z","iopub.execute_input":"2025-11-13T08:42:18.941031Z","iopub.status.idle":"2025-11-13T08:42:18.955392Z","shell.execute_reply.started":"2025-11-13T08:42:18.941010Z","shell.execute_reply":"2025-11-13T08:42:18.954641Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"# ------------------- 9. TRAINING LOOP -------------------\ncriterion = nn.CrossEntropyLoss(ignore_index=-100)\n\ntokens_seen = 0\nstep = 0\nstart_time = time.time()\n\nprint(\"Starting training …\")\nfor batch in dataloader:\n    step += 1\n    batch = {k: v.to(CFG[\"device\"], non_blocking=True) for k, v in batch.items()}\n\n    # ---- forward + loss -------------------------------------------------\n    with autocast(device_type=CFG[\"device\"], dtype=torch.float16):\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(\n            logits.view(-1, logits.size(-1)),\n            batch[\"labels\"].view(-1),\n        )\n\n    # ---- backward -------------------------------------------------------\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n    optimizer.zero_grad(set_to_none=True)\n\n    # ---- LR step (per token, not per step) -------------------------------\n    tokens_seen += batch[\"input_ids\"].numel()\n    scheduler.step()                     # LambdaLR uses the *current* token count\n\n    # ---- logging ---------------------------------------------------------\n    if step % CFG[\"log_interval\"] == 0:\n        elapsed = time.time() - start_time\n        tokens_per_sec = tokens_seen / elapsed\n        lr = optimizer.param_groups[0][\"lr\"]\n\n        wandb.log({\n            \"step\": step,\n            \"loss\": loss.item(),\n            \"lr\": lr,\n            \"tokens_seen\": tokens_seen,\n            \"tokens_per_sec\": tokens_per_sec,\n            \"gpu_mem_gb\": torch.cuda.max_memory_allocated() / 1e9,\n        }, step=step)\n        print(\n            f\"Step {step:5d} | \"\n            f\"Loss {loss.item():.4f} | \"\n            f\"LR {lr:.2e} | \"\n            f\"Tokens {tokens_seen:,}/{CFG['max_tokens']:,} | \"\n            f\"Speed {tokens_per_sec:,.0f} t/s\"\n        )\n\n    # ---- early stop ------------------------------------------------------\n    if tokens_seen >= CFG[\"max_tokens\"]:\n        print(\"\\nReached target token count → stopping.\")\n        break\n\n# --------------------------------------------------------------\nwandb.finish()\nprint(\"Training finished!\")\n# --------------------------------------------------------------","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":297},"id":"hJqw86DNhlDd","outputId":"4108863f-c484-458d-8a14-807051d30a2c","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:42:18.956274Z","iopub.execute_input":"2025-11-13T08:42:18.956503Z","iopub.status.idle":"2025-11-13T08:42:48.969179Z","shell.execute_reply.started":"2025-11-13T08:42:18.956478Z","shell.execute_reply":"2025-11-13T08:42:48.968380Z"}},"outputs":[{"name":"stdout","text":"Starting training …\n","output_type":"stream"},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (3260 > 2048). Running this sequence through the model will result in indexing errors\nToken indices sequence length is longer than the specified maximum sequence length for this model (3669 > 2048). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"Step    20 | Loss 544.7821 | LR 6.00e-07 | Tokens 5,120/500,000 | Speed 369 t/s\nStep    40 | Loss 431.9211 | LR 1.20e-06 | Tokens 10,240/500,000 | Speed 727 t/s\nStep    60 | Loss 448.5125 | LR 1.80e-06 | Tokens 15,360/500,000 | Speed 1,077 t/s\nStep    80 | Loss 461.1272 | LR 2.40e-06 | Tokens 20,480/500,000 | Speed 1,420 t/s\nStep   100 | Loss 492.6755 | LR 3.00e-06 | Tokens 25,600/500,000 | Speed 1,757 t/s\nStep   120 | Loss 335.7589 | LR 3.60e-06 | Tokens 30,720/500,000 | Speed 2,088 t/s\nStep   140 | Loss 356.7100 | LR 4.20e-06 | Tokens 35,840/500,000 | Speed 2,409 t/s\nStep   160 | Loss 364.8120 | LR 4.80e-06 | Tokens 40,960/500,000 | Speed 2,717 t/s\nStep   180 | Loss 224.7365 | LR 5.40e-06 | Tokens 46,080/500,000 | Speed 3,022 t/s\nStep   200 | Loss 416.1475 | LR 6.00e-06 | Tokens 51,200/500,000 | Speed 3,327 t/s\nStep   220 | Loss 306.1766 | LR 6.60e-06 | Tokens 56,320/500,000 | Speed 3,628 t/s\nStep   240 | Loss 312.0487 | LR 7.20e-06 | Tokens 61,440/500,000 | Speed 3,909 t/s\nStep   260 | Loss 266.2343 | LR 7.80e-06 | Tokens 66,560/500,000 | Speed 4,184 t/s\nStep   280 | Loss 283.1250 | LR 8.40e-06 | Tokens 71,680/500,000 | Speed 4,457 t/s\nStep   300 | Loss 222.0158 | LR 9.00e-06 | Tokens 76,800/500,000 | Speed 4,735 t/s\nStep   320 | Loss 236.2877 | LR 9.60e-06 | Tokens 81,920/500,000 | Speed 5,007 t/s\nStep   340 | Loss 104.8964 | LR 1.02e-05 | Tokens 87,040/500,000 | Speed 5,270 t/s\nStep   360 | Loss 129.1694 | LR 1.08e-05 | Tokens 92,160/500,000 | Speed 5,520 t/s\nStep   380 | Loss 91.2437 | LR 1.14e-05 | Tokens 97,280/500,000 | Speed 5,752 t/s\nStep   400 | Loss 144.7572 | LR 1.20e-05 | Tokens 102,400/500,000 | Speed 5,995 t/s\nStep   420 | Loss 90.6684 | LR 1.26e-05 | Tokens 107,520/500,000 | Speed 6,247 t/s\nStep   440 | Loss 49.3006 | LR 1.32e-05 | Tokens 112,640/500,000 | Speed 6,490 t/s\nStep   460 | Loss 32.8636 | LR 1.38e-05 | Tokens 117,760/500,000 | Speed 6,723 t/s\nStep   480 | Loss 29.8049 | LR 1.44e-05 | Tokens 122,880/500,000 | Speed 6,943 t/s\nStep   500 | Loss 45.5724 | LR 1.50e-05 | Tokens 128,000/500,000 | Speed 7,170 t/s\nStep   520 | Loss 11.8897 | LR 1.56e-05 | Tokens 133,120/500,000 | Speed 7,389 t/s\nStep   540 | Loss 23.0859 | LR 1.62e-05 | Tokens 138,240/500,000 | Speed 7,605 t/s\nStep   560 | Loss 12.0006 | LR 1.68e-05 | Tokens 143,360/500,000 | Speed 7,820 t/s\nStep   580 | Loss 13.3441 | LR 1.74e-05 | Tokens 148,480/500,000 | Speed 8,030 t/s\nStep   600 | Loss 18.6916 | LR 1.80e-05 | Tokens 153,600/500,000 | Speed 8,241 t/s\nStep   620 | Loss 10.6229 | LR 1.86e-05 | Tokens 158,720/500,000 | Speed 8,447 t/s\nStep   640 | Loss 10.3318 | LR 1.92e-05 | Tokens 163,840/500,000 | Speed 8,624 t/s\nStep   660 | Loss 16.1458 | LR 1.98e-05 | Tokens 168,960/500,000 | Speed 8,820 t/s\nStep   680 | Loss 11.3147 | LR 2.04e-05 | Tokens 174,080/500,000 | Speed 9,022 t/s\nStep   700 | Loss 10.8044 | LR 2.10e-05 | Tokens 179,200/500,000 | Speed 9,225 t/s\nStep   720 | Loss 13.7962 | LR 2.16e-05 | Tokens 184,320/500,000 | Speed 9,424 t/s\nStep   740 | Loss 16.4035 | LR 2.22e-05 | Tokens 189,440/500,000 | Speed 9,621 t/s\nStep   760 | Loss 9.9888 | LR 2.28e-05 | Tokens 194,560/500,000 | Speed 9,807 t/s\nStep   780 | Loss 9.8413 | LR 2.34e-05 | Tokens 199,680/500,000 | Speed 9,997 t/s\nStep   800 | Loss 9.8971 | LR 2.40e-05 | Tokens 204,800/500,000 | Speed 10,181 t/s\nStep   820 | Loss 10.0457 | LR 2.46e-05 | Tokens 209,920/500,000 | Speed 10,363 t/s\nStep   840 | Loss 9.7211 | LR 2.52e-05 | Tokens 215,040/500,000 | Speed 10,539 t/s\nStep   860 | Loss 10.2521 | LR 2.58e-05 | Tokens 220,160/500,000 | Speed 10,712 t/s\nStep   880 | Loss 11.3160 | LR 2.64e-05 | Tokens 225,280/500,000 | Speed 10,874 t/s\nStep   900 | Loss 11.9009 | LR 2.70e-05 | Tokens 230,400/500,000 | Speed 11,033 t/s\nStep   920 | Loss 11.8798 | LR 2.76e-05 | Tokens 235,520/500,000 | Speed 11,211 t/s\nStep   940 | Loss 11.2017 | LR 2.82e-05 | Tokens 240,640/500,000 | Speed 11,384 t/s\nStep   960 | Loss 9.4861 | LR 2.88e-05 | Tokens 245,760/500,000 | Speed 11,553 t/s\nStep   980 | Loss 9.5456 | LR 2.94e-05 | Tokens 250,880/500,000 | Speed 11,719 t/s\nStep  1000 | Loss 9.9129 | LR 3.00e-05 | Tokens 256,000/500,000 | Speed 11,884 t/s\nStep  1020 | Loss 9.2828 | LR 3.06e-05 | Tokens 261,120/500,000 | Speed 12,044 t/s\nStep  1040 | Loss 8.9787 | LR 3.12e-05 | Tokens 266,240/500,000 | Speed 12,199 t/s\nStep  1060 | Loss 9.9404 | LR 3.18e-05 | Tokens 271,360/500,000 | Speed 12,351 t/s\nStep  1080 | Loss 9.1438 | LR 3.24e-05 | Tokens 276,480/500,000 | Speed 12,503 t/s\nStep  1100 | Loss 9.1582 | LR 3.30e-05 | Tokens 281,600/500,000 | Speed 12,653 t/s\nStep  1120 | Loss 9.1077 | LR 3.36e-05 | Tokens 286,720/500,000 | Speed 12,802 t/s\nStep  1140 | Loss 9.3940 | LR 3.42e-05 | Tokens 291,840/500,000 | Speed 12,937 t/s\nStep  1160 | Loss 8.9919 | LR 3.48e-05 | Tokens 296,960/500,000 | Speed 13,071 t/s\nStep  1180 | Loss 9.1634 | LR 3.54e-05 | Tokens 302,080/500,000 | Speed 13,214 t/s\nStep  1200 | Loss 9.2110 | LR 3.60e-05 | Tokens 307,200/500,000 | Speed 13,360 t/s\nStep  1220 | Loss 9.0345 | LR 3.66e-05 | Tokens 312,320/500,000 | Speed 13,508 t/s\nStep  1240 | Loss 8.8781 | LR 3.72e-05 | Tokens 317,440/500,000 | Speed 13,653 t/s\nStep  1260 | Loss 8.9193 | LR 3.78e-05 | Tokens 322,560/500,000 | Speed 13,794 t/s\nStep  1280 | Loss 9.0669 | LR 3.84e-05 | Tokens 327,680/500,000 | Speed 13,933 t/s\nStep  1300 | Loss 11.4578 | LR 3.90e-05 | Tokens 332,800/500,000 | Speed 14,065 t/s\nStep  1320 | Loss 8.8863 | LR 3.96e-05 | Tokens 337,920/500,000 | Speed 14,198 t/s\nStep  1340 | Loss 8.8332 | LR 4.02e-05 | Tokens 343,040/500,000 | Speed 14,330 t/s\nStep  1360 | Loss 9.0507 | LR 4.08e-05 | Tokens 348,160/500,000 | Speed 14,460 t/s\nStep  1380 | Loss 9.0570 | LR 4.14e-05 | Tokens 353,280/500,000 | Speed 14,585 t/s\nStep  1400 | Loss 9.2280 | LR 4.20e-05 | Tokens 358,400/500,000 | Speed 14,706 t/s\nStep  1420 | Loss 9.1415 | LR 4.26e-05 | Tokens 363,520/500,000 | Speed 14,819 t/s\nStep  1440 | Loss 9.0741 | LR 4.32e-05 | Tokens 368,640/500,000 | Speed 14,931 t/s\nStep  1460 | Loss 8.9093 | LR 4.38e-05 | Tokens 373,760/500,000 | Speed 15,055 t/s\nStep  1480 | Loss 8.9209 | LR 4.44e-05 | Tokens 378,880/500,000 | Speed 15,175 t/s\nStep  1500 | Loss 8.7883 | LR 4.50e-05 | Tokens 384,000/500,000 | Speed 15,300 t/s\nStep  1520 | Loss 9.0443 | LR 4.56e-05 | Tokens 389,120/500,000 | Speed 15,407 t/s\nStep  1540 | Loss 9.0914 | LR 4.62e-05 | Tokens 394,240/500,000 | Speed 15,513 t/s\nStep  1560 | Loss 8.7547 | LR 4.68e-05 | Tokens 399,360/500,000 | Speed 15,627 t/s\nStep  1580 | Loss 8.8587 | LR 4.74e-05 | Tokens 404,480/500,000 | Speed 15,736 t/s\nStep  1600 | Loss 9.6694 | LR 4.80e-05 | Tokens 409,600/500,000 | Speed 15,830 t/s\nStep  1620 | Loss 9.6117 | LR 4.86e-05 | Tokens 414,720/500,000 | Speed 15,941 t/s\nStep  1640 | Loss 9.3411 | LR 4.92e-05 | Tokens 419,840/500,000 | Speed 16,043 t/s\nStep  1660 | Loss 9.1782 | LR 4.98e-05 | Tokens 424,960/500,000 | Speed 16,142 t/s\nStep  1680 | Loss 9.1049 | LR 5.04e-05 | Tokens 430,080/500,000 | Speed 16,245 t/s\nStep  1700 | Loss 12.5030 | LR 5.10e-05 | Tokens 435,200/500,000 | Speed 16,356 t/s\nStep  1720 | Loss 9.0718 | LR 5.16e-05 | Tokens 440,320/500,000 | Speed 16,464 t/s\nStep  1740 | Loss 8.5934 | LR 5.22e-05 | Tokens 445,440/500,000 | Speed 16,571 t/s\nStep  1760 | Loss 9.8830 | LR 5.28e-05 | Tokens 450,560/500,000 | Speed 16,677 t/s\nStep  1780 | Loss 9.4056 | LR 5.34e-05 | Tokens 455,680/500,000 | Speed 16,778 t/s\nStep  1800 | Loss 9.6957 | LR 5.40e-05 | Tokens 460,800/500,000 | Speed 16,879 t/s\nStep  1820 | Loss 8.7129 | LR 5.46e-05 | Tokens 465,920/500,000 | Speed 16,985 t/s\nStep  1840 | Loss 9.0181 | LR 5.52e-05 | Tokens 471,040/500,000 | Speed 17,082 t/s\nStep  1860 | Loss 8.6204 | LR 5.58e-05 | Tokens 476,160/500,000 | Speed 17,182 t/s\nStep  1880 | Loss 8.8895 | LR 5.64e-05 | Tokens 481,280/500,000 | Speed 17,280 t/s\nStep  1900 | Loss 9.0171 | LR 5.70e-05 | Tokens 486,400/500,000 | Speed 17,378 t/s\nStep  1920 | Loss 8.8014 | LR 5.76e-05 | Tokens 491,520/500,000 | Speed 17,470 t/s\nStep  1940 | Loss 8.7777 | LR 5.82e-05 | Tokens 496,640/500,000 | Speed 17,564 t/s\n\nReached target token count → stopping.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>gpu_mem_gb</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>▇██▆▄▅▅▄▄▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>▁▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇██</td></tr><tr><td>step</td><td>▁▁▁▂▂▂▂▃▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>tokens_per_sec</td><td>▁▂▂▂▂▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇██████</td></tr><tr><td>tokens_seen</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>gpu_mem_gb</td><td>0.14571</td></tr><tr><td>loss</td><td>8.77768</td></tr><tr><td>lr</td><td>6e-05</td></tr><tr><td>step</td><td>1940</td></tr><tr><td>tokens_per_sec</td><td>17563.62683</td></tr><tr><td>tokens_seen</td><td>496640</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">swift-leaf-10</strong> at: <a href='https://wandb.ai/tusharmishra802-/llama-demo/runs/fsbp9mvr' target=\"_blank\">https://wandb.ai/tusharmishra802-/llama-demo/runs/fsbp9mvr</a><br> View project at: <a href='https://wandb.ai/tusharmishra802-/llama-demo' target=\"_blank\">https://wandb.ai/tusharmishra802-/llama-demo</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20251113_084137-fsbp9mvr/logs</code>"},"metadata":{}},{"name":"stdout","text":"Training finished!\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"# Fixed generation function (top-k sampling)\ndef generate(model, tokenizer, prompt, max_new_tokens=50, top_k=50, temperature=0.8, pad_token_id=None):\n    if pad_token_id is None:\n        pad_token_id = tokenizer.eos_token_id\n    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(CFG[\"device\"])\n    \n    with torch.no_grad():\n        for _ in range(max_new_tokens):\n            with autocast(device_type=\"cuda\" if torch.cuda.is_available() else \"cpu\", dtype=torch.bfloat16, enabled=True):\n                logits = model(input_ids)[:, -1, :]  # Last position logits\n                logits = logits / temperature\n                # Top-k filtering\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                probs = torch.softmax(logits, dim=-1)\n                mask = logits[0] < v[0].min()  # 1D boolean [vocab]\n                probs[0][mask] = 0  # Set low-prob to 0\n                next_token = torch.multinomial(probs, num_samples=1)\n                input_ids = torch.cat([input_ids, next_token], dim=-1)\n                if next_token.item() == pad_token_id:\n                    break  # Stop at EOS\n    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n\n# Test prompts (web/edu themed)\nprompts = [\n    \"Web hosting is essential for\",\n    \"Machine learning models train on datasets like\",\n    \"A good developer should know\",\n    \"FineWeb-Edu is a filtered version of\",\n    \"The future of AI in education involves\",\n    \"Why do you think people follow me\"\n]\n\nprint(\"=== Generation Tests (Final Train Loss: 4.19 | PPL: {:.0f}) ===\".format(math.exp(4.19)))\nfor prompt in prompts:\n    with torch.no_grad():  # Per-prompt for safety\n        generated = generate(model, tokenizer, prompt, max_new_tokens=50, top_k=50, temperature=0.8)\n        continuation = generated[len(prompt):].strip()  # Continuation only\n        print(f\"\\nPrompt: {prompt}\")\n        print(f\"Output: {continuation}\")\n        print(\"-\" * 80)","metadata":{"id":"GKBRXQaewseU","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:43:31.957616Z","iopub.execute_input":"2025-11-13T08:43:31.958115Z","iopub.status.idle":"2025-11-13T08:43:32.639835Z","shell.execute_reply.started":"2025-11-13T08:43:31.958092Z","shell.execute_reply":"2025-11-13T08:43:32.639017Z"}},"outputs":[{"name":"stdout","text":"=== Generation Tests (Final Train Loss: 4.19 | PPL: 66) ===\n\nPrompt: Web hosting is essential for\nOutput: at at- “etchetchetchetchetchetchetchetchetchetchetchetchetchetchetchetchetchetchetchetchetchetchetchetchetchetchetchetchetchetchetchetchetchetchetchetchetchetchetchetchetchetchetchetchetchetch\n--------------------------------------------------------------------------------\n\nPrompt: Machine learning models train on datasets like\nOutput: have\n of I9 and we ( at  thatal Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot\n--------------------------------------------------------------------------------\n\nPrompt: A good developer should know\nOutput: 9 to one in0 we an (: but is1 be a an when in1 to to,\n of with the but 0 they a 01 of they \n their the of to 5al to to a will with5\n--------------------------------------------------------------------------------\n\nPrompt: FineWeb-Edu is a filtered version of\nOutput: 9 a Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics Olympics\n--------------------------------------------------------------------------------\n\nPrompt: The future of AI in education involves\nOutput: ,  I when0,\n I to whenal as it1 a that- is7 have to127, the we have as and3 will\n with with,,s to6al over atal the it by it of to\n--------------------------------------------------------------------------------\n\nPrompt: Why do you think people follow me\nOutput: be89\n\n to will at “ to5 C and to9 as in (03 on, the can-  to, to an they Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot Bot\n--------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}