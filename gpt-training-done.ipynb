{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13626940,"sourceType":"datasetVersion","datasetId":8660922}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport torch\n\n# === PATHS ===\ninput_dir = \"/kaggle/input/gpt-fineweb-100m-checkpoint\"\nextract_dir = \"/kaggle/working/checkpoints\"  # Not needed, but keeping for clarity\nos.makedirs(extract_dir, exist_ok=True)\n\n# === FIND .pth FILE ===\nckpt_files = [f for f in os.listdir(input_dir) if f.endswith('.pth')]\n\nif not ckpt_files:\n    raise FileNotFoundError(\"No .pth file found in input directory\")\n\n# Use the only (or latest) one\ncheckpoint_path = os.path.join(input_dir, ckpt_files[0])\nprint(f\"Found checkpoint: {checkpoint_path}\")\n\n# === LOAD CHECKPOINT ===\n# Replace with your actual device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ncheckpoint = torch.load(checkpoint_path, map_location=device)\n\ntokens_seen = checkpoint.get('tokens_seen', 0)\nstep = checkpoint.get('step', 0)\n\nprint(f\"Loaded: {checkpoint_path}\")\nprint(f\"   → Resuming at step {step}, tokens seen: {tokens_seen:,}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-06T11:14:34.282175Z","iopub.execute_input":"2025-11-06T11:14:34.282403Z","iopub.status.idle":"2025-11-06T11:14:45.989697Z","shell.execute_reply.started":"2025-11-06T11:14:34.282386Z","shell.execute_reply":"2025-11-06T11:14:45.988909Z"}},"outputs":[{"name":"stdout","text":"Found checkpoint: /kaggle/input/gpt-fineweb-100m-checkpoint/checkpoint_100016128.pth\nLoaded: /kaggle/input/gpt-fineweb-100m-checkpoint/checkpoint_100016128.pth\n   → Resuming at step 12210, tokens seen: 100,016,128\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nwandb_token = user_secrets.get_secret(\"wandb\")\n\nimport os\nos.environ[\"HF_TOKEN\"] = hf_token\nos.environ[\"WANDB_API_KEY\"] = wandb_token\nprint(\"HF token loaded—rate limit bypassed!\")  # Optional confirm\n\n!uv pip install -q wandb transformers\n\nimport torch, random, os, math, time, wandb\nimport numpy as np\n# import tiktoken\n# from datatrove.pipeline.readers import ParquetReader\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\n# from itertools import cycle\nfrom torch.utils.data import DataLoader, IterableDataset\nimport torch.nn as nn\nimport torch.nn.functional as F  # For scaled_dot_product_attention\nimport torch.optim as optim\nfrom torch.amp import autocast, GradScaler\nimport matplotlib.pyplot as plt\n# from torch.profiler import profile, record_function, ProfilerActivity\n\ntokenizer = AutoTokenizer.from_pretrained('gpt2')\ntokenizer.pad_token = tokenizer.eos_token\nwandb.login(key=wandb_token) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T11:14:50.671184Z","iopub.execute_input":"2025-11-06T11:14:50.671820Z","iopub.status.idle":"2025-11-06T11:15:11.512602Z","shell.execute_reply.started":"2025-11-06T11:14:50.671792Z","shell.execute_reply":"2025-11-06T11:15:11.511772Z"}},"outputs":[{"name":"stdout","text":"HF token loaded—rate limit bypassed!\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3765f0f245ae491f90b88e10df6a87a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"51e23a0798ce4ccca8a5c85cfb659a38"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7bca238f23244e8e8cfd507285176c2a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18a089a3d480400a829f79622e215ab0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57f9f7c6b1664bbe9f63d2b772655500"}},"metadata":{}},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtusharmishra802\u001b[0m (\u001b[33mtusharmishra802-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"CFG = {\n    \"seed\": 42,\n    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n\n    # Model\n    \"vocab_size\": 50257,          # GPT-2 vocab\n    \"emb_dim\": 768,\n    \"context_length\": 256,        # keep small for speed\n    \"n_heads\": 12,\n    \"n_layers\": 12,\n    \"drop_rate\": 0.0, # 0.1 or 0.2 during fine tuning\n    \"qkv_bias\": False,\n\n    # Data\n    \"max_tokens\": 200_000_000,        # STOP after this many tokens\n    \"warmup_tokens\": 2_000_000,      # linear warm-up\n    \"batch_size\": 16,\n    \"shuffle_buffer\": 5_000,\n    \"val_max_tokens\" : 10_000,\n    \"tokens_per_step\": 16 * 256,\n\n    # Optimiser\n    \"optimizer\": \"adamw\",\n    \"lr\": 3e-4,\n    \"final_lr\": 6e-5,\n    \"eps\": 1e-8,\n    \"weight_decay\": 0.1,\n    \"betas\": (0.9, 0.95),\n\n    # Misc\n    \"log_interval\": 20,           # steps\n    \"checkpoint_interval\": 50_000_000,  # New: Save checkpoint every 50M tokens\n    \"validate_interval\": 50_000_000,    # New: Validate every 50M tokens (align with checkpoints)\n    \"wandb_project\": \"gpt-fineweb-demo\",\n    \"wandb_run_id\": 'icy-night-14',       # auto-generated\n}\n\ndef set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)\n\nif torch.cuda.is_available():\n    try:\n        torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=True, enable_math=False)\n        print(\"PyTorch SDP kernels enabled (mem_efficient & not math for speed on T4/P100)!\")\n    except Exception as e:\n        print(f\"Could not enable SDP kernels: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T11:16:48.505674Z","iopub.execute_input":"2025-11-06T11:16:48.506502Z","iopub.status.idle":"2025-11-06T11:16:48.520012Z","shell.execute_reply.started":"2025-11-06T11:16:48.506475Z","shell.execute_reply":"2025-11-06T11:16:48.519325Z"}},"outputs":[{"name":"stdout","text":"PyTorch SDP kernels enabled (mem_efficient & not math for speed on T4/P100)!\n","output_type":"stream"},{"name":"stderr","text":"/usr/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n  self.gen = func(*args, **kwds)\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"torch.manual_seed(CFG[\"seed\"])\nif CFG[\"device\"] == \"cuda\":\n    torch.cuda.manual_seed_all(CFG[\"seed\"])\n\n# ------------------- 2. WANDB INIT -------------------\n# wandb.init(\n#     project=CFG[\"wandb_project\"],\n#     name=CFG[\"wandb_run_name\"],\n#     config=CFG,\n#     mode=\"online\",   # set \"offline\" if you have no internet\n# )\n\n# Wandb: Resume existing run (or start new with resume='allow')\nif 'wandb_run_id' in checkpoint:  # Saved from prior\n    wandb.init(project=CFG['wandb_project'], id=checkpoint['wandb_run_id'], resume='allow')\nelse:\n    wandb.init(project=CFG['wandb_project'], name=CFG.get('wandb_run_name', f\"resume_{tokens_seen}\"), resume='allow')\n\nprint(f\"Wandb resumed: step {step}, tokens {tokens_seen:,}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T11:16:51.060968Z","iopub.execute_input":"2025-11-06T11:16:51.061500Z","iopub.status.idle":"2025-11-06T11:16:58.412217Z","shell.execute_reply.started":"2025-11-06T11:16:51.061475Z","shell.execute_reply":"2025-11-06T11:16:58.411597Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251106_111651-1q8eg9as</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/tusharmishra802-/gpt-fineweb-demo/runs/1q8eg9as' target=\"_blank\">resume_100016128</a></strong> to <a href='https://wandb.ai/tusharmishra802-/gpt-fineweb-demo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/tusharmishra802-/gpt-fineweb-demo' target=\"_blank\">https://wandb.ai/tusharmishra802-/gpt-fineweb-demo</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/tusharmishra802-/gpt-fineweb-demo/runs/1q8eg9as' target=\"_blank\">https://wandb.ai/tusharmishra802-/gpt-fineweb-demo/runs/1q8eg9as</a>"},"metadata":{}},{"name":"stdout","text":"Wandb resumed: step 12210, tokens 100,016,128\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"class LayerNorm(nn.Module):\n    def __init__(self, emb_dim):\n        super().__init__()\n        self.eps = 1e-5\n        self.scale = nn.Parameter(torch.ones(emb_dim))\n        self.shift = nn.Parameter(torch.zeros(emb_dim))\n        \n    def forward(self, x):\n        mean = x.mean(dim=-1, keepdim=True)\n        var = x.var(dim=-1, keepdim=True, unbiased=False)\n        norm_x = (x-mean) / torch.sqrt(var + self.eps)\n        return self.scale * norm_x + self.shift\n\nclass GELU(nn.Module):\n    def __init__(self):\n        super().__init__()\n        \n    def forward(self, x):\n        return 0.5*x*(1 + torch.tanh(\n            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n            (x + 0.044715*torch.pow(x, 3))\n        ))\n\nclass FeedForward(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.layers = nn.Sequential(\n            nn.Linear(cfg['emb_dim'], 4*cfg['emb_dim']),\n            GELU(),\n            nn.Linear(4*cfg['emb_dim'], cfg['emb_dim'])\n        )\n        \n    def forward(self, x):\n        return self.layers(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T11:16:58.413246Z","iopub.execute_input":"2025-11-06T11:16:58.413523Z","iopub.status.idle":"2025-11-06T11:16:58.421061Z","shell.execute_reply.started":"2025-11-06T11:16:58.413507Z","shell.execute_reply":"2025-11-06T11:16:58.420498Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n        super().__init__()\n        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n\n        self.d_out = d_out\n        self.num_heads = num_heads\n        self.head_dim = d_out // num_heads  # d_k = d_v = d_out / num_heads\n\n        # Linear projections for Q, K, V\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.out_proj = nn.Linear(d_out, d_out)  # Output projection\n\n        self.dropout = dropout  # Store value for SDPA\n        self.scale = self.head_dim ** -0.5\n\n        # Optional: causal mask buffer (not needed with is_causal=True)\n        # self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n\n        print(f\"MHA: {num_heads} heads, head_dim={self.head_dim}, SDPA enabled\")\n\n    def forward(self, x):\n        b, num_tokens, d_in = x.shape\n\n        # (b, num_tokens, d_out)\n        queries = self.W_query(x)\n        keys    = self.W_key(x)\n        values  = self.W_value(x)\n\n        # Split into heads: (b, num_tokens, num_heads, head_dim)\n        # Use .reshape() instead of .view() for safety\n        queries = queries.reshape(b, num_tokens, self.num_heads, self.head_dim)\n        keys    = keys.reshape(b, num_tokens, self.num_heads, self.head_dim)\n        values  = values.reshape(b, num_tokens, self.num_heads, self.head_dim)\n\n        # Transpose: (b, num_heads, num_tokens, head_dim)\n        queries = queries.transpose(1, 2)\n        keys    = keys.transpose(1, 2)\n        values  = values.transpose(1, 2)\n\n        # Use SDPA with causal masking\n        # IMPORTANT: dropout_p only applies during training\n        attn_output = F.scaled_dot_product_attention(\n            query=queries,\n            key=keys,\n            value=values,\n            attn_mask=None,\n            dropout_p=self.dropout if self.training else 0.0,\n            is_causal=True  # This creates the causal mask automatically\n        )\n\n        # Merge heads: (b, num_tokens, num_heads, head_dim) → (b, num_tokens, d_out)\n        attn_output = attn_output.transpose(1, 2).contiguous()  # (b, num_tokens, num_heads, head_dim)\n        context_vec = attn_output.reshape(b, num_tokens, self.d_out)\n\n        # Final linear projection + dropout\n        context_vec = self.out_proj(context_vec)\n        context_vec = F.dropout(context_vec, p=self.dropout, training=self.training)\n\n        return context_vec","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T11:16:58.421805Z","iopub.execute_input":"2025-11-06T11:16:58.422067Z","iopub.status.idle":"2025-11-06T11:16:58.437420Z","shell.execute_reply.started":"2025-11-06T11:16:58.422044Z","shell.execute_reply":"2025-11-06T11:16:58.436714Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.att = MultiHeadAttention(\n            d_in = cfg['emb_dim'],\n            d_out = cfg['emb_dim'],\n            context_length = cfg['context_length'],\n            num_heads = cfg['n_heads'],\n            dropout = cfg['drop_rate'],\n            qkv_bias = cfg['qkv_bias']        \n        )\n        self.ff = FeedForward(cfg)\n        self.norm1 = LayerNorm(cfg['emb_dim'])\n        self.norm2 = LayerNorm(cfg['emb_dim'])\n        self.drop_shortcut = nn.Dropout(cfg['drop_rate'])\n        \n    def forward(self, x):\n        shortcut = x\n        x = self.norm1(x)\n        x = self.att(x)\n        x = shortcut + self.drop_shortcut(x)\n        \n        shortcut = x\n        x = self.norm2(x)\n        x = self.ff(x)\n        x = shortcut + self.drop_shortcut(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T11:17:01.396158Z","iopub.execute_input":"2025-11-06T11:17:01.396914Z","iopub.status.idle":"2025-11-06T11:17:01.403239Z","shell.execute_reply.started":"2025-11-06T11:17:01.396889Z","shell.execute_reply":"2025-11-06T11:17:01.402478Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"class GPTModel(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        self.tok_emb = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n        self.pos_emb = nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n        self.drop_emb = nn.Dropout(cfg['drop_rate'])\n        self.trf_blocks = nn.Sequential(\n            *[TransformerBlock(cfg) for _ in range(cfg['n_layers'])]\n        )\n        self.final_norm = LayerNorm(cfg['emb_dim'])\n        self.out_head = nn.Linear(\n            cfg['emb_dim'], cfg['vocab_size'], bias=False\n        )\n        self.out_head.weight = self.tok_emb.weight # weight tying\n        # === Weight Initialization (CRITICAL) ===\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n        \n    def forward(self, in_idx):\n        batch_size, seq_len = in_idx.shape\n        tok_embeds = self.tok_emb(in_idx)\n        pos_embeds = self.pos_emb(\n            torch.arange(seq_len, device=in_idx.device)\n        )\n        x = tok_embeds + pos_embeds\n        x = self.drop_emb(x)\n        x = self.trf_blocks(x)\n        x = self.final_norm(x)\n        logits = self.out_head(x)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T11:17:03.708901Z","iopub.execute_input":"2025-11-06T11:17:03.709169Z","iopub.status.idle":"2025-11-06T11:17:03.717207Z","shell.execute_reply.started":"2025-11-06T11:17:03.709148Z","shell.execute_reply":"2025-11-06T11:17:03.716688Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"model_cfg = {\n    \"vocab_size\": CFG[\"vocab_size\"],\n    \"context_length\": CFG[\"context_length\"],\n    \"emb_dim\": CFG[\"emb_dim\"],\n    \"n_heads\": CFG[\"n_heads\"],\n    \"n_layers\": CFG[\"n_layers\"],\n    \"drop_rate\": CFG[\"drop_rate\"],\n    \"qkv_bias\": CFG[\"qkv_bias\"],\n}\nmodel = GPTModel(model_cfg).to(CFG[\"device\"])\nmodel = model.to(dtype=torch.bfloat16)      # FP16 for P100\nmodel.load_state_dict(checkpoint['model_state_dict'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T11:17:06.315815Z","iopub.execute_input":"2025-11-06T11:17:06.316420Z","iopub.status.idle":"2025-11-06T11:17:08.857373Z","shell.execute_reply.started":"2025-11-06T11:17:06.316398Z","shell.execute_reply":"2025-11-06T11:17:08.856596Z"}},"outputs":[{"name":"stdout","text":"MHA: 12 heads, head_dim=64, SDPA enabled\nMHA: 12 heads, head_dim=64, SDPA enabled\nMHA: 12 heads, head_dim=64, SDPA enabled\nMHA: 12 heads, head_dim=64, SDPA enabled\nMHA: 12 heads, head_dim=64, SDPA enabled\nMHA: 12 heads, head_dim=64, SDPA enabled\nMHA: 12 heads, head_dim=64, SDPA enabled\nMHA: 12 heads, head_dim=64, SDPA enabled\nMHA: 12 heads, head_dim=64, SDPA enabled\nMHA: 12 heads, head_dim=64, SDPA enabled\nMHA: 12 heads, head_dim=64, SDPA enabled\nMHA: 12 heads, head_dim=64, SDPA enabled\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"ds = load_dataset(\"HuggingFaceFW/fineweb-edu\", split='train', streaming=True)\n\ndef tokenize_function(examples):\n    texts = examples['text']\n    tokenized = tokenizer(texts, truncation=False, add_special_tokens=False)  # Batched for speed\n    tokenized['input_ids'] = [ids + [tokenizer.eos_token_id] for ids in tokenized['input_ids']]\n    return tokenized\n\nds = ds.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['text'])  # Drops raw text, keeps input_ids\nds = ds.shuffle(buffer_size=CFG['shuffle_buffer'])\n\n\nval_raw = load_dataset(\"wikitext\", \"wikitext-103-v1\", split=\"test\", streaming=True)  # Or \"HuggingFaceFW/fineweb\" for raw contrast\nval_raw = val_raw.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['text'])\nval_raw = val_raw.shuffle(buffer_size=CFG['shuffle_buffer'], seed=CFG['seed'] + 1)  # Fixed seed, diff from train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T11:17:12.538814Z","iopub.execute_input":"2025-11-06T11:17:12.539485Z","iopub.status.idle":"2025-11-06T11:17:18.133270Z","shell.execute_reply.started":"2025-11-06T11:17:12.539463Z","shell.execute_reply":"2025-11-06T11:17:18.132504Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52a10eded26d43c797dcc9a909af5fce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/2410 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a16ab39196a24a5ab478cb280598402b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/2410 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bb1a2759f244d478a7364fced00cd96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8de0233469aa4c14b49672b9c4059bb2"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Before dataset: Set pad_token for GPT-2 (safe default)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    print(f\"Set pad_token to EOS: {tokenizer.pad_token_id}\")\n\nclass SlidingWindowDataset(IterableDataset):\n    def __init__(self, ds, tokenizer, context_len, stride, target_tokens, skip_tokens=0, pad_labels=True):\n        self.ds = ds\n        self.tokenizer = tokenizer\n        self.context_len = context_len\n        self.stride = stride\n        self.max_tokens = target_tokens\n        self.skip_tokens = skip_tokens\n        self.pad_id = tokenizer.pad_token_id  # Now safely EOS\n        self.pad_labels = pad_labels  # New: Enable label masking\n\n    def __iter__(self):\n        buffer = []\n        token_count = 0\n        skipping = self.skip_tokens > 0\n        pad_val_labels = -100 if self.pad_labels else self.pad_id\n\n        for example in self.ds:\n            toks = example['input_ids']\n            if not toks:\n                continue\n            buffer.extend(toks)\n\n            while len(buffer) > self.context_len:\n                # Extract full sequences\n                x = buffer[:self.context_len]  # Inputs: always full, no pad needed here\n                y_base = buffer[1:self.context_len + 1]\n                \n                # Pad if short (remnant)\n                num_pads = self.context_len - len(y_base)\n                if num_pads > 0:\n                    # Pad inputs too for consistency (though rare)\n                    x += [self.pad_id] * num_pads\n                    # Pad labels with -100 to mask\n                    y_base += [pad_val_labels] * num_pads\n                \n                y = torch.tensor(y_base, dtype=torch.long)\n                x = torch.tensor(x, dtype=torch.long)\n\n                seq_tokens = self.context_len\n\n                if skipping:\n                    self.skip_tokens -= seq_tokens\n                    if self.skip_tokens <= 0:\n                        skipping = False\n                        print(f\"Skipped ~{token_count:,} tokens; resuming.\")\n                    buffer = buffer[self.stride:]\n                    token_count += seq_tokens\n                else:\n                    yield {'input_ids': x, 'labels': y}\n                    buffer = buffer[self.stride:]\n                    token_count += seq_tokens\n                    if token_count >= self.max_tokens:\n                        return\n\n                if len(buffer) > 2 * self.context_len:\n                    buffer = buffer[-self.context_len:]\n\n        # Remnant: Same padding logic\n        if not skipping and len(buffer) >= 128:\n            x = buffer[:self.context_len]\n            y_base = buffer[1:min(self.context_len + 1, len(buffer) + 1)]\n            num_pads = self.context_len - len(y_base)\n            if num_pads > 0:\n                x += [self.pad_id] * num_pads\n                y_base += [pad_val_labels] * num_pads\n            y = torch.tensor(y_base, dtype=torch.long)\n            x = torch.tensor(x, dtype=torch.long)\n            yield {'input_ids': x, 'labels': y}\n            \n# Usage\ndataset = SlidingWindowDataset(ds, tokenizer, context_len=CFG['context_length'], stride=CFG[\"context_length\"] // 2, target_tokens=CFG['max_tokens'], skip_tokens=tokens_seen)\ndataloader = DataLoader(dataset, batch_size=32, num_workers=2, pin_memory=True, prefetch_factor=2, collate_fn=lambda b: {k: torch.stack([d[k] for d in b]) for k in b[0]})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T11:17:30.491535Z","iopub.execute_input":"2025-11-06T11:17:30.492088Z","iopub.status.idle":"2025-11-06T11:17:30.503721Z","shell.execute_reply.started":"2025-11-06T11:17:30.492068Z","shell.execute_reply":"2025-11-06T11:17:30.503025Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"dataset = SlidingWindowDataset(ds, tokenizer, context_len=CFG['context_length'], stride=CFG[\"context_length\"] // 2, target_tokens=CFG['max_tokens'], skip_tokens=tokens_seen)\ndataloader = DataLoader(dataset, batch_size=32, num_workers=2, pin_memory=True, prefetch_factor=2, collate_fn=lambda b: {k: torch.stack([d[k] for d in b]) for k in b[0]})\n\nvalid_dataset = SlidingWindowDataset(val_raw, tokenizer, context_len=CFG['context_length'], \n                                     stride=CFG[\"context_length\"] // 2, target_tokens=CFG['val_max_tokens'])\nvalid_dataloader = DataLoader(valid_dataset, batch_size=32, num_workers=2, pin_memory=True, \n                              prefetch_factor=2, collate_fn=lambda b: {k: torch.stack([d[k] for d in b]) for k in b[0]})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T11:17:32.467035Z","iopub.execute_input":"2025-11-06T11:17:32.467680Z","iopub.status.idle":"2025-11-06T11:17:32.474401Z","shell.execute_reply.started":"2025-11-06T11:17:32.467656Z","shell.execute_reply":"2025-11-06T11:17:32.473672Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# ------------------- 7. OPTIMISER + SCALER -------------------\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=CFG[\"lr\"],\n    betas=CFG[\"betas\"],\n    weight_decay=CFG[\"weight_decay\"],\n    fused=True,                          # works on P100\n    eps=CFG[\"eps\"],               \n)\n\noptimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n\n# ------------------- 8. LR SCHEDULER -------------------\ntotal_train_tokens = CFG[\"max_tokens\"]\nwarmup_tokens      = CFG[\"warmup_tokens\"]\nwarmup_tokens = CFG[\"warmup_tokens\"]\ntokens_per_step = CFG[\"tokens_per_step\"]\nwarmup_steps = warmup_tokens // tokens_per_step  # ~122 steps\ntotal_steps = total_train_tokens // tokens_per_step  # ~24k steps\n\ndef lr_lambda(step):  # Now takes step, computes tokens internally\n    tokens_current = tokens_seen + (step * tokens_per_step)\n    if tokens_current < warmup_tokens:  # Use < to avoid float issues\n        return tokens_current / warmup_tokens\n    progress = (tokens_current - warmup_tokens) / (total_train_tokens - warmup_tokens)\n    cosine = 0.5 * (1.0 + math.cos(math.pi * min(1.0, progress)))\n    return (CFG[\"final_lr\"] / CFG[\"lr\"]) + (1.0 - CFG[\"final_lr\"] / CFG[\"lr\"]) * cosine\n\nscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\nscheduler.load_state_dict(checkpoint['scheduler_state_dict'])  # Restores any internal state\n\nprint(\"Model, optimizer, and scheduler loaded successfully.\")\nprint(f\"LR Scheduler: Warmup ~{warmup_steps:,} steps ({warmup_tokens:,} tokens) → Cosine decay to {CFG['final_lr']:.1e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T11:17:35.793539Z","iopub.execute_input":"2025-11-06T11:17:35.794160Z","iopub.status.idle":"2025-11-06T11:17:35.807060Z","shell.execute_reply.started":"2025-11-06T11:17:35.794129Z","shell.execute_reply":"2025-11-06T11:17:35.806353Z"}},"outputs":[{"name":"stdout","text":"Model, optimizer, and scheduler loaded successfully.\nLR Scheduler: Warmup ~488 steps (2,000,000 tokens) → Cosine decay to 6.0e-05\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"def validate(model, loader):\n    model.eval()\n    total_loss = 0.0\n    total_tokens = 0\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(CFG[\"device\"], non_blocking=True) for k, v in batch.items()}\n            with autocast(device_type=\"cuda\", dtype=torch.bfloat16, enabled=True):\n                logits = model(batch[\"input_ids\"])\n                loss = criterion(\n                    logits.view(-1, logits.size(-1)),\n                    batch[\"labels\"].view(-1),\n                )\n            num_tokens = (batch[\"labels\"] != -100).sum().item()\n            total_loss += loss.item() * num_tokens\n            total_tokens += num_tokens\n    model.train()\n    return total_loss / total_tokens\n\n# Checkpoint dirs\nos.makedirs(\"checkpoints\", exist_ok=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T11:17:41.098173Z","iopub.execute_input":"2025-11-06T11:17:41.098786Z","iopub.status.idle":"2025-11-06T11:17:41.106526Z","shell.execute_reply.started":"2025-11-06T11:17:41.098761Z","shell.execute_reply":"2025-11-06T11:17:41.105721Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# ------------------- 9. TRAINING LOOP -------------------\ncriterion = nn.CrossEntropyLoss(ignore_index=-100)\nstart_time = time.time()\n\nprint(f\"Resuming training from {tokens_seen:,} tokens...\")\nfor batch in dataloader:\n    step += 1\n    batch = {k: v.to(CFG[\"device\"], non_blocking=True) for k, v in batch.items()}\n\n    # ---- forward + loss -------------------------------------------------\n    with autocast(device_type=\"cuda\", dtype=torch.bfloat16, enabled=True):\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(\n            logits.view(-1, logits.size(-1)),\n            batch[\"labels\"].view(-1),\n        )\n        if not torch.isfinite(loss):\n            print(f\"Non-finite loss at step {step}: {loss.item()} – Skipping.\")\n            optimizer.zero_grad(set_to_none=True)\n            continue\n\n    # ---- backward -------------------------------------------------------\n    # scaler.scale(loss).backward()\n    loss.backward()\n\n    # Gradient clipping – caps norms to prevent explosion\n    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n    # scaler.step(optimizer)\n    # scaler.update()\n    optimizer.step()\n    optimizer.zero_grad(set_to_none=True)\n\n    # ---- LR step (per token, not per step) -------------------------------\n    tokens_seen += batch[\"input_ids\"].numel()\n    scheduler.step()                     # LambdaLR uses the *current* token count\n\n    # ---- logging ---------------------------------------------------------\n    if step % CFG[\"log_interval\"] == 0:\n        elapsed = time.time() - start_time\n        tokens_per_sec = tokens_seen / elapsed\n        lr = optimizer.param_groups[0][\"lr\"]\n\n        wandb.log({\n            \"step\": step,\n            \"loss\": loss.item(),\n            \"lr\": lr,\n            \"tokens_seen\": tokens_seen,\n            \"tokens_per_sec\": tokens_per_sec,\n            \"gpu_mem_gb\": torch.cuda.max_memory_allocated() / 1e9,\n        }, step=step)\n        # print(\n        #     f\"Step {step:5d} | \"\n        #     f\"Loss {loss.item():.4f} | \"\n        #     f\"LR {lr:.2e} | \"\n        #     f\"Tokens {tokens_seen:,}/{CFG['max_tokens']:,} | \"\n        #     f\"Speed {tokens_per_sec:,.0f} t/s\"\n        # )\n\n    # ---- early stop ------------------------------------------------------\n    if tokens_seen >= CFG[\"max_tokens\"]:\n        print(\"\\nReached target token count → stopping.\")\n        break\n\n# --------------------------------------------------------------\nwandb.finish()\nprint(\"Training finished!\")\n# --------------------------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T11:17:55.738646Z","iopub.execute_input":"2025-11-06T11:17:55.739164Z","iopub.status.idle":"2025-11-06T15:55:59.612510Z","shell.execute_reply.started":"2025-11-06T11:17:55.739142Z","shell.execute_reply":"2025-11-06T15:55:59.611607Z"}},"outputs":[{"name":"stdout","text":"Resuming training from 100,016,128 tokens...\n","output_type":"stream"},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (2530 > 1024). Running this sequence through the model will result in indexing errors\nToken indices sequence length is longer than the specified maximum sequence length for this model (1085 > 1024). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"Skipped ~100,015,872 tokens; resuming.\nSkipped ~100,015,872 tokens; resuming.\n\nReached target token count → stopping.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>gpu_mem_gb</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>▂▃▅▅▆▄▂█▄▄▃▅▃▃▂▇▅▄▃▅▇▄▅▂▄▄▆▃▁▃▄▄▆▄▄▄▄▂▂▄</td></tr><tr><td>lr</td><td>██▅▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▅▆▇▇▇▇▇▇▇▇███</td></tr><tr><td>tokens_per_sec</td><td>█▇▆▄▃▃▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>tokens_seen</td><td>▁▁▁▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>gpu_mem_gb</td><td>14.34544</td></tr><tr><td>loss</td><td>4.19841</td></tr><tr><td>lr</td><td>6e-05</td></tr><tr><td>step</td><td>24400</td></tr><tr><td>tokens_per_sec</td><td>11996.00085</td></tr><tr><td>tokens_seen</td><td>199876608</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">resume_100016128</strong> at: <a href='https://wandb.ai/tusharmishra802-/gpt-fineweb-demo/runs/1q8eg9as' target=\"_blank\">https://wandb.ai/tusharmishra802-/gpt-fineweb-demo/runs/1q8eg9as</a><br> View project at: <a href='https://wandb.ai/tusharmishra802-/gpt-fineweb-demo' target=\"_blank\">https://wandb.ai/tusharmishra802-/gpt-fineweb-demo</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20251106_111651-1q8eg9as/logs</code>"},"metadata":{}},{"name":"stdout","text":"Training finished!\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"# ---- checkpoint + validation -----------------------------------------\n# if tokens_seen >= CFG[\"checkpoint_interval\"] + CFG[\"checkpoint_interval\"]:\n# Save checkpoint\n# checkpoint_path = f\"checkpoints/checkpoint_{tokens_seen}.pth\"\n# torch.save({\n#     'model_state_dict': model.state_dict(),\n#     'optimizer_state_dict': optimizer.state_dict(),\n#     'scheduler_state_dict': scheduler.state_dict(),\n#     'tokens_seen': tokens_seen,\n#     'step': step,\n# }, checkpoint_path)\n# print(f\"Saved checkpoint at {tokens_seen:,} tokens: {checkpoint_path}\")\n\n# Run validation (processes ~10k tokens via val_max_tokens)\nval_loss = validate(model, valid_dataloader)\n# wandb.log({\"val_loss\": val_loss, \"tokens_seen\": tokens_seen}, step=step)\nprint(f\"Validation loss: {val_loss:.4f} (over {CFG['val_max_tokens']:,} tokens)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:57:21.002720Z","iopub.execute_input":"2025-11-06T15:57:21.003105Z","iopub.status.idle":"2025-11-06T15:57:23.561940Z","shell.execute_reply.started":"2025-11-06T15:57:21.003075Z","shell.execute_reply":"2025-11-06T15:57:23.561145Z"}},"outputs":[{"name":"stderr","text":"Too many dataloader workers: 2 (max is dataset.num_shards=1). Stopping 1 dataloader workers.\n","output_type":"stream"},{"name":"stdout","text":"Validation loss: 6.0194 (over 10,000 tokens)\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import os, glob\n\nckpt_dir = \"/kaggle/working/checkpoints\"\nprint(\"=== RAW ls ===\")\n!ls -lh {ckpt_dir}\n\nprint(\"\\n=== Python list ===\")\nfiles = sorted(glob.glob(f\"{ckpt_dir}/*.pth\"))\nfor f in files:\n    size = os.path.getsize(f) / (1024**2)\n    print(f\"{f}  ({size:.2f} MB)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:57:28.523627Z","iopub.execute_input":"2025-11-06T15:57:28.524433Z","iopub.status.idle":"2025-11-06T15:57:28.681657Z","shell.execute_reply.started":"2025-11-06T15:57:28.524405Z","shell.execute_reply":"2025-11-06T15:57:28.680791Z"}},"outputs":[{"name":"stdout","text":"=== RAW ls ===\ntotal 709M\n-rw-r--r-- 1 root root 709M Nov  6 15:56 checkpoint_200007680.pth\n\n=== Python list ===\n/kaggle/working/checkpoints/checkpoint_200007680.pth  (708.71 MB)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"import os\nimport zipfile\n\nckpt_path = \"/kaggle/working/checkpoints/checkpoint_200007680.pth\"\nzip_path = \"/kaggle/working/checkpoint_200M.zip\"\n\n# Create zip\nwith zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n    zipf.write(ckpt_path, os.path.basename(ckpt_path))\n\n# Verify size\nzip_size_mb = os.path.getsize(zip_path) / (1024**2)\nprint(f\"Zipped: {zip_path} ({zip_size_mb:.2f} MB)\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:58:13.277469Z","iopub.execute_input":"2025-11-06T15:58:13.278021Z","iopub.status.idle":"2025-11-06T15:59:22.727787Z","shell.execute_reply.started":"2025-11-06T15:58:13.277996Z","shell.execute_reply":"2025-11-06T15:59:22.726988Z"}},"outputs":[{"name":"stdout","text":"Zipped: /kaggle/working/checkpoint_200M.zip (543.76 MB)\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"# Fixed generation function (top-k sampling)\ndef generate(model, tokenizer, prompt, max_new_tokens=50, top_k=50, temperature=0.8, pad_token_id=None):\n    if pad_token_id is None:\n        pad_token_id = tokenizer.eos_token_id\n    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n    \n    with torch.no_grad():\n        for _ in range(max_new_tokens):\n            with autocast(device_type=\"cuda\" if torch.cuda.is_available() else \"cpu\", dtype=torch.bfloat16, enabled=True):\n                logits = model(input_ids)[:, -1, :]  # Last position logits\n                logits = logits / temperature\n                # Top-k filtering\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                probs = torch.softmax(logits, dim=-1)\n                mask = logits[0] < v[0].min()  # 1D boolean [vocab]\n                probs[0][mask] = 0  # Set low-prob to 0\n                next_token = torch.multinomial(probs, num_samples=1)\n                input_ids = torch.cat([input_ids, next_token], dim=-1)\n                if next_token.item() == pad_token_id:\n                    break  # Stop at EOS\n    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n\n# Test prompts (web/edu themed)\nprompts = [\n    \"Web hosting is essential for\",\n    \"Machine learning models train on datasets like\",\n    \"A good developer should know\",\n    \"FineWeb-Edu is a filtered version of\",\n    \"The future of AI in education involves\",\n    \"Why do you think people follow me\"\n]\n\nprint(\"=== Generation Tests (Final Train Loss: 4.19 | PPL: {:.0f}) ===\".format(math.exp(4.19)))\nfor prompt in prompts:\n    with torch.no_grad():  # Per-prompt for safety\n        generated = generate(model, tokenizer, prompt, max_new_tokens=50, top_k=50, temperature=0.8)\n        continuation = generated[len(prompt):].strip()  # Continuation only\n        print(f\"\\nPrompt: {prompt}\")\n        print(f\"Output: {continuation}\")\n        print(\"-\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T16:15:59.953168Z","iopub.execute_input":"2025-11-06T16:15:59.953716Z","iopub.status.idle":"2025-11-06T16:16:06.217651Z","shell.execute_reply.started":"2025-11-06T16:15:59.953693Z","shell.execute_reply":"2025-11-06T16:16:06.216780Z"}},"outputs":[{"name":"stdout","text":"=== Generation Tests (Final Train Loss: 4.19 | PPL: 66) ===\n\nPrompt: Web hosting is essential for\nOutput: businesses and businesses alike, as it promotes a culture where people can interact and feel like they can’t take it home. This article will show why web sites and online platforms offer a range of opportunities, from the rise of new and new platforms\n--------------------------------------------------------------------------------\n\nPrompt: Machine learning models train on datasets like\nOutput: the GIS and Google Data. This model also helps to develop the tools to learn and create a more accurate analysis of user experience for a wide range of applications (e.g., virtual reality, virtual reality, or online devices), and so it\n--------------------------------------------------------------------------------\n\nPrompt: A good developer should know\nOutput: when they use the correct code.\nIn terms of a good user-friendly software, your software can be used for small, dynamic, easy, and convenient systems. To do this, you can use the necessary features like code, code and the\n--------------------------------------------------------------------------------\n\nPrompt: FineWeb-Edu is a filtered version of\nOutput: the text used in web development for web development (e.g. web development).\nThe following file is the same and is known as:\n- R: It is also used for web development.\n- R: It is used for web\n--------------------------------------------------------------------------------\n\nPrompt: The future of AI in education involves\nOutput: a series of challenges and challenges. The goal is to explore the potential of AI in education and to explore new solutions. This approach is particularly critical for the adoption of AI in education. This approach ensures that AI is not only accessible but also accessible for\n--------------------------------------------------------------------------------\n\nPrompt: Why do you think people follow me\nOutput: in the world?\nIn a world where the future of technology is being developed, we think we can become more more flexible and more equipped to implement the technology. We all know that technology will be more efficient and effective than ever, but it�\n--------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}