{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"94a01212bd544d73a12671c6c8d9356a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d54426a55d944573aad832b4e1783815","IPY_MODEL_4a937d9c91e84fc098eda43a36609f9a","IPY_MODEL_6a7e7662587e49e6855027811c5017a4"],"layout":"IPY_MODEL_4d202abc354c4436847a92412cf1d718"}},"d54426a55d944573aad832b4e1783815":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2bb85598eafb40618c5046bad7f586f4","placeholder":"​","style":"IPY_MODEL_535ac665ba81433791f9136fb7f56fea","value":"README.md: "}},"4a937d9c91e84fc098eda43a36609f9a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0e15b15f2c049b888f84d310fa330a8","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d85f1b4970d441d6a2d0c6c2dcb39c14","value":1}},"6a7e7662587e49e6855027811c5017a4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_65517edb617a436babdf73a0ed32f8bc","placeholder":"​","style":"IPY_MODEL_fa05c01170b94541a7f701e710e96093","value":" 44.3k/? [00:00&lt;00:00, 4.20MB/s]"}},"4d202abc354c4436847a92412cf1d718":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2bb85598eafb40618c5046bad7f586f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"535ac665ba81433791f9136fb7f56fea":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c0e15b15f2c049b888f84d310fa330a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"d85f1b4970d441d6a2d0c6c2dcb39c14":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"65517edb617a436babdf73a0ed32f8bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fa05c01170b94541a7f701e710e96093":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d6b32663fa644b038fbd29dadb8dce16":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d6da71c134844efba11e3abe0ee157d4","IPY_MODEL_ef39fd50d8054b26a912cbb3e77866b6","IPY_MODEL_2a3686c94db24023bd9ea054df9f00a9"],"layout":"IPY_MODEL_0529533f85974ebeb89b9ea4d9c6f2c6"}},"d6da71c134844efba11e3abe0ee157d4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd9ebe1a76fe4c06b810f56cafe2dabe","placeholder":"​","style":"IPY_MODEL_15a67395f476467a9c707ac0c1355192","value":"Resolving data files: 100%"}},"ef39fd50d8054b26a912cbb3e77866b6":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_14c66ff0e8c243beb0923dfedea610b8","max":27468,"min":0,"orientation":"horizontal","style":"IPY_MODEL_81202529d822402496b73c078fe53904","value":27468}},"2a3686c94db24023bd9ea054df9f00a9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_44e680b963f44c6598b67b134bd03bcb","placeholder":"​","style":"IPY_MODEL_0c2af3ed5f9745b2a878fafd055de98a","value":" 27468/27468 [00:02&lt;00:00, 11247.70it/s]"}},"0529533f85974ebeb89b9ea4d9c6f2c6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd9ebe1a76fe4c06b810f56cafe2dabe":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"15a67395f476467a9c707ac0c1355192":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"14c66ff0e8c243beb0923dfedea610b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"81202529d822402496b73c078fe53904":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"44e680b963f44c6598b67b134bd03bcb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c2af3ed5f9745b2a878fafd055de98a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0f7324186c6249b3badf9b5eb287bc71":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ffc9791af8644fc69c9f802469b400d7","IPY_MODEL_ab85091d34454135801f9d648cca6dea","IPY_MODEL_e578317531a24046914f220ed8cccfef"],"layout":"IPY_MODEL_0f1b4f99b0564a4eb85a19f87ce1ae62"}},"ffc9791af8644fc69c9f802469b400d7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b9ca47321b334a5db059aa4d7a29090e","placeholder":"​","style":"IPY_MODEL_036bc17617cd4186bc8d0d5c8ab5207f","value":"Resolving data files: 100%"}},"ab85091d34454135801f9d648cca6dea":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3828b15119ca4570b54cdfaefe6a5f59","max":27468,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b18e588ad72d4655bffcd88125bb7caa","value":27468}},"e578317531a24046914f220ed8cccfef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0d9670c6e7574eba92f2fbf96b2eb3a3","placeholder":"​","style":"IPY_MODEL_91b01c66c9b94a12bbe054dc3503398e","value":" 27468/27468 [00:00&lt;00:00, 323992.50it/s]"}},"0f1b4f99b0564a4eb85a19f87ce1ae62":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9ca47321b334a5db059aa4d7a29090e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"036bc17617cd4186bc8d0d5c8ab5207f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3828b15119ca4570b54cdfaefe6a5f59":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b18e588ad72d4655bffcd88125bb7caa":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0d9670c6e7574eba92f2fbf96b2eb3a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91b01c66c9b94a12bbe054dc3503398e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nwandb_token = user_secrets.get_secret(\"wandb\")\n\nimport os\nos.environ[\"HF_TOKEN\"] = hf_token\nos.environ[\"WANDB_API_KEY\"] = wandb_token\nprint(\"HF token loaded—rate limit bypassed!\")  # Optional confirm","metadata":{"id":"dzNdkipKsiVJ","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:09:12.473785Z","iopub.execute_input":"2025-11-13T08:09:12.474136Z","iopub.status.idle":"2025-11-13T08:09:12.710392Z","shell.execute_reply.started":"2025-11-13T08:09:12.474117Z","shell.execute_reply":"2025-11-13T08:09:12.709618Z"}},"outputs":[{"name":"stdout","text":"HF token loaded—rate limit bypassed!\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!uv pip install -q wandb transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:09:12.711910Z","iopub.execute_input":"2025-11-13T08:09:12.712169Z","iopub.status.idle":"2025-11-13T08:09:13.704347Z","shell.execute_reply.started":"2025-11-13T08:09:12.712151Z","shell.execute_reply":"2025-11-13T08:09:13.703297Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch, random, os, math, time, wandb\nimport numpy as np\n# import tiktoken\n# from datatrove.pipeline.readers import ParquetReader\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\n# from itertools import cycle\nfrom torch.utils.data import DataLoader, IterableDataset\nimport torch.nn as nn\nimport torch.nn.functional as F  # For scaled_dot_product_attention\nimport torch.optim as optim\nfrom torch.amp import autocast, GradScaler\nimport matplotlib.pyplot as plt\n# from torch.profiler import profile, record_function, ProfilerActivity\n\nwandb.login(key=wandb_token) ","metadata":{"id":"1yrMCzDPstK7","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:09:13.705518Z","iopub.execute_input":"2025-11-13T08:09:13.705811Z","iopub.status.idle":"2025-11-13T08:09:34.013094Z","shell.execute_reply.started":"2025-11-13T08:09:13.705777Z","shell.execute_reply":"2025-11-13T08:09:34.012325Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtusharmishra802\u001b[0m (\u001b[33mtusharmishra802-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('huggyllama/llama-30b')\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:09:34.605313Z","iopub.execute_input":"2025-11-13T08:09:34.606023Z","iopub.status.idle":"2025-11-13T08:09:36.355031Z","shell.execute_reply.started":"2025-11-13T08:09:34.605994Z","shell.execute_reply":"2025-11-13T08:09:36.354421Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/700 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f66c8296253462cabaea00cb28a2a8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"113d5417e41740e4b963068a64a2f8d1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ae94432dc424b6a8b9da78607d29677"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2f95c2d069a24dd785cd872cc7699173"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)","metadata":{"id":"ZjYaNXk3s8v0","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:09:36.355946Z","iopub.execute_input":"2025-11-13T08:09:36.356194Z","iopub.status.idle":"2025-11-13T08:09:36.365337Z","shell.execute_reply.started":"2025-11-13T08:09:36.356168Z","shell.execute_reply":"2025-11-13T08:09:36.364768Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"CFG = {\n    \"seed\": 42,\n    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n\n    # Model\n    \"vocab_size\": 32000,          # llama vocab\n    \"emb_dim\": 128,               # 4096 \n    \"context_length\": 8,          # 4096\n    \"n_heads\": 4,                 # 32\n    \"num_kv_heads\": 2,\n    \"n_layers\": 1,                # 32\n    \"drop_rate\": 0.1,\n    \"qkv_bias\": False,\n    'base': 10000,\n    \"intermediate_size\": int(8/3 * 4096),  # ~11008 for full\n\n    # Data\n    \"max_tokens\": 500_000,        # STOP after this many tokens\n    \"warmup_tokens\": 10_000,      # linear warm-up\n    \"batch_size\": 32,\n    \"shuffle_buffer\": 5_000,\n\n    # Optimiser\n    \"optimizer\": \"adamw\",\n    \"lr\": 3e-4,\n    \"final_lr\": 3e-5,\n    \"weight_decay\": 0.1,\n    \"betas\": (0.9, 0.95),\n\n    # SwiGLU\n    'beta' : 1,\n\n    # Misc\n    \"log_interval\": 20,           # steps\n    \"wandb_project\": \"llama-demo\",\n    \"wandb_run_name\": None,       # auto-generated\n}\n","metadata":{"id":"zF2gSbvhPfeA","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:13:31.465060Z","iopub.execute_input":"2025-11-13T08:13:31.465402Z","iopub.status.idle":"2025-11-13T08:13:31.471564Z","shell.execute_reply.started":"2025-11-13T08:13:31.465381Z","shell.execute_reply":"2025-11-13T08:13:31.470808Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"torch.manual_seed(CFG[\"seed\"])\nif CFG[\"device\"] == \"cuda\":\n    torch.cuda.manual_seed_all(CFG[\"seed\"])\n\n# ------------------- 2. WANDB INIT -------------------\nwandb.init(\n    project=CFG[\"wandb_project\"],\n    name=CFG[\"wandb_run_name\"],\n    config=CFG,\n    mode=\"online\",   # set \"offline\" if you have no internet\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:09:42.649129Z","iopub.execute_input":"2025-11-13T08:09:42.649660Z","iopub.status.idle":"2025-11-13T08:09:49.872011Z","shell.execute_reply.started":"2025-11-13T08:09:42.649636Z","shell.execute_reply":"2025-11-13T08:09:49.871347Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251113_080942-y2715d2u</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/tusharmishra802-/llama-demo/runs/y2715d2u' target=\"_blank\">sweet-totem-8</a></strong> to <a href='https://wandb.ai/tusharmishra802-/llama-demo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/tusharmishra802-/llama-demo' target=\"_blank\">https://wandb.ai/tusharmishra802-/llama-demo</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/tusharmishra802-/llama-demo/runs/y2715d2u' target=\"_blank\">https://wandb.ai/tusharmishra802-/llama-demo/runs/y2715d2u</a>"},"metadata":{}},{"execution_count":7,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/tusharmishra802-/llama-demo/runs/y2715d2u?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7ad5183e62d0>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"class RMSNorm(nn.Module):\n    def __init__(self, emb_dim, eps=1e-6):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(emb_dim))\n\n    def forward(self, x):\n        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n        x_norm = x / rms\n        return x_norm * self.weight\n\nclass SiLU(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return x * F.sigmoid(CFG[\"beta\"] * x)\n\nclass FeedForward(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        intermediate_size = cfg.get(\"intermediate_size\", int(8/3 * cfg[\"emb_dim\"]))\n        self.gate_proj = nn.Linear(cfg[\"emb_dim\"], intermediate_size, bias=False)\n        self.up_proj = nn.Linear(cfg[\"emb_dim\"], intermediate_size, bias=False)\n        self.down_proj = nn.Linear(intermediate_size, cfg[\"emb_dim\"], bias=False)\n        self.act_fn = SiLU()\n\n    def forward(self, x):\n        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n","metadata":{"id":"TBX6i-FuPqYp","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:09:49.873278Z","iopub.execute_input":"2025-11-13T08:09:49.873612Z","iopub.status.idle":"2025-11-13T08:09:49.880629Z","shell.execute_reply.started":"2025-11-13T08:09:49.873592Z","shell.execute_reply":"2025-11-13T08:09:49.880133Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"class RotaryEmbeddings(nn.Module):\n    def __init__(self, dim: int, max_position_embeddings: int = 2048, base: int = 10000, device=CFG[\"device\"]):\n        super().__init__()\n        self.dim = dim\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.float32, device=device) / self.dim))\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n        \n        t = torch.arange(self.max_position_embeddings, dtype=torch.float32, device=device)\n        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)  # (T, D/2)\n        self.register_buffer(\"cos_cached\", freqs.cos(), persistent=False)\n        self.register_buffer(\"sin_cached\", freqs.sin(), persistent=False)\n\n    def forward(self, x, seq_len=None):\n        if seq_len is None:\n            seq_len = x.shape[-2]\n        cos = self.cos_cached[:seq_len, ...].unsqueeze(0).unsqueeze(0)  # (1,1,T,D//2)\n        sin = self.sin_cached[:seq_len, ...].unsqueeze(0).unsqueeze(0)\n        \n        x1 = x[..., : self.dim : 2]  # (..., D//2)\n        x2 = x[..., 1 : self.dim : 2]\n        \n        rotated_x1 = x1 * cos - x2 * sin\n        rotated_x2 = x1 * sin + x2 * cos\n        \n        return torch.cat((rotated_x1, rotated_x2), dim=-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:13:58.035167Z","iopub.execute_input":"2025-11-13T08:13:58.035878Z","iopub.status.idle":"2025-11-13T08:13:58.043720Z","shell.execute_reply.started":"2025-11-13T08:13:58.035851Z","shell.execute_reply":"2025-11-13T08:13:58.042941Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"ds = load_dataset(\"HuggingFaceFW/fineweb\", split=\"train\", streaming=True)\n\ndef tokenize_function(examples):\n    texts = examples['text']\n    tokenized = tokenizer(texts, truncation=False, add_special_tokens=False)  # Batched for speed\n    tokenized['input_ids'] = [ids + [tokenizer.eos_token_id] for ids in tokenized['input_ids']]\n    return tokenized\n\nds = ds.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['text'])  # Drops raw text, keeps input_ids\nds = ds.shuffle(buffer_size=CFG['shuffle_buffer'])","metadata":{"id":"FaZbm8Fcs9tu","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:12:38.574683Z","iopub.execute_input":"2025-11-13T08:12:38.575364Z","iopub.status.idle":"2025-11-13T08:12:52.217577Z","shell.execute_reply.started":"2025-11-13T08:12:38.575341Z","shell.execute_reply":"2025-11-13T08:12:52.216781Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/27468 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8d4ab5f0300430f99c61e9e71705ed6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/27468 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"89c8f51833964f83aba7bb8aa4bbd96d"}},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"class SlidingWindowDataset(IterableDataset):\n    def __init__(self, ds, tokenizer, context_len, stride, target_tokens):\n        self.ds = ds  # HF streaming iterable\n        self.tokenizer = tokenizer\n        self.context_len = context_len\n        self.stride = stride\n        self.max_tokens = target_tokens\n        self.pad_id = tokenizer.pad_token_id\n\n    def __iter__(self):\n        buffer = []\n        token_count = 0\n        for example in self.ds:  # Streams tokenized input_ids\n            toks = example['input_ids']\n            if not toks:\n                continue\n            buffer.extend(toks)\n\n            # Fixed buffer logic\n            while len(buffer) > self.context_len:\n                x = buffer[:self.context_len]\n                y = buffer[1:self.context_len + 1]\n                # Pad if needed (rare post-fix)\n                if len(y) < self.context_len:\n                    y += [self.pad_id] * (self.context_len - len(y))\n                yield {'input_ids': torch.tensor(x, dtype=torch.long),\n                       'labels': torch.tensor(y, dtype=torch.long)}  # Dict for HF Trainer\n                buffer = buffer[self.stride:]\n                token_count += self.context_len\n                if token_count >= self.max_tokens:\n                    return\n\n            # Cap buffer to prevent OOM\n            if len(buffer) > 2 * self.context_len:\n                buffer = buffer[-self.context_len:]\n\n        # Remnant with padding\n        if len(buffer) >= 128:  # Min threshold\n            x = buffer[:self.context_len]\n            y = buffer[1:min(self.context_len + 1, len(buffer) + 1)]\n            if len(y) < self.context_len:\n                y += [self.pad_id] * (self.context_len - len(y))\n            yield {'input_ids': torch.tensor(x, dtype=torch.long),\n                   'labels': torch.tensor(y, dtype=torch.long)}\n\n# Usage\ndataset = SlidingWindowDataset(ds, tokenizer, context_len=CFG['context_length'], stride=CFG[\"context_length\"] // 2, target_tokens=CFG['max_tokens'])\ndataloader = DataLoader(dataset, batch_size=32, num_workers=2, pin_memory=True, prefetch_factor=2, collate_fn=lambda b: {k: torch.stack([d[k] for d in b]) for k in b[0]})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:12:52.218829Z","iopub.execute_input":"2025-11-13T08:12:52.219076Z","iopub.status.idle":"2025-11-13T08:12:52.231799Z","shell.execute_reply.started":"2025-11-13T08:12:52.219059Z","shell.execute_reply":"2025-11-13T08:12:52.231245Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"if torch.cuda.is_available():\n    try:\n        torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=True, enable_math=False)\n        print(\"PyTorch SDP kernels enabled (mem_efficient & not math for speed on T4/P100)!\")\n    except Exception as e:\n        print(f\"Could not enable SDP kernels: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:12:52.232554Z","iopub.execute_input":"2025-11-13T08:12:52.233203Z","iopub.status.idle":"2025-11-13T08:12:52.252540Z","shell.execute_reply.started":"2025-11-13T08:12:52.233179Z","shell.execute_reply":"2025-11-13T08:12:52.252003Z"}},"outputs":[{"name":"stdout","text":"PyTorch SDP kernels enabled (mem_efficient & not math for speed on T4/P100)!\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_in, d_out, dropout, num_heads, qkv_bias=False, device=CFG[\"device\"]):\n        super().__init__()\n        assert d_out % num_heads == 0\n        self.d_out = d_out\n        self.num_heads = num_heads\n        self.head_dim = d_out // num_heads\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.out_proj = nn.Linear(d_out, d_out, bias=False)\n        self.rope = RotaryEmbeddings(self.head_dim, device=device)\n        self.dropout = dropout\n        print(f\"MHA: {num_heads} heads, head_dim={self.head_dim}\")\n\n    def forward(self, x):\n        b, t, _ = x.shape\n        q = self.W_query(x).reshape(b, t, self.num_heads, self.head_dim).transpose(1, 2)\n        k = self.W_key(x).reshape(b, t, self.num_heads, self.head_dim).transpose(1, 2)\n        v = self.W_value(x).reshape(b, t, self.num_heads, self.head_dim).transpose(1, 2)\n        q = self.rope(q)\n        k = self.rope(k)\n        attn = F.scaled_dot_product_attention(q, k, v, dropout_p=self.dropout if self.training else 0.0, is_causal=True)\n        attn = attn.transpose(1, 2).reshape(b, t, self.d_out)\n        return self.out_proj(attn)\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3aw80MvytyCg","outputId":"6aaf875e-5eed-4dbd-dd32-7960836c6db9","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:14:17.568506Z","iopub.execute_input":"2025-11-13T08:14:17.569102Z","iopub.status.idle":"2025-11-13T08:14:17.576626Z","shell.execute_reply.started":"2025-11-13T08:14:17.569075Z","shell.execute_reply":"2025-11-13T08:14:17.576017Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"class GroupedQueryAttention(nn.Module):\n    def __init__(self, d_in, d_out, dropout, num_heads, num_kv_heads=None, qkv_bias=False, device=CFG[\"device\"]):\n        super().__init__()\n        assert d_out % num_heads == 0\n        self.d_out = d_out\n        self.num_heads = num_heads\n        self.head_dim = d_out // num_heads\n        self.num_kv_heads = num_kv_heads or num_heads\n        assert self.num_kv_heads <= self.num_heads, \"num_kv_heads must <= num_heads\"\n        assert d_out % self.num_kv_heads == 0 or self.num_kv_heads == num_heads, \"Inconsistent head dims for GQA\"\n        \n        # Projections\n        self.W_query = nn.Linear(d_in, self.num_heads * self.head_dim, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, self.num_kv_heads * self.head_dim, bias=qkv_bias)  # Renamed for clarity\n        self.W_value = nn.Linear(d_in, self.num_kv_heads * self.head_dim, bias=qkv_bias)\n        self.out_proj = nn.Linear(self.num_heads * self.head_dim, d_out, bias=False)\n        \n        # RoPE with device\n        self.rope = RotaryEmbeddings(self.head_dim, device=device)\n        self.dropout = dropout\n\n    def forward(self, x):\n        b, t, d_in = x.shape\n        \n        # Project Q/K/V\n        q = self.W_query(x).reshape(b, t, self.num_heads, self.head_dim).transpose(1, 2)\n        k = self.W_key(x).reshape(b, t, self.num_kv_heads, self.head_dim).transpose(1, 2)\n        v = self.W_value(x).reshape(b, t, self.num_kv_heads, self.head_dim).transpose(1, 2)\n\n        # GQA Repeat\n        if self.num_kv_heads != self.num_heads:\n            repeat_factor = self.num_heads // self.num_kv_heads\n            k = k.repeat_interleave(repeat_factor, dim=1)\n            v = v.repeat_interleave(repeat_factor, dim=1)\n\n        q = self.rope(q)\n        k = self.rope(k)\n\n        # SDPA (use full k/v seq_len for mask/attn)\n        attn_output = F.scaled_dot_product_attention(\n            q, k, v, dropout_p=self.dropout if self.training else 0.0, is_causal=True, attn_mask=None\n        )\n        \n        # Merge\n        attn_output = attn_output.transpose(1, 2).contiguous().reshape(b, t, self.d_out)\n        output = self.out_proj(attn_output)\n                \n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:14:31.528705Z","iopub.execute_input":"2025-11-13T08:14:31.529417Z","iopub.status.idle":"2025-11-13T08:14:31.538528Z","shell.execute_reply.started":"2025-11-13T08:14:31.529391Z","shell.execute_reply":"2025-11-13T08:14:31.537911Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self, cfg, device=CFG[\"device\"]):\n        super().__init__()\n        self.att = GroupedQueryAttention(cfg[\"emb_dim\"], cfg[\"emb_dim\"], cfg[\"drop_rate\"], cfg[\"n_heads\"], cfg['num_kv_heads'], cfg[\"qkv_bias\"], device)\n        self.ff = FeedForward(cfg)\n        self.norm1 = RMSNorm(cfg[\"emb_dim\"])\n        self.norm2 = RMSNorm(cfg[\"emb_dim\"])\n\n    def forward(self, x):\n        x = x + self.att(self.norm1(x))\n        x = x + self.ff(self.norm2(x))\n        return x","metadata":{"id":"_iCWz1QSNMBP","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:14:52.868403Z","iopub.execute_input":"2025-11-13T08:14:52.869098Z","iopub.status.idle":"2025-11-13T08:14:52.874868Z","shell.execute_reply.started":"2025-11-13T08:14:52.869072Z","shell.execute_reply":"2025-11-13T08:14:52.874243Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"class LlamaModel(nn.Module):\n    def __init__(self, cfg, device=CFG[\"device\"]):\n        super().__init__()\n        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg, device) for _ in range(cfg[\"n_layers\"])])\n        self.final_norm = RMSNorm(cfg[\"emb_dim\"])\n        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n        self.out_head.weight = self.tok_emb.weight\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx):\n        x = self.tok_emb(idx)\n        x = self.trf_blocks(x)\n        x = self.final_norm(x)\n        return self.out_head(x)","metadata":{"id":"zpDx0t1QP00N","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:14:59.375020Z","iopub.execute_input":"2025-11-13T08:14:59.375593Z","iopub.status.idle":"2025-11-13T08:14:59.382698Z","shell.execute_reply.started":"2025-11-13T08:14:59.375571Z","shell.execute_reply":"2025-11-13T08:14:59.381882Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"model_cfg = {\n    \"vocab_size\": CFG[\"vocab_size\"],\n    \"context_length\": CFG[\"context_length\"],\n    \"emb_dim\": CFG[\"emb_dim\"],\n    \"n_heads\": CFG[\"n_heads\"],\n    \"num_kv_heads\": CFG[\"num_kv_heads\"],\n    \"n_layers\": CFG[\"n_layers\"],\n    \"drop_rate\": CFG[\"drop_rate\"],\n    \"qkv_bias\": CFG[\"qkv_bias\"],\n}\nmodel = LlamaModel(model_cfg, device=CFG[\"device\"])  # Pass device here!\n\n# 2. Move to GPU + FP16\nmodel = model.to(CFG[\"device\"]).half()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mxa4M1AYP4fH","outputId":"077af1ed-bbb6-45c3-9af3-1a6f1a7fadf9","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:15:08.557845Z","iopub.execute_input":"2025-11-13T08:15:08.558484Z","iopub.status.idle":"2025-11-13T08:15:09.054383Z","shell.execute_reply.started":"2025-11-13T08:15:08.558459Z","shell.execute_reply":"2025-11-13T08:15:09.053758Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"def count_params(model):\n    return sum(p.numel() for p in model.parameters())\n\nprint(f\"Total params: {count_params(model):,}\")\nprint(\"Data equal:\", torch.equal(model.out_head.weight, model.tok_emb.weight))\n\nmodel.tok_emb.weight.data[0, 0] = 999.0  # Modify embedding\nprint(\"Shared? out_head[0,0] after change:\", model.out_head.weight.data[0, 0])  # Should be 999.0\nprint(\"Total params after mod:\", count_params(model))  # Still ~124M—no extra","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UZHVivGMQOia","outputId":"309a609b-5957-4a8f-bb63-1ba73525ec23","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:15:14.686189Z","iopub.execute_input":"2025-11-13T08:15:14.686449Z","iopub.status.idle":"2025-11-13T08:15:14.887453Z","shell.execute_reply.started":"2025-11-13T08:15:14.686432Z","shell.execute_reply":"2025-11-13T08:15:14.886825Z"}},"outputs":[{"name":"stdout","text":"Total params: 4,276,480\nData equal: True\nShared? out_head[0,0] after change: tensor(999., device='cuda:0', dtype=torch.float16)\nTotal params after mod: 4276480\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"# ------------------- 7. OPTIMISER + SCALER -------------------\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=CFG[\"lr\"],\n    betas=CFG[\"betas\"],\n    weight_decay=CFG[\"weight_decay\"],\n    fused=True,                          # works on P100\n)\n\nscaler = GradScaler()\n\n# ------------------- 8. LR SCHEDULER -------------------\ntotal_train_tokens = CFG[\"max_tokens\"]\nwarmup_tokens      = CFG[\"warmup_tokens\"]\nbase_lr            = CFG[\"lr\"]\nfinal_lr           = CFG[\"final_lr\"]\n\ndef lr_lambda(tokens_seen):\n    if tokens_seen <= warmup_tokens:\n        return tokens_seen / max(1, warmup_tokens)               # linear warm-up\n    progress = (tokens_seen - warmup_tokens) / max(1, total_train_tokens - warmup_tokens)\n    cosine = 0.5 * (1.0 + math.cos(math.pi * min(1.0, progress)))\n    return (final_lr / base_lr) + (1.0 - final_lr / base_lr) * cosine\n\nscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\nprint(f\"LR Scheduler: Warmup {warmup_tokens:,} → Cosine decay to {final_lr:.1e}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":651},"id":"EIpJL94CQVNP","outputId":"52b805a7-53da-439f-8ede-36c5ff8dfc6b","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:15:17.085229Z","iopub.execute_input":"2025-11-13T08:15:17.085703Z","iopub.status.idle":"2025-11-13T08:15:17.093655Z","shell.execute_reply.started":"2025-11-13T08:15:17.085679Z","shell.execute_reply":"2025-11-13T08:15:17.092889Z"}},"outputs":[{"name":"stdout","text":"LR Scheduler: Warmup 10,000 → Cosine decay to 3.0e-05\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"def validate(model, loader):\n    model.eval()\n    total_loss = 0.0\n    total_tokens = 0\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(CFG[\"device\"], non_blocking=True) for k, v in batch.items()}\n            with autocast(device_type=CFG['device'], dtype=torch.bfloat16, enabled=True):\n                logits = model(batch[\"input_ids\"])\n                loss = criterion(\n                    logits.view(-1, logits.size(-1)),\n                    batch[\"labels\"].view(-1),\n                )\n            num_tokens = (batch[\"labels\"] != -100).sum().item()\n            total_loss += loss.item() * num_tokens\n            total_tokens += num_tokens\n    model.train()\n    return total_loss / total_tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:15:19.979831Z","iopub.execute_input":"2025-11-13T08:15:19.980560Z","iopub.status.idle":"2025-11-13T08:15:19.987181Z","shell.execute_reply.started":"2025-11-13T08:15:19.980535Z","shell.execute_reply":"2025-11-13T08:15:19.986549Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# ------------------- 9. TRAINING LOOP -------------------\ncriterion = nn.CrossEntropyLoss(ignore_index=-100)\n\ntokens_seen = 0\nstep = 0\nstart_time = time.time()\n\nprint(\"Starting training …\")\nfor batch in dataloader:\n    step += 1\n    batch = {k: v.to(CFG[\"device\"], non_blocking=True) for k, v in batch.items()}\n\n    # ---- forward + loss -------------------------------------------------\n    with autocast(device_type=CFG[\"device\"], dtype=torch.float16):\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(\n            logits.view(-1, logits.size(-1)),\n            batch[\"labels\"].view(-1),\n        )\n\n    # ---- backward -------------------------------------------------------\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n    optimizer.zero_grad(set_to_none=True)\n\n    # ---- LR step (per token, not per step) -------------------------------\n    tokens_seen += batch[\"input_ids\"].numel()\n    scheduler.step()                     # LambdaLR uses the *current* token count\n\n    # ---- logging ---------------------------------------------------------\n    if step % CFG[\"log_interval\"] == 0:\n        elapsed = time.time() - start_time\n        tokens_per_sec = tokens_seen / elapsed\n        lr = optimizer.param_groups[0][\"lr\"]\n\n        wandb.log({\n            \"step\": step,\n            \"loss\": loss.item(),\n            \"lr\": lr,\n            \"tokens_seen\": tokens_seen,\n            \"tokens_per_sec\": tokens_per_sec,\n            \"gpu_mem_gb\": torch.cuda.max_memory_allocated() / 1e9,\n        }, step=step)\n        print(\n            f\"Step {step:5d} | \"\n            f\"Loss {loss.item():.4f} | \"\n            f\"LR {lr:.2e} | \"\n            f\"Tokens {tokens_seen:,}/{CFG['max_tokens']:,} | \"\n            f\"Speed {tokens_per_sec:,.0f} t/s\"\n        )\n\n    # ---- early stop ------------------------------------------------------\n    if tokens_seen >= CFG[\"max_tokens\"]:\n        print(\"\\nReached target token count → stopping.\")\n        break\n\n# --------------------------------------------------------------\nwandb.finish()\nprint(\"Training finished!\")\n# --------------------------------------------------------------","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":297},"id":"hJqw86DNhlDd","outputId":"4108863f-c484-458d-8a14-807051d30a2c","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:15:21.127726Z","iopub.execute_input":"2025-11-13T08:15:21.128458Z","iopub.status.idle":"2025-11-13T08:15:50.988363Z","shell.execute_reply.started":"2025-11-13T08:15:21.128435Z","shell.execute_reply":"2025-11-13T08:15:50.987498Z"}},"outputs":[{"name":"stdout","text":"Starting training …\n","output_type":"stream"},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (3536 > 2048). Running this sequence through the model will result in indexing errors\nToken indices sequence length is longer than the specified maximum sequence length for this model (9730 > 2048). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"Step    20 | Loss 516.2537 | LR 6.00e-07 | Tokens 5,120/500,000 | Speed 370 t/s\nStep    40 | Loss 524.5618 | LR 1.20e-06 | Tokens 10,240/500,000 | Speed 732 t/s\nStep    60 | Loss 390.9808 | LR 1.80e-06 | Tokens 15,360/500,000 | Speed 1,087 t/s\nStep    80 | Loss 285.4486 | LR 2.40e-06 | Tokens 20,480/500,000 | Speed 1,436 t/s\nStep   100 | Loss 289.5216 | LR 3.00e-06 | Tokens 25,600/500,000 | Speed 1,776 t/s\nStep   120 | Loss 304.0412 | LR 3.60e-06 | Tokens 30,720/500,000 | Speed 2,109 t/s\nStep   140 | Loss 578.2516 | LR 4.20e-06 | Tokens 35,840/500,000 | Speed 2,433 t/s\nStep   160 | Loss 506.7226 | LR 4.80e-06 | Tokens 40,960/500,000 | Speed 2,744 t/s\nStep   180 | Loss 505.5831 | LR 5.40e-06 | Tokens 46,080/500,000 | Speed 3,050 t/s\nStep   200 | Loss 418.0981 | LR 6.00e-06 | Tokens 51,200/500,000 | Speed 3,353 t/s\nStep   220 | Loss 415.9190 | LR 6.60e-06 | Tokens 56,320/500,000 | Speed 3,654 t/s\nStep   240 | Loss 367.5887 | LR 7.20e-06 | Tokens 61,440/500,000 | Speed 3,951 t/s\nStep   260 | Loss 266.1587 | LR 7.80e-06 | Tokens 66,560/500,000 | Speed 4,241 t/s\nStep   280 | Loss 212.6902 | LR 8.40e-06 | Tokens 71,680/500,000 | Speed 4,521 t/s\nStep   300 | Loss 245.6009 | LR 9.00e-06 | Tokens 76,800/500,000 | Speed 4,785 t/s\nStep   320 | Loss 277.2322 | LR 9.60e-06 | Tokens 81,920/500,000 | Speed 5,054 t/s\nStep   340 | Loss 83.7228 | LR 1.02e-05 | Tokens 87,040/500,000 | Speed 5,320 t/s\nStep   360 | Loss 113.8532 | LR 1.08e-05 | Tokens 92,160/500,000 | Speed 5,576 t/s\nStep   380 | Loss 73.8660 | LR 1.14e-05 | Tokens 97,280/500,000 | Speed 5,832 t/s\nStep   400 | Loss 84.4576 | LR 1.20e-05 | Tokens 102,400/500,000 | Speed 6,087 t/s\nStep   420 | Loss 75.0440 | LR 1.26e-05 | Tokens 107,520/500,000 | Speed 6,341 t/s\nStep   440 | Loss 47.6697 | LR 1.32e-05 | Tokens 112,640/500,000 | Speed 6,588 t/s\nStep   460 | Loss 31.2313 | LR 1.38e-05 | Tokens 117,760/500,000 | Speed 6,829 t/s\nStep   480 | Loss 28.0060 | LR 1.44e-05 | Tokens 122,880/500,000 | Speed 7,068 t/s\nStep   500 | Loss 33.9993 | LR 1.50e-05 | Tokens 128,000/500,000 | Speed 7,294 t/s\nStep   520 | Loss 23.3548 | LR 1.56e-05 | Tokens 133,120/500,000 | Speed 7,520 t/s\nStep   540 | Loss 10.2258 | LR 1.62e-05 | Tokens 138,240/500,000 | Speed 7,742 t/s\nStep   560 | Loss 20.7142 | LR 1.68e-05 | Tokens 143,360/500,000 | Speed 7,958 t/s\nStep   580 | Loss 10.8743 | LR 1.74e-05 | Tokens 148,480/500,000 | Speed 8,170 t/s\nStep   600 | Loss 20.2397 | LR 1.80e-05 | Tokens 153,600/500,000 | Speed 8,386 t/s\nStep   620 | Loss 16.4509 | LR 1.86e-05 | Tokens 158,720/500,000 | Speed 8,603 t/s\nStep   640 | Loss 12.4601 | LR 1.92e-05 | Tokens 163,840/500,000 | Speed 8,809 t/s\nStep   660 | Loss 15.5527 | LR 1.98e-05 | Tokens 168,960/500,000 | Speed 9,012 t/s\nStep   680 | Loss 17.4626 | LR 2.04e-05 | Tokens 174,080/500,000 | Speed 9,206 t/s\nStep   700 | Loss 17.6404 | LR 2.10e-05 | Tokens 179,200/500,000 | Speed 9,390 t/s\nStep   720 | Loss 14.0991 | LR 2.16e-05 | Tokens 184,320/500,000 | Speed 9,581 t/s\nStep   740 | Loss 14.7532 | LR 2.22e-05 | Tokens 189,440/500,000 | Speed 9,784 t/s\nStep   760 | Loss 10.3096 | LR 2.28e-05 | Tokens 194,560/500,000 | Speed 9,982 t/s\nStep   780 | Loss 12.7407 | LR 2.34e-05 | Tokens 199,680/500,000 | Speed 10,175 t/s\nStep   800 | Loss 10.7033 | LR 2.40e-05 | Tokens 204,800/500,000 | Speed 10,360 t/s\nStep   820 | Loss 9.7246 | LR 2.46e-05 | Tokens 209,920/500,000 | Speed 10,541 t/s\nStep   840 | Loss 9.6948 | LR 2.52e-05 | Tokens 215,040/500,000 | Speed 10,709 t/s\nStep   860 | Loss 9.9470 | LR 2.58e-05 | Tokens 220,160/500,000 | Speed 10,868 t/s\nStep   880 | Loss 10.0177 | LR 2.64e-05 | Tokens 225,280/500,000 | Speed 11,039 t/s\nStep   900 | Loss 9.6706 | LR 2.70e-05 | Tokens 230,400/500,000 | Speed 11,209 t/s\nStep   920 | Loss 10.0502 | LR 2.76e-05 | Tokens 235,520/500,000 | Speed 11,369 t/s\nStep   940 | Loss 9.3797 | LR 2.82e-05 | Tokens 240,640/500,000 | Speed 11,533 t/s\nStep   960 | Loss 9.7719 | LR 2.88e-05 | Tokens 245,760/500,000 | Speed 11,694 t/s\nStep   980 | Loss 9.5367 | LR 2.94e-05 | Tokens 250,880/500,000 | Speed 11,848 t/s\nStep  1000 | Loss 9.1944 | LR 3.00e-05 | Tokens 256,000/500,000 | Speed 11,999 t/s\nStep  1020 | Loss 9.4141 | LR 3.06e-05 | Tokens 261,120/500,000 | Speed 12,141 t/s\nStep  1040 | Loss 9.2337 | LR 3.12e-05 | Tokens 266,240/500,000 | Speed 12,275 t/s\nStep  1060 | Loss 8.9887 | LR 3.18e-05 | Tokens 271,360/500,000 | Speed 12,411 t/s\nStep  1080 | Loss 9.0312 | LR 3.24e-05 | Tokens 276,480/500,000 | Speed 12,555 t/s\nStep  1100 | Loss 9.1890 | LR 3.30e-05 | Tokens 281,600/500,000 | Speed 12,698 t/s\nStep  1120 | Loss 9.1462 | LR 3.36e-05 | Tokens 286,720/500,000 | Speed 12,841 t/s\nStep  1140 | Loss 9.8963 | LR 3.42e-05 | Tokens 291,840/500,000 | Speed 12,987 t/s\nStep  1160 | Loss 9.0405 | LR 3.48e-05 | Tokens 296,960/500,000 | Speed 13,130 t/s\nStep  1180 | Loss 9.3013 | LR 3.54e-05 | Tokens 302,080/500,000 | Speed 13,272 t/s\nStep  1200 | Loss 9.4511 | LR 3.60e-05 | Tokens 307,200/500,000 | Speed 13,394 t/s\nStep  1220 | Loss 9.0798 | LR 3.66e-05 | Tokens 312,320/500,000 | Speed 13,520 t/s\nStep  1240 | Loss 9.0876 | LR 3.72e-05 | Tokens 317,440/500,000 | Speed 13,643 t/s\nStep  1260 | Loss 9.1525 | LR 3.78e-05 | Tokens 322,560/500,000 | Speed 13,770 t/s\nStep  1280 | Loss 9.3065 | LR 3.84e-05 | Tokens 327,680/500,000 | Speed 13,906 t/s\nStep  1300 | Loss 8.8268 | LR 3.90e-05 | Tokens 332,800/500,000 | Speed 14,041 t/s\nStep  1320 | Loss 9.3032 | LR 3.96e-05 | Tokens 337,920/500,000 | Speed 14,156 t/s\nStep  1340 | Loss 10.0144 | LR 4.02e-05 | Tokens 343,040/500,000 | Speed 14,261 t/s\nStep  1360 | Loss 8.7757 | LR 4.08e-05 | Tokens 348,160/500,000 | Speed 14,363 t/s\nStep  1380 | Loss 9.1788 | LR 4.14e-05 | Tokens 353,280/500,000 | Speed 14,476 t/s\nStep  1400 | Loss 11.3438 | LR 4.20e-05 | Tokens 358,400/500,000 | Speed 14,607 t/s\nStep  1420 | Loss 9.0140 | LR 4.26e-05 | Tokens 363,520/500,000 | Speed 14,735 t/s\nStep  1440 | Loss 9.1620 | LR 4.32e-05 | Tokens 368,640/500,000 | Speed 14,855 t/s\nStep  1460 | Loss 8.9850 | LR 4.38e-05 | Tokens 373,760/500,000 | Speed 14,948 t/s\nStep  1480 | Loss 8.7001 | LR 4.44e-05 | Tokens 378,880/500,000 | Speed 15,046 t/s\nStep  1500 | Loss 8.8832 | LR 4.50e-05 | Tokens 384,000/500,000 | Speed 15,139 t/s\nStep  1520 | Loss 13.0455 | LR 4.56e-05 | Tokens 389,120/500,000 | Speed 15,239 t/s\nStep  1540 | Loss 9.1097 | LR 4.62e-05 | Tokens 394,240/500,000 | Speed 15,352 t/s\nStep  1560 | Loss 9.1069 | LR 4.68e-05 | Tokens 399,360/500,000 | Speed 15,467 t/s\nStep  1580 | Loss 9.4097 | LR 4.74e-05 | Tokens 404,480/500,000 | Speed 15,580 t/s\nStep  1600 | Loss 8.8141 | LR 4.80e-05 | Tokens 409,600/500,000 | Speed 15,697 t/s\nStep  1620 | Loss 9.3008 | LR 4.86e-05 | Tokens 414,720/500,000 | Speed 15,798 t/s\nStep  1640 | Loss 8.3707 | LR 4.92e-05 | Tokens 419,840/500,000 | Speed 15,899 t/s\nStep  1660 | Loss 8.6613 | LR 4.98e-05 | Tokens 424,960/500,000 | Speed 15,990 t/s\nStep  1680 | Loss 8.7236 | LR 5.04e-05 | Tokens 430,080/500,000 | Speed 16,080 t/s\nStep  1700 | Loss 9.1465 | LR 5.10e-05 | Tokens 435,200/500,000 | Speed 16,176 t/s\nStep  1720 | Loss 8.9476 | LR 5.16e-05 | Tokens 440,320/500,000 | Speed 16,275 t/s\nStep  1740 | Loss 10.6750 | LR 5.22e-05 | Tokens 445,440/500,000 | Speed 16,384 t/s\nStep  1760 | Loss 9.1882 | LR 5.28e-05 | Tokens 450,560/500,000 | Speed 16,491 t/s\nStep  1780 | Loss 8.3192 | LR 5.34e-05 | Tokens 455,680/500,000 | Speed 16,594 t/s\nStep  1800 | Loss 14.7435 | LR 5.40e-05 | Tokens 460,800/500,000 | Speed 16,689 t/s\nStep  1820 | Loss 9.4611 | LR 5.46e-05 | Tokens 465,920/500,000 | Speed 16,770 t/s\nStep  1840 | Loss 8.3955 | LR 5.52e-05 | Tokens 471,040/500,000 | Speed 16,852 t/s\nStep  1860 | Loss 8.5370 | LR 5.58e-05 | Tokens 476,160/500,000 | Speed 16,926 t/s\nStep  1880 | Loss 8.9748 | LR 5.64e-05 | Tokens 481,280/500,000 | Speed 17,011 t/s\nStep  1900 | Loss 8.8400 | LR 5.70e-05 | Tokens 486,400/500,000 | Speed 17,111 t/s\nStep  1920 | Loss 8.9684 | LR 5.76e-05 | Tokens 491,520/500,000 | Speed 17,199 t/s\nStep  1940 | Loss 10.2026 | LR 5.82e-05 | Tokens 496,640/500,000 | Speed 17,289 t/s\n\nReached target token count → stopping.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>gpu_mem_gb</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>▇▄█▇▆▄▄▄▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇███</td></tr><tr><td>step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>tokens_per_sec</td><td>▁▂▂▃▃▃▃▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇███████</td></tr><tr><td>tokens_seen</td><td>▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>gpu_mem_gb</td><td>0.14571</td></tr><tr><td>loss</td><td>10.20264</td></tr><tr><td>lr</td><td>6e-05</td></tr><tr><td>step</td><td>1940</td></tr><tr><td>tokens_per_sec</td><td>17289.1173</td></tr><tr><td>tokens_seen</td><td>496640</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">sweet-totem-8</strong> at: <a href='https://wandb.ai/tusharmishra802-/llama-demo/runs/y2715d2u' target=\"_blank\">https://wandb.ai/tusharmishra802-/llama-demo/runs/y2715d2u</a><br> View project at: <a href='https://wandb.ai/tusharmishra802-/llama-demo' target=\"_blank\">https://wandb.ai/tusharmishra802-/llama-demo</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20251113_080942-y2715d2u/logs</code>"},"metadata":{}},{"name":"stdout","text":"Training finished!\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"# Fixed generation function (top-k sampling)\ndef generate(model, tokenizer, prompt, max_new_tokens=50, top_k=50, temperature=0.8, pad_token_id=None):\n    if pad_token_id is None:\n        pad_token_id = tokenizer.eos_token_id\n    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(CFG[\"device\"])\n    \n    with torch.no_grad():\n        for _ in range(max_new_tokens):\n            with autocast(device_type=\"cuda\" if torch.cuda.is_available() else \"cpu\", dtype=torch.bfloat16, enabled=True):\n                logits = model(input_ids)[:, -1, :]  # Last position logits\n                logits = logits / temperature\n                # Top-k filtering\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                probs = torch.softmax(logits, dim=-1)\n                mask = logits[0] < v[0].min()  # 1D boolean [vocab]\n                probs[0][mask] = 0  # Set low-prob to 0\n                next_token = torch.multinomial(probs, num_samples=1)\n                input_ids = torch.cat([input_ids, next_token], dim=-1)\n                if next_token.item() == pad_token_id:\n                    break  # Stop at EOS\n    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n\n# Test prompts (web/edu themed)\nprompts = [\n    \"Web hosting is essential for\",\n    \"Machine learning models train on datasets like\",\n    \"A good developer should know\",\n    \"FineWeb-Edu is a filtered version of\",\n    \"The future of AI in education involves\",\n    \"Why do you think people follow me\"\n]\n\nprint(\"=== Generation Tests (Final Train Loss: 4.19 | PPL: {:.0f}) ===\".format(math.exp(4.19)))\nfor prompt in prompts:\n    with torch.no_grad():  # Per-prompt for safety\n        generated = generate(model, tokenizer, prompt, max_new_tokens=50, top_k=50, temperature=0.8)\n        continuation = generated[len(prompt):].strip()  # Continuation only\n        print(f\"\\nPrompt: {prompt}\")\n        print(f\"Output: {continuation}\")\n        print(\"-\" * 80)","metadata":{"id":"GKBRXQaewseU","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T08:17:22.080432Z","iopub.execute_input":"2025-11-13T08:17:22.081075Z","iopub.status.idle":"2025-11-13T08:17:22.951100Z","shell.execute_reply.started":"2025-11-13T08:17:22.081050Z","shell.execute_reply":"2025-11-13T08:17:22.950371Z"}},"outputs":[{"name":"stdout","text":"=== Generation Tests (Final Train Loss: 4.19 | PPL: 66) ===\n\nPrompt: Web hosting is essential for\nOutput: 0, they- in is the from have will who ( or that, to to time, ( the have it in5 time be I the one that, an-06 we. for you, This to a to7 so also that0\n--------------------------------------------------------------------------------\n\nPrompt: Machine learning models train on datasets like\nOutput: the of and have as. to of I to or be it\n to have for of time1 we is,\n you of a your\n for who in a71 is or it in in be and their’ to7 oration be\n--------------------------------------------------------------------------------\n\nPrompt: A good developer should know\nOutput: 5 and the for the that5\n to to ofes in1 we that your have I to\n to a for7 be or them the one ofation from is, they theires1 in1. their one15 it 0\n--------------------------------------------------------------------------------\n\nPrompt: FineWeb-Edu is a filtered version of\nOutput: . of it1 for can which in that one is  of so the of the that\n we. to canThe they I ands which-5 that ,71 is can thats9,. be at-ations and to\n--------------------------------------------------------------------------------\n\nPrompt: The future of AI in education involves\nOutput: This to the on it in have to to a your that and who from their their you a is, be for more is of it the I which0ation as9 a This- an one,  their a This time be as ( in at\n--------------------------------------------------------------------------------\n\nPrompt: Why do you think people follow me\nOutput: ,, at it. is’ time be of, an5 to\n\n to and. of to0 and their can, an isation an that be be they and that at you at and s1 to a to to,- (\n--------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}