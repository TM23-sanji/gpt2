{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT-2 FineWeb-Edu Training Visualizations\n",
        "\n",
        "This notebook provides comprehensive visualizations for training metrics including:\n",
        "- Training and validation loss curves\n",
        "- Perplexity over time\n",
        "- Learning rate schedule\n",
        "- Token processing statistics\n",
        "- Training progress tracking\n",
        "\n",
        "## Data Sources\n",
        "- WandB API (if available)\n",
        "- Local checkpoint files\n",
        "- Synthetic data for demonstration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "# !pip install wandb matplotlib seaborn numpy pandas torch transformers\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import wandb\n",
        "import os\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 6)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Training Data from WandB\n",
        "\n",
        "Load training metrics from WandB API. If WandB is not available, we'll generate synthetic data based on expected training behavior.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def load_wandb_data(project_name=\"gpt-fineweb-demo\", run_id=None, use_api=True):\n",
        "    \"\"\"\n",
        "    Load training data from WandB.\n",
        "    If WandB is not available, generate synthetic data based on expected metrics.\n",
        "    \"\"\"\n",
        "    if use_api:\n",
        "        try:\n",
        "            api = wandb.Api()\n",
        "            if run_id:\n",
        "                run = api.run(f\"{project_name}/{run_id}\")\n",
        "            else:\n",
        "                # Get the latest run\n",
        "                runs = api.runs(project_name)\n",
        "                if len(runs) == 0:\n",
        "                    raise ValueError(f\"No runs found in project {project_name}\")\n",
        "                run = runs[0]\n",
        "            \n",
        "            # Get history\n",
        "            history = run.history()\n",
        "            return history, run\n",
        "        except Exception as e:\n",
        "            print(f\"WandB API error: {e}\")\n",
        "            print(\"Generating synthetic data instead...\")\n",
        "            use_api = False\n",
        "    \n",
        "    if not use_api:\n",
        "        # Generate synthetic training data based on expected behavior\n",
        "        # Final stats: Train loss 4.19 (PPL 66), Val loss 6.16 (PPL 478) at 200M tokens\n",
        "        max_tokens = 200_000_000\n",
        "        log_interval = 20\n",
        "        tokens_per_step = 16 * 256  # batch_size * context_length\n",
        "        \n",
        "        steps = np.arange(0, max_tokens // tokens_per_step, log_interval)\n",
        "        tokens_seen = steps * tokens_per_step\n",
        "        \n",
        "        # Training loss: starts at ~10.8, decays to 4.19\n",
        "        train_loss = 10.8 * np.exp(-tokens_seen / 50_000_000) + 4.19 * (1 - np.exp(-tokens_seen / 50_000_000))\n",
        "        train_loss += np.random.normal(0, 0.1, len(train_loss))  # Add noise\n",
        "        \n",
        "        # Validation loss: higher, starts at ~10, decays to 6.16\n",
        "        val_steps = np.arange(0, len(steps), max(1, len(steps) // 10))  # Less frequent\n",
        "        val_tokens = val_steps * tokens_per_step * log_interval\n",
        "        val_loss = 10.0 * np.exp(-val_tokens / 80_000_000) + 6.16 * (1 - np.exp(-val_tokens / 80_000_000))\n",
        "        val_loss += np.random.normal(0, 0.2, len(val_loss))\n",
        "        \n",
        "        # Learning rate: warmup then cosine decay\n",
        "        warmup_tokens = 2_000_000\n",
        "        peak_lr = 3e-4\n",
        "        final_lr = 6e-5\n",
        "        \n",
        "        lr = np.zeros_like(tokens_seen)\n",
        "        for i, tok in enumerate(tokens_seen):\n",
        "            if tok <= warmup_tokens:\n",
        "                lr[i] = peak_lr * (tok / warmup_tokens)\n",
        "            else:\n",
        "                progress = (tok - warmup_tokens) / (max_tokens - warmup_tokens)\n",
        "                cosine = 0.5 * (1.0 + np.cos(np.pi * min(1.0, progress)))\n",
        "                lr[i] = final_lr + (peak_lr - final_lr) * cosine\n",
        "        \n",
        "        # Token processing speed: starts slow, stabilizes\n",
        "        tokens_per_sec = 5000 + 2000 * (1 - np.exp(-tokens_seen / 20_000_000))\n",
        "        tokens_per_sec += np.random.normal(0, 200, len(tokens_per_sec))\n",
        "        \n",
        "        # Create DataFrame\n",
        "        data = pd.DataFrame({\n",
        "            'step': steps,\n",
        "            'tokens_seen': tokens_seen,\n",
        "            'loss': train_loss,\n",
        "            'lr': lr,\n",
        "            'tokens_per_sec': tokens_per_sec,\n",
        "        })\n",
        "        \n",
        "        val_data = pd.DataFrame({\n",
        "            'step': val_steps * log_interval,\n",
        "            'tokens_seen': val_tokens,\n",
        "            'val_loss': val_loss,\n",
        "        })\n",
        "        \n",
        "        return data, val_data, None\n",
        "    \n",
        "    return None, None, None\n",
        "\n",
        "# Load data\n",
        "train_data, val_data, wandb_run = load_wandb_data(use_api=False)  # Set to True if WandB is available\n",
        "\n",
        "print(f\"Loaded {len(train_data)} training data points\")\n",
        "if val_data is not None:\n",
        "    print(f\"Loaded {len(val_data)} validation data points\")\n",
        "print(\"\\nFirst few rows:\")\n",
        "print(train_data.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Training and Validation Loss Curves\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_loss_curves(train_data, val_data=None, tokens_or_steps='tokens'):\n",
        "    \"\"\"\n",
        "    Plot training and validation loss curves.\n",
        "    \n",
        "    Args:\n",
        "        train_data: DataFrame with training metrics\n",
        "        val_data: DataFrame with validation metrics (optional)\n",
        "        tokens_or_steps: 'tokens' or 'steps' for x-axis\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    x_train = train_data['tokens_seen'] if tokens_or_steps == 'tokens' else train_data['step']\n",
        "    x_label = 'Tokens Seen' if tokens_or_steps == 'tokens' else 'Training Steps'\n",
        "    \n",
        "    # Plot 1: Linear scale\n",
        "    ax1 = axes[0]\n",
        "    ax1.plot(x_train, train_data['loss'], label='Training Loss', color='#2E86AB', linewidth=2, alpha=0.8)\n",
        "    if val_data is not None:\n",
        "        x_val = val_data['tokens_seen'] if tokens_or_steps == 'tokens' else val_data['step']\n",
        "        ax1.plot(x_val, val_data['val_loss'], label='Validation Loss', color='#A23B72', \n",
        "                linewidth=2, alpha=0.8, marker='o', markersize=4)\n",
        "    \n",
        "    ax1.set_xlabel(x_label, fontsize=12)\n",
        "    ax1.set_ylabel('Loss', fontsize=12)\n",
        "    ax1.set_title('Training and Validation Loss', fontsize=14, fontweight='bold')\n",
        "    ax1.legend(fontsize=11)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Format x-axis for tokens\n",
        "    if tokens_or_steps == 'tokens':\n",
        "        ax1.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e6:.1f}M'))\n",
        "    \n",
        "    # Plot 2: Log scale (better for viewing decay)\n",
        "    ax2 = axes[1]\n",
        "    ax2.semilogy(x_train, train_data['loss'], label='Training Loss', color='#2E86AB', linewidth=2, alpha=0.8)\n",
        "    if val_data is not None:\n",
        "        ax2.semilogy(x_val, val_data['val_loss'], label='Validation Loss', color='#A23B72', \n",
        "                    linewidth=2, alpha=0.8, marker='o', markersize=4)\n",
        "    \n",
        "    ax2.set_xlabel(x_label, fontsize=12)\n",
        "    ax2.set_ylabel('Loss (Log Scale)', fontsize=12)\n",
        "    ax2.set_title('Training and Validation Loss (Log Scale)', fontsize=14, fontweight='bold')\n",
        "    ax2.legend(fontsize=11)\n",
        "    ax2.grid(True, alpha=0.3, which='both')\n",
        "    \n",
        "    if tokens_or_steps == 'tokens':\n",
        "        ax2.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e6:.1f}M'))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# Plot loss curves\n",
        "fig = plot_loss_curves(train_data, val_data, tokens_or_steps='tokens')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_perplexity(train_data, val_data=None, tokens_or_steps='tokens'):\n",
        "    \"\"\"\n",
        "    Plot perplexity over training.\n",
        "    Perplexity = exp(loss)\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    x_train = train_data['tokens_seen'] if tokens_or_steps == 'tokens' else train_data['step']\n",
        "    x_label = 'Tokens Seen' if tokens_or_steps == 'tokens' else 'Training Steps'\n",
        "    \n",
        "    # Calculate perplexity\n",
        "    train_ppl = np.exp(train_data['loss'])\n",
        "    if val_data is not None:\n",
        "        val_ppl = np.exp(val_data['val_loss'])\n",
        "        x_val = val_data['tokens_seen'] if tokens_or_steps == 'tokens' else val_data['step']\n",
        "    \n",
        "    # Plot 1: Linear scale\n",
        "    ax1 = axes[0]\n",
        "    ax1.plot(x_train, train_ppl, label='Train PPL', color='#06A77D', linewidth=2, alpha=0.8)\n",
        "    if val_data is not None:\n",
        "        ax1.plot(x_val, val_ppl, label='Val PPL', color='#F18F01', \n",
        "                linewidth=2, alpha=0.8, marker='o', markersize=4)\n",
        "    \n",
        "    ax1.set_xlabel(x_label, fontsize=12)\n",
        "    ax1.set_ylabel('Perplexity', fontsize=12)\n",
        "    ax1.set_title('Training and Validation Perplexity', fontsize=14, fontweight='bold')\n",
        "    ax1.legend(fontsize=11)\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add final values as text\n",
        "    final_train_ppl = train_ppl.iloc[-1]\n",
        "    ax1.axhline(y=final_train_ppl, color='#06A77D', linestyle='--', alpha=0.5, linewidth=1)\n",
        "    ax1.text(x_train.iloc[-1] * 0.7, final_train_ppl * 1.1, \n",
        "             f'Final Train PPL: {final_train_ppl:.1f}', fontsize=10, color='#06A77D')\n",
        "    \n",
        "    if val_data is not None:\n",
        "        final_val_ppl = val_ppl.iloc[-1]\n",
        "        ax1.axhline(y=final_val_ppl, color='#F18F01', linestyle='--', alpha=0.5, linewidth=1)\n",
        "        ax1.text(x_val.iloc[-1] * 0.7, final_val_ppl * 1.1, \n",
        "                 f'Final Val PPL: {final_val_ppl:.1f}', fontsize=10, color='#F18F01')\n",
        "    \n",
        "    if tokens_or_steps == 'tokens':\n",
        "        ax1.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e6:.1f}M'))\n",
        "    \n",
        "    # Plot 2: Log scale\n",
        "    ax2 = axes[1]\n",
        "    ax2.semilogy(x_train, train_ppl, label='Train PPL', color='#06A77D', linewidth=2, alpha=0.8)\n",
        "    if val_data is not None:\n",
        "        ax2.semilogy(x_val, val_ppl, label='Val PPL', color='#F18F01', \n",
        "                    linewidth=2, alpha=0.8, marker='o', markersize=4)\n",
        "    \n",
        "    ax2.set_xlabel(x_label, fontsize=12)\n",
        "    ax2.set_ylabel('Perplexity (Log Scale)', fontsize=12)\n",
        "    ax2.set_title('Training and Validation Perplexity (Log Scale)', fontsize=14, fontweight='bold')\n",
        "    ax2.legend(fontsize=11)\n",
        "    ax2.grid(True, alpha=0.3, which='both')\n",
        "    \n",
        "    if tokens_or_steps == 'tokens':\n",
        "        ax2.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e6:.1f}M'))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# Plot perplexity\n",
        "fig = plot_perplexity(train_data, val_data, tokens_or_steps='tokens')\n",
        "plt.show()\n",
        "\n",
        "# Print final metrics\n",
        "print(f\"\\n=== Final Training Metrics ===\")\n",
        "print(f\"Final Train Loss: {train_data['loss'].iloc[-1]:.4f}\")\n",
        "print(f\"Final Train PPL: {np.exp(train_data['loss'].iloc[-1]):.2f}\")\n",
        "if val_data is not None:\n",
        "    print(f\"Final Val Loss: {val_data['val_loss'].iloc[-1]:.4f}\")\n",
        "    print(f\"Final Val PPL: {np.exp(val_data['val_loss'].iloc[-1]):.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Learning Rate Schedule Visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_learning_rate_schedule(train_data, tokens_or_steps='tokens'):\n",
        "    \"\"\"\n",
        "    Plot learning rate schedule over training.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(12, 6))\n",
        "    \n",
        "    x = train_data['tokens_seen'] if tokens_or_steps == 'tokens' else train_data['step']\n",
        "    x_label = 'Tokens Seen' if tokens_or_steps == 'tokens' else 'Training Steps'\n",
        "    \n",
        "    ax.plot(x, train_data['lr'], color='#C73E1D', linewidth=2, alpha=0.8)\n",
        "    ax.set_xlabel(x_label, fontsize=12)\n",
        "    ax.set_ylabel('Learning Rate', fontsize=12)\n",
        "    ax.set_title('Learning Rate Schedule (Warmup + Cosine Decay)', fontsize=14, fontweight='bold')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_yscale('log')  # Log scale for LR\n",
        "    \n",
        "    # Annotate key points\n",
        "    peak_lr_idx = train_data['lr'].idxmax()\n",
        "    peak_lr = train_data['lr'].loc[peak_lr_idx]\n",
        "    peak_x = x.loc[peak_lr_idx]\n",
        "    \n",
        "    ax.plot(peak_x, peak_lr, 'ro', markersize=8, label=f'Peak LR: {peak_lr:.2e}')\n",
        "    ax.axvline(x=peak_x, color='r', linestyle='--', alpha=0.5, linewidth=1)\n",
        "    \n",
        "    final_lr = train_data['lr'].iloc[-1]\n",
        "    final_x = x.iloc[-1]\n",
        "    ax.plot(final_x, final_lr, 'go', markersize=8, label=f'Final LR: {final_lr:.2e}')\n",
        "    \n",
        "    ax.legend(fontsize=11)\n",
        "    \n",
        "    if tokens_or_steps == 'tokens':\n",
        "        ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e6:.1f}M'))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# Plot LR schedule\n",
        "fig = plot_learning_rate_schedule(train_data, tokens_or_steps='tokens')\n",
        "plt.show()\n",
        "\n",
        "# Print LR stats\n",
        "print(f\"\\n=== Learning Rate Statistics ===\")\n",
        "print(f\"Peak LR: {train_data['lr'].max():.2e}\")\n",
        "print(f\"Final LR: {train_data['lr'].iloc[-1]:.2e}\")\n",
        "print(f\"LR Ratio (final/peak): {train_data['lr'].iloc[-1] / train_data['lr'].max():.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Token Processing Speed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_token_processing_speed(train_data, tokens_or_steps='tokens'):\n",
        "    \"\"\"\n",
        "    Plot token processing speed over training.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    x = train_data['tokens_seen'] if tokens_or_steps == 'tokens' else train_data['step']\n",
        "    x_label = 'Tokens Seen' if tokens_or_steps == 'tokens' else 'Training Steps'\n",
        "    \n",
        "    # Plot 1: Speed over time\n",
        "    ax1 = axes[0]\n",
        "    ax1.plot(x, train_data['tokens_per_sec'], color='#6A4C93', linewidth=2, alpha=0.8)\n",
        "    ax1.set_xlabel(x_label, fontsize=12)\n",
        "    ax1.set_ylabel('Tokens per Second', fontsize=12)\n",
        "    ax1.set_title('Token Processing Speed', fontsize=14, fontweight='bold')\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Add average line\n",
        "    avg_speed = train_data['tokens_per_sec'].mean()\n",
        "    ax1.axhline(y=avg_speed, color='r', linestyle='--', alpha=0.7, linewidth=2, \n",
        "                label=f'Average: {avg_speed:,.0f} tok/s')\n",
        "    ax1.legend(fontsize=11)\n",
        "    \n",
        "    if tokens_or_steps == 'tokens':\n",
        "        ax1.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e6:.1f}M'))\n",
        "    \n",
        "    # Plot 2: Speed distribution\n",
        "    ax2 = axes[1]\n",
        "    ax2.hist(train_data['tokens_per_sec'], bins=50, color='#6A4C93', alpha=0.7, edgecolor='black')\n",
        "    ax2.axvline(x=avg_speed, color='r', linestyle='--', linewidth=2, label=f'Mean: {avg_speed:,.0f}')\n",
        "    ax2.axvline(x=train_data['tokens_per_sec'].median(), color='g', linestyle='--', linewidth=2, \n",
        "                label=f'Median: {train_data[\"tokens_per_sec\"].median():,.0f}')\n",
        "    ax2.set_xlabel('Tokens per Second', fontsize=12)\n",
        "    ax2.set_ylabel('Frequency', fontsize=12)\n",
        "    ax2.set_title('Token Processing Speed Distribution', fontsize=14, fontweight='bold')\n",
        "    ax2.legend(fontsize=11)\n",
        "    ax2.grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# Plot token processing speed\n",
        "if 'tokens_per_sec' in train_data.columns:\n",
        "    fig = plot_token_processing_speed(train_data, tokens_or_steps='tokens')\n",
        "    plt.show()\n",
        "    \n",
        "    print(f\"\\n=== Token Processing Statistics ===\")\n",
        "    print(f\"Average Speed: {train_data['tokens_per_sec'].mean():,.0f} tokens/sec\")\n",
        "    print(f\"Median Speed: {train_data['tokens_per_sec'].median():,.0f} tokens/sec\")\n",
        "    print(f\"Max Speed: {train_data['tokens_per_sec'].max():,.0f} tokens/sec\")\n",
        "    print(f\"Min Speed: {train_data['tokens_per_sec'].min():,.0f} tokens/sec\")\n",
        "else:\n",
        "    print(\"Token processing speed data not available in the dataset.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Comprehensive Training Dashboard\n",
        "\n",
        "Combine all metrics into a single comprehensive dashboard.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_training_dashboard(train_data, val_data=None, tokens_or_steps='tokens'):\n",
        "    \"\"\"\n",
        "    Create a comprehensive training dashboard with all key metrics.\n",
        "    \"\"\"\n",
        "    fig = plt.figure(figsize=(18, 12))\n",
        "    gs = fig.add_gridspec(3, 3, hspace=0.3, wspace=0.3)\n",
        "    \n",
        "    x = train_data['tokens_seen'] if tokens_or_steps == 'tokens' else train_data['step']\n",
        "    x_label = 'Tokens Seen (M)' if tokens_or_steps == 'tokens' else 'Training Steps'\n",
        "    \n",
        "    # 1. Loss curves (top left, spans 2 columns)\n",
        "    ax1 = fig.add_subplot(gs[0, :2])\n",
        "    ax1.plot(x, train_data['loss'], label='Training Loss', color='#2E86AB', linewidth=2)\n",
        "    if val_data is not None:\n",
        "        x_val = val_data['tokens_seen'] if tokens_or_steps == 'tokens' else val_data['step']\n",
        "        ax1.plot(x_val, val_data['val_loss'], label='Validation Loss', color='#A23B72', \n",
        "                linewidth=2, marker='o', markersize=4)\n",
        "    ax1.set_xlabel(x_label, fontsize=11)\n",
        "    ax1.set_ylabel('Loss', fontsize=11)\n",
        "    ax1.set_title('Training and Validation Loss', fontsize=12, fontweight='bold')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    if tokens_or_steps == 'tokens':\n",
        "        ax1.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e6:.1f}'))\n",
        "    \n",
        "    # 2. Perplexity (top right)\n",
        "    ax2 = fig.add_subplot(gs[0, 2])\n",
        "    train_ppl = np.exp(train_data['loss'])\n",
        "    ax2.plot(x, train_ppl, label='Train PPL', color='#06A77D', linewidth=2)\n",
        "    if val_data is not None:\n",
        "        val_ppl = np.exp(val_data['val_loss'])\n",
        "        ax2.plot(x_val, val_ppl, label='Val PPL', color='#F18F01', linewidth=2, marker='o', markersize=4)\n",
        "    ax2.set_xlabel(x_label, fontsize=11)\n",
        "    ax2.set_ylabel('Perplexity', fontsize=11)\n",
        "    ax2.set_title('Perplexity', fontsize=12, fontweight='bold')\n",
        "    ax2.legend(fontsize=9)\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    if tokens_or_steps == 'tokens':\n",
        "        ax2.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e6:.1f}'))\n",
        "    \n",
        "    # 3. Learning rate (middle left)\n",
        "    ax3 = fig.add_subplot(gs[1, 0])\n",
        "    ax3.plot(x, train_data['lr'], color='#C73E1D', linewidth=2)\n",
        "    ax3.set_xlabel(x_label, fontsize=11)\n",
        "    ax3.set_ylabel('Learning Rate', fontsize=11)\n",
        "    ax3.set_title('Learning Rate Schedule', fontsize=12, fontweight='bold')\n",
        "    ax3.set_yscale('log')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    if tokens_or_steps == 'tokens':\n",
        "        ax3.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e6:.1f}'))\n",
        "    \n",
        "    # 4. Token processing speed (middle center)\n",
        "    if 'tokens_per_sec' in train_data.columns:\n",
        "        ax4 = fig.add_subplot(gs[1, 1])\n",
        "        ax4.plot(x, train_data['tokens_per_sec'], color='#6A4C93', linewidth=2)\n",
        "        ax4.axhline(y=train_data['tokens_per_sec'].mean(), color='r', linestyle='--', \n",
        "                   label=f'Avg: {train_data[\"tokens_per_sec\"].mean():,.0f}')\n",
        "        ax4.set_xlabel(x_label, fontsize=11)\n",
        "        ax4.set_ylabel('Tokens/sec', fontsize=11)\n",
        "        ax4.set_title('Processing Speed', fontsize=12, fontweight='bold')\n",
        "        ax4.legend(fontsize=9)\n",
        "        ax4.grid(True, alpha=0.3)\n",
        "        if tokens_or_steps == 'tokens':\n",
        "            ax4.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e6:.1f}'))\n",
        "    \n",
        "    # 5. Loss vs Learning Rate (middle right)\n",
        "    ax5 = fig.add_subplot(gs[1, 2])\n",
        "    scatter = ax5.scatter(train_data['lr'], train_data['loss'], \n",
        "                         c=x, cmap='viridis', alpha=0.6, s=20)\n",
        "    ax5.set_xlabel('Learning Rate', fontsize=11)\n",
        "    ax5.set_ylabel('Loss', fontsize=11)\n",
        "    ax5.set_title('Loss vs Learning Rate', fontsize=12, fontweight='bold')\n",
        "    ax5.set_xscale('log')\n",
        "    ax5.grid(True, alpha=0.3)\n",
        "    plt.colorbar(scatter, ax=ax5, label=x_label.replace(' (M)', ''))\n",
        "    \n",
        "    # 6. Metrics summary table (bottom, spans all columns)\n",
        "    ax6 = fig.add_subplot(gs[2, :])\n",
        "    ax6.axis('off')\n",
        "    \n",
        "    # Create summary statistics\n",
        "    summary_data = {\n",
        "        'Metric': [\n",
        "            'Final Train Loss',\n",
        "            'Final Train PPL',\n",
        "            'Final Val Loss' if val_data is not None else 'N/A',\n",
        "            'Final Val PPL' if val_data is not None else 'N/A',\n",
        "            'Peak Learning Rate',\n",
        "            'Final Learning Rate',\n",
        "            'Avg Processing Speed' if 'tokens_per_sec' in train_data.columns else 'N/A',\n",
        "            'Total Tokens Processed',\n",
        "        ],\n",
        "        'Value': [\n",
        "            f\"{train_data['loss'].iloc[-1]:.4f}\",\n",
        "            f\"{np.exp(train_data['loss'].iloc[-1]):.2f}\",\n",
        "            f\"{val_data['val_loss'].iloc[-1]:.4f}\" if val_data is not None else 'N/A',\n",
        "            f\"{np.exp(val_data['val_loss'].iloc[-1]):.2f}\" if val_data is not None else 'N/A',\n",
        "            f\"{train_data['lr'].max():.2e}\",\n",
        "            f\"{train_data['lr'].iloc[-1]:.2e}\",\n",
        "            f\"{train_data['tokens_per_sec'].mean():,.0f} tok/s\" if 'tokens_per_sec' in train_data.columns else 'N/A',\n",
        "            f\"{train_data['tokens_seen'].iloc[-1]:,}\",\n",
        "        ]\n",
        "    }\n",
        "    \n",
        "    summary_df = pd.DataFrame(summary_data)\n",
        "    table = ax6.table(cellText=summary_df.values, colLabels=summary_df.columns,\n",
        "                     cellLoc='left', loc='center', bbox=[0, 0, 1, 1])\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(11)\n",
        "    table.scale(1, 2)\n",
        "    ax6.set_title('Training Summary Statistics', fontsize=14, fontweight='bold', pad=20)\n",
        "    \n",
        "    plt.suptitle('GPT-2 FineWeb-Edu Training Dashboard', fontsize=16, fontweight='bold', y=0.995)\n",
        "    return fig\n",
        "\n",
        "# Create comprehensive dashboard\n",
        "fig = create_training_dashboard(train_data, val_data, tokens_or_steps='tokens')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
