{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPT-2 FineWeb-Edu Model Comparison & Analysis\n",
        "\n",
        "This notebook provides tools for comparing multiple training runs, checkpoints, and model configurations:\n",
        "- Multi-run loss comparison\n",
        "- Checkpoint comparison\n",
        "- Hyperparameter impact analysis\n",
        "- Model performance benchmarks\n",
        "- A/B testing visualizations\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "# !pip install wandb matplotlib seaborn numpy pandas torch transformers\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import wandb\n",
        "import os\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "\n",
        "# Set style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (14, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "\n",
        "print(\"Libraries imported successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load Multiple Training Runs\n",
        "\n",
        "Load data from multiple WandB runs or generate synthetic runs for comparison.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_comparison_runs(n_runs=3, use_wandb=False, project_name=\"gpt-fineweb-demo\", run_ids=None):\n",
        "    \"\"\"\n",
        "    Generate or load multiple training runs for comparison.\n",
        "    \n",
        "    Args:\n",
        "        n_runs: Number of runs to generate/load\n",
        "        use_wandb: Whether to use WandB API\n",
        "        project_name: WandB project name\n",
        "        run_ids: List of WandB run IDs (if None, uses latest runs)\n",
        "    \"\"\"\n",
        "    runs_data = []\n",
        "    \n",
        "    if use_wandb:\n",
        "        try:\n",
        "            api = wandb.Api()\n",
        "            if run_ids:\n",
        "                runs = [api.run(f\"{project_name}/{rid}\") for rid in run_ids]\n",
        "            else:\n",
        "                runs = list(api.runs(project_name))[:n_runs]\n",
        "            \n",
        "            for run in runs:\n",
        "                history = run.history()\n",
        "                history['run_name'] = run.name\n",
        "                history['run_id'] = run.id\n",
        "                runs_data.append(history)\n",
        "            return runs_data\n",
        "        except Exception as e:\n",
        "            print(f\"WandB API error: {e}\")\n",
        "            print(\"Generating synthetic runs instead...\")\n",
        "            use_wandb = False\n",
        "    \n",
        "    if not use_wandb:\n",
        "        # Generate synthetic runs with variations\n",
        "        max_tokens = 200_000_000\n",
        "        log_interval = 20\n",
        "        tokens_per_step = 16 * 256\n",
        "        \n",
        "        steps = np.arange(0, max_tokens // tokens_per_step, log_interval)\n",
        "        tokens_seen = steps * tokens_per_step\n",
        "        \n",
        "        # Run variations: different learning rates, batch sizes, etc.\n",
        "        configs = [\n",
        "            {'name': 'Baseline (LR=3e-4)', 'lr_peak': 3e-4, 'lr_final': 6e-5, 'batch': 16, 'decay': 50e6},\n",
        "            {'name': 'High LR (LR=5e-4)', 'lr_peak': 5e-4, 'lr_final': 8e-5, 'batch': 16, 'decay': 50e6},\n",
        "            {'name': 'Low LR (LR=1e-4)', 'lr_peak': 1e-4, 'lr_final': 2e-5, 'batch': 16, 'decay': 50e6},\n",
        "        ][:n_runs]\n",
        "        \n",
        "        for config in configs:\n",
        "            # Training loss with config-dependent decay\n",
        "            base_loss = 10.8 * np.exp(-tokens_seen / config['decay']) + 4.19 * (1 - np.exp(-tokens_seen / config['decay']))\n",
        "            # Add variation based on LR (higher LR might converge faster but less stable)\n",
        "            if config['lr_peak'] > 3e-4:\n",
        "                base_loss += 0.2 * np.exp(-tokens_seen / 30e6)  # Slightly higher early loss\n",
        "            elif config['lr_peak'] < 3e-4:\n",
        "                base_loss += 0.3 * np.exp(-tokens_seen / 70e6)  # Slower convergence\n",
        "            base_loss += np.random.normal(0, 0.1, len(base_loss))\n",
        "            \n",
        "            # Learning rate schedule\n",
        "            warmup_tokens = 2_000_000\n",
        "            lr = np.zeros_like(tokens_seen)\n",
        "            for i, tok in enumerate(tokens_seen):\n",
        "                if tok <= warmup_tokens:\n",
        "                    lr[i] = config['lr_peak'] * (tok / warmup_tokens)\n",
        "                else:\n",
        "                    progress = (tok - warmup_tokens) / (max_tokens - warmup_tokens)\n",
        "                    cosine = 0.5 * (1.0 + np.cos(np.pi * min(1.0, progress)))\n",
        "                    lr[i] = config['lr_final'] + (config['lr_peak'] - config['lr_final']) * cosine\n",
        "            \n",
        "            # Token processing speed (slight variation)\n",
        "            base_speed = 6000 + np.random.normal(0, 500)\n",
        "            tokens_per_sec = base_speed + 1000 * (1 - np.exp(-tokens_seen / 20_000_000))\n",
        "            tokens_per_sec += np.random.normal(0, 200, len(tokens_per_sec))\n",
        "            \n",
        "            data = pd.DataFrame({\n",
        "                'step': steps,\n",
        "                'tokens_seen': tokens_seen,\n",
        "                'loss': base_loss,\n",
        "                'lr': lr,\n",
        "                'tokens_per_sec': tokens_per_sec,\n",
        "                'run_name': config['name'],\n",
        "                'run_id': f\"run_{len(runs_data)}\",\n",
        "            })\n",
        "            \n",
        "            runs_data.append(data)\n",
        "    \n",
        "    return runs_data\n",
        "\n",
        "# Generate comparison runs\n",
        "runs_data = generate_comparison_runs(n_runs=3, use_wandb=False)\n",
        "\n",
        "print(f\"Loaded {len(runs_data)} runs for comparison:\")\n",
        "for i, run in enumerate(runs_data):\n",
        "    print(f\"  {i+1}. {run['run_name'].iloc[0]} ({len(run)} data points)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Multi-Run Loss Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_multi_run_comparison(runs_data, metric='loss', tokens_or_steps='tokens'):\n",
        "    \"\"\"\n",
        "    Plot comparison of multiple runs for a given metric.\n",
        "    \n",
        "    Args:\n",
        "        runs_data: List of DataFrames, one per run\n",
        "        metric: Metric to compare ('loss', 'lr', 'tokens_per_sec', etc.)\n",
        "        tokens_or_steps: 'tokens' or 'steps' for x-axis\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, len(runs_data)))\n",
        "    \n",
        "    for i, run_data in enumerate(runs_data):\n",
        "        run_name = run_data['run_name'].iloc[0]\n",
        "        x = run_data['tokens_seen'] if tokens_or_steps == 'tokens' else run_data['step']\n",
        "        \n",
        "        # Plot 1: Linear scale\n",
        "        axes[0].plot(x, run_data[metric], label=run_name, color=colors[i], \n",
        "                    linewidth=2, alpha=0.8)\n",
        "        \n",
        "        # Plot 2: Log scale (if loss)\n",
        "        if metric == 'loss':\n",
        "            axes[1].semilogy(x, run_data[metric], label=run_name, color=colors[i], \n",
        "                           linewidth=2, alpha=0.8)\n",
        "        else:\n",
        "            axes[1].plot(x, run_data[metric], label=run_name, color=colors[i], \n",
        "                        linewidth=2, alpha=0.8)\n",
        "    \n",
        "    x_label = 'Tokens Seen' if tokens_or_steps == 'tokens' else 'Training Steps'\n",
        "    y_label = metric.replace('_', ' ').title()\n",
        "    \n",
        "    axes[0].set_xlabel(x_label, fontsize=12)\n",
        "    axes[0].set_ylabel(y_label, fontsize=12)\n",
        "    axes[0].set_title(f'{y_label} Comparison (Linear Scale)', fontsize=14, fontweight='bold')\n",
        "    axes[0].legend(fontsize=10)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    if tokens_or_steps == 'tokens':\n",
        "        axes[0].xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e6:.1f}M'))\n",
        "    \n",
        "    axes[1].set_xlabel(x_label, fontsize=12)\n",
        "    axes[1].set_ylabel(y_label + (' (Log Scale)' if metric == 'loss' else ''), fontsize=12)\n",
        "    axes[1].set_title(f'{y_label} Comparison ({\"Log\" if metric == \"loss\" else \"Linear\"} Scale)', \n",
        "                     fontsize=14, fontweight='bold')\n",
        "    axes[1].legend(fontsize=10)\n",
        "    axes[1].grid(True, alpha=0.3, which='both' if metric == 'loss' else 'major')\n",
        "    if tokens_or_steps == 'tokens':\n",
        "        axes[1].xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e6:.1f}M'))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# Plot loss comparison\n",
        "fig = plot_multi_run_comparison(runs_data, metric='loss', tokens_or_steps='tokens')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Final Metrics Comparison Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_comparison_table(runs_data):\n",
        "    \"\"\"\n",
        "    Create a comparison table of final metrics for all runs.\n",
        "    \"\"\"\n",
        "    comparison_data = []\n",
        "    \n",
        "    for run_data in runs_data:\n",
        "        run_name = run_data['run_name'].iloc[0]\n",
        "        final_loss = run_data['loss'].iloc[-1]\n",
        "        final_ppl = np.exp(final_loss)\n",
        "        final_lr = run_data['lr'].iloc[-1]\n",
        "        peak_lr = run_data['lr'].max()\n",
        "        \n",
        "        if 'tokens_per_sec' in run_data.columns:\n",
        "            avg_speed = run_data['tokens_per_sec'].mean()\n",
        "        else:\n",
        "            avg_speed = None\n",
        "        \n",
        "        total_tokens = run_data['tokens_seen'].iloc[-1]\n",
        "        \n",
        "        comparison_data.append({\n",
        "            'Run Name': run_name,\n",
        "            'Final Loss': f\"{final_loss:.4f}\",\n",
        "            'Final PPL': f\"{final_ppl:.2f}\",\n",
        "            'Peak LR': f\"{peak_lr:.2e}\",\n",
        "            'Final LR': f\"{final_lr:.2e}\",\n",
        "            'Avg Speed (tok/s)': f\"{avg_speed:,.0f}\" if avg_speed else 'N/A',\n",
        "            'Total Tokens': f\"{total_tokens:,}\",\n",
        "        })\n",
        "    \n",
        "    comparison_df = pd.DataFrame(comparison_data)\n",
        "    \n",
        "    # Create visualization\n",
        "    fig, ax = plt.subplots(figsize=(14, len(comparison_data) * 0.8 + 1))\n",
        "    ax.axis('off')\n",
        "    \n",
        "    table = ax.table(cellText=comparison_df.values, colLabels=comparison_df.columns,\n",
        "                    cellLoc='left', loc='center', bbox=[0, 0, 1, 1])\n",
        "    table.auto_set_font_size(False)\n",
        "    table.set_fontsize(11)\n",
        "    table.scale(1, 2.5)\n",
        "    \n",
        "    # Color code best metrics\n",
        "    for i in range(1, len(comparison_data) + 1):\n",
        "        # Highlight best (lowest) loss\n",
        "        losses = [float(r['Final Loss']) for r in comparison_data]\n",
        "        best_loss_idx = np.argmin(losses)\n",
        "        if i == best_loss_idx + 1:\n",
        "            table[(i, 1)].set_facecolor('#90EE90')  # Light green\n",
        "            table[(i, 2)].set_facecolor('#90EE90')\n",
        "        \n",
        "        # Highlight best (lowest) PPL\n",
        "        ppls = [float(r['Final PPL']) for r in comparison_data]\n",
        "        best_ppl_idx = np.argmin(ppls)\n",
        "        if i == best_ppl_idx + 1:\n",
        "            table[(i, 3)].set_facecolor('#90EE90')\n",
        "    \n",
        "    ax.set_title('Run Comparison - Final Metrics', fontsize=14, fontweight='bold', pad=20)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    return fig, comparison_df\n",
        "\n",
        "# Create comparison table\n",
        "fig, comparison_df = create_comparison_table(runs_data)\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n=== Comparison Summary ===\")\n",
        "print(comparison_df.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Perplexity Comparison with Confidence Intervals\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_perplexity_comparison(runs_data, tokens_or_steps='tokens', show_confidence=True):\n",
        "    \"\"\"\n",
        "    Plot perplexity comparison with optional confidence intervals.\n",
        "    \"\"\"\n",
        "    fig, ax = plt.subplots(figsize=(14, 8))\n",
        "    \n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, len(runs_data)))\n",
        "    \n",
        "    # Calculate perplexity for each run\n",
        "    all_ppls = []\n",
        "    all_x = []\n",
        "    all_names = []\n",
        "    \n",
        "    for i, run_data in enumerate(runs_data):\n",
        "        run_name = run_data['run_name'].iloc[0]\n",
        "        x = run_data['tokens_seen'] if tokens_or_steps == 'tokens' else run_data['step']\n",
        "        ppl = np.exp(run_data['loss'])\n",
        "        \n",
        "        all_ppls.append(ppl.values)\n",
        "        all_x.append(x.values)\n",
        "        all_names.append(run_name)\n",
        "        \n",
        "        ax.plot(x, ppl, label=run_name, color=colors[i], linewidth=2.5, alpha=0.8)\n",
        "    \n",
        "    # Add final PPL annotations\n",
        "    for i, (x, ppl, name) in enumerate(zip(all_x, all_ppls, all_names)):\n",
        "        final_ppl = ppl[-1]\n",
        "        final_x = x[-1]\n",
        "        ax.plot(final_x, final_ppl, 'o', color=colors[i], markersize=10, markeredgecolor='black', \n",
        "               markeredgewidth=1)\n",
        "        ax.annotate(f'{final_ppl:.1f}', xy=(final_x, final_ppl), \n",
        "                   xytext=(10, 10), textcoords='offset points', fontsize=9,\n",
        "                   bbox=dict(boxstyle='round,pad=0.3', facecolor=colors[i], alpha=0.3))\n",
        "    \n",
        "    x_label = 'Tokens Seen' if tokens_or_steps == 'tokens' else 'Training Steps'\n",
        "    ax.set_xlabel(x_label, fontsize=12)\n",
        "    ax.set_ylabel('Perplexity', fontsize=12)\n",
        "    ax.set_title('Perplexity Comparison Across Runs', fontsize=14, fontweight='bold')\n",
        "    ax.legend(fontsize=11, loc='upper right')\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    ax.set_yscale('log')\n",
        "    \n",
        "    if tokens_or_steps == 'tokens':\n",
        "        ax.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e6:.1f}M'))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# Plot perplexity comparison\n",
        "fig = plot_perplexity_comparison(runs_data, tokens_or_steps='tokens')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Learning Rate Schedule Comparison\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_lr_schedule_comparison(runs_data, tokens_or_steps='tokens'):\n",
        "    \"\"\"\n",
        "    Compare learning rate schedules across runs.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, len(runs_data)))\n",
        "    \n",
        "    for i, run_data in enumerate(runs_data):\n",
        "        run_name = run_data['run_name'].iloc[0]\n",
        "        x = run_data['tokens_seen'] if tokens_or_steps == 'tokens' else run_data['step']\n",
        "        \n",
        "        # Plot 1: Linear scale\n",
        "        axes[0].plot(x, run_data['lr'], label=run_name, color=colors[i], linewidth=2, alpha=0.8)\n",
        "        \n",
        "        # Plot 2: Log scale\n",
        "        axes[1].semilogy(x, run_data['lr'], label=run_name, color=colors[i], linewidth=2, alpha=0.8)\n",
        "    \n",
        "    x_label = 'Tokens Seen' if tokens_or_steps == 'tokens' else 'Training Steps'\n",
        "    \n",
        "    axes[0].set_xlabel(x_label, fontsize=12)\n",
        "    axes[0].set_ylabel('Learning Rate', fontsize=12)\n",
        "    axes[0].set_title('Learning Rate Schedule Comparison (Linear Scale)', fontsize=14, fontweight='bold')\n",
        "    axes[0].legend(fontsize=10)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    if tokens_or_steps == 'tokens':\n",
        "        axes[0].xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e6:.1f}M'))\n",
        "    \n",
        "    axes[1].set_xlabel(x_label, fontsize=12)\n",
        "    axes[1].set_ylabel('Learning Rate (Log Scale)', fontsize=12)\n",
        "    axes[1].set_title('Learning Rate Schedule Comparison (Log Scale)', fontsize=14, fontweight='bold')\n",
        "    axes[1].legend(fontsize=10)\n",
        "    axes[1].grid(True, alpha=0.3, which='both')\n",
        "    if tokens_or_steps == 'tokens':\n",
        "        axes[1].xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e6:.1f}M'))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# Plot LR schedule comparison\n",
        "fig = plot_lr_schedule_comparison(runs_data, tokens_or_steps='tokens')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Convergence Speed Analysis\n",
        "\n",
        "Compare how quickly different runs converge to their final performance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_convergence_analysis(runs_data, target_loss=5.0, tokens_or_steps='tokens'):\n",
        "    \"\"\"\n",
        "    Analyze convergence speed: when each run reaches a target loss.\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "    \n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, len(runs_data)))\n",
        "    \n",
        "    convergence_tokens = []\n",
        "    convergence_steps = []\n",
        "    run_names = []\n",
        "    \n",
        "    for i, run_data in enumerate(runs_data):\n",
        "        run_name = run_data['run_name'].iloc[0]\n",
        "        x = run_data['tokens_seen'] if tokens_or_steps == 'tokens' else run_data['step']\n",
        "        losses = run_data['loss'].values\n",
        "        \n",
        "        # Find when loss drops below target\n",
        "        below_target = np.where(losses <= target_loss)[0]\n",
        "        if len(below_target) > 0:\n",
        "            conv_idx = below_target[0]\n",
        "            conv_tokens = x.iloc[conv_idx] if hasattr(x, 'iloc') else x[conv_idx]\n",
        "            conv_step = run_data['step'].iloc[conv_idx] if 'step' in run_data.columns else conv_idx\n",
        "        else:\n",
        "            conv_tokens = None\n",
        "            conv_step = None\n",
        "        \n",
        "        convergence_tokens.append(conv_tokens)\n",
        "        convergence_steps.append(conv_step)\n",
        "        run_names.append(run_name)\n",
        "        \n",
        "        # Plot 1: Loss curves with target line\n",
        "        axes[0].plot(x, losses, label=run_name, color=colors[i], linewidth=2, alpha=0.8)\n",
        "        if conv_tokens is not None:\n",
        "            axes[0].plot(conv_tokens, target_loss, 'o', color=colors[i], markersize=10, \n",
        "                        markeredgecolor='black', markeredgewidth=1)\n",
        "            axes[0].annotate(f'{conv_tokens/1e6:.1f}M', xy=(conv_tokens, target_loss),\n",
        "                           xytext=(10, 10), textcoords='offset points', fontsize=9)\n",
        "    \n",
        "    # Add target line\n",
        "    axes[0].axhline(y=target_loss, color='red', linestyle='--', linewidth=2, \n",
        "                   label=f'Target Loss: {target_loss}', alpha=0.7)\n",
        "    \n",
        "    x_label = 'Tokens Seen' if tokens_or_steps == 'tokens' else 'Training Steps'\n",
        "    axes[0].set_xlabel(x_label, fontsize=12)\n",
        "    axes[0].set_ylabel('Loss', fontsize=12)\n",
        "    axes[0].set_title(f'Convergence to Target Loss = {target_loss}', fontsize=14, fontweight='bold')\n",
        "    axes[0].legend(fontsize=10)\n",
        "    axes[0].grid(True, alpha=0.3)\n",
        "    if tokens_or_steps == 'tokens':\n",
        "        axes[0].xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e6:.1f}M'))\n",
        "    \n",
        "    # Plot 2: Bar chart of convergence tokens\n",
        "    valid_convs = [(name, tok) for name, tok in zip(run_names, convergence_tokens) if tok is not None]\n",
        "    if valid_convs:\n",
        "        names, tokens = zip(*valid_convs)\n",
        "        bars = axes[1].barh(names, [t/1e6 for t in tokens], color=colors[:len(names)], alpha=0.7)\n",
        "        axes[1].set_xlabel('Tokens to Converge (Millions)', fontsize=12)\n",
        "        axes[1].set_title('Convergence Speed Comparison', fontsize=14, fontweight='bold')\n",
        "        axes[1].grid(True, alpha=0.3, axis='x')\n",
        "        \n",
        "        # Add value labels on bars\n",
        "        for i, (bar, tok) in enumerate(zip(bars, tokens)):\n",
        "            axes[1].text(tok/1e6, i, f' {tok/1e6:.1f}M', va='center', fontsize=10)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# Plot convergence analysis\n",
        "fig = plot_convergence_analysis(runs_data, target_loss=5.0, tokens_or_steps='tokens')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Side-by-Side Dashboard Comparison\n",
        "\n",
        "Create a comprehensive side-by-side comparison of all runs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_comparison_dashboard(runs_data, tokens_or_steps='tokens'):\n",
        "    \"\"\"\n",
        "    Create a comprehensive comparison dashboard for all runs.\n",
        "    \"\"\"\n",
        "    n_runs = len(runs_data)\n",
        "    fig = plt.figure(figsize=(20, 5 * n_runs))\n",
        "    gs = fig.add_gridspec(n_runs, 4, hspace=0.4, wspace=0.3)\n",
        "    \n",
        "    colors = plt.cm.tab10(np.linspace(0, 1, n_runs))\n",
        "    \n",
        "    for i, run_data in enumerate(runs_data):\n",
        "        run_name = run_data['run_name'].iloc[0]\n",
        "        x = run_data['tokens_seen'] if tokens_or_steps == 'tokens' else run_data['step']\n",
        "        x_label = 'Tokens Seen (M)' if tokens_or_steps == 'tokens' else 'Steps'\n",
        "        \n",
        "        # Loss curve\n",
        "        ax1 = fig.add_subplot(gs[i, 0])\n",
        "        ax1.plot(x, run_data['loss'], color=colors[i], linewidth=2)\n",
        "        ax1.set_xlabel(x_label, fontsize=10)\n",
        "        ax1.set_ylabel('Loss', fontsize=10)\n",
        "        ax1.set_title(f'{run_name}\\nLoss', fontsize=11, fontweight='bold')\n",
        "        ax1.grid(True, alpha=0.3)\n",
        "        if tokens_or_steps == 'tokens':\n",
        "            ax1.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e6:.1f}'))\n",
        "        \n",
        "        # Perplexity\n",
        "        ax2 = fig.add_subplot(gs[i, 1])\n",
        "        ppl = np.exp(run_data['loss'])\n",
        "        ax2.semilogy(x, ppl, color=colors[i], linewidth=2)\n",
        "        ax2.set_xlabel(x_label, fontsize=10)\n",
        "        ax2.set_ylabel('PPL', fontsize=10)\n",
        "        ax2.set_title(f'{run_name}\\nPerplexity', fontsize=11, fontweight='bold')\n",
        "        ax2.grid(True, alpha=0.3, which='both')\n",
        "        if tokens_or_steps == 'tokens':\n",
        "            ax2.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e6:.1f}'))\n",
        "        \n",
        "        # Learning rate\n",
        "        ax3 = fig.add_subplot(gs[i, 2])\n",
        "        ax3.semilogy(x, run_data['lr'], color=colors[i], linewidth=2)\n",
        "        ax3.set_xlabel(x_label, fontsize=10)\n",
        "        ax3.set_ylabel('LR', fontsize=10)\n",
        "        ax3.set_title(f'{run_name}\\nLearning Rate', fontsize=11, fontweight='bold')\n",
        "        ax3.grid(True, alpha=0.3, which='both')\n",
        "        if tokens_or_steps == 'tokens':\n",
        "            ax3.xaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'{x/1e6:.1f}'))\n",
        "        \n",
        "        # Metrics summary\n",
        "        ax4 = fig.add_subplot(gs[i, 3])\n",
        "        ax4.axis('off')\n",
        "        \n",
        "        final_loss = run_data['loss'].iloc[-1]\n",
        "        final_ppl = np.exp(final_loss)\n",
        "        peak_lr = run_data['lr'].max()\n",
        "        final_lr = run_data['lr'].iloc[-1]\n",
        "        \n",
        "        summary_text = (\n",
        "            f\"Final Loss: {final_loss:.4f}\\n\"\n",
        "            f\"Final PPL: {final_ppl:.2f}\\n\"\n",
        "            f\"Peak LR: {peak_lr:.2e}\\n\"\n",
        "            f\"Final LR: {final_lr:.2e}\"\n",
        "        )\n",
        "        \n",
        "        if 'tokens_per_sec' in run_data.columns:\n",
        "            avg_speed = run_data['tokens_per_sec'].mean()\n",
        "            summary_text += f\"\\nAvg Speed: {avg_speed:,.0f} tok/s\"\n",
        "        \n",
        "        ax4.text(0.1, 0.5, summary_text, fontsize=10, verticalalignment='center',\n",
        "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.3))\n",
        "        ax4.set_title(f'{run_name}\\nSummary', fontsize=11, fontweight='bold')\n",
        "    \n",
        "    plt.suptitle('Multi-Run Comparison Dashboard', fontsize=16, fontweight='bold', y=0.995)\n",
        "    return fig\n",
        "\n",
        "# Create comparison dashboard\n",
        "fig = create_comparison_dashboard(runs_data, tokens_or_steps='tokens')\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
