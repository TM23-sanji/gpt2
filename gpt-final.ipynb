{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-02T23:16:36.123203Z",
     "iopub.status.busy": "2025-11-02T23:16:36.122989Z",
     "iopub.status.idle": "2025-11-02T23:16:36.612682Z",
     "shell.execute_reply": "2025-11-02T23:16:36.611963Z",
     "shell.execute_reply.started": "2025-11-02T23:16:36.123182Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF token loaded—rate limit bypassed!\n"
     ]
    }
   ],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "wandb_token = user_secrets.get_secret(\"wandb\")\n",
    "\n",
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = hf_token\n",
    "os.environ[\"WANDB_API_KEY\"] = wandb_token\n",
    "print(\"HF token loaded—rate limit bypassed!\")  # Optional confirm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T23:16:39.230290Z",
     "iopub.status.busy": "2025-11-02T23:16:39.229631Z",
     "iopub.status.idle": "2025-11-02T23:16:40.797710Z",
     "shell.execute_reply": "2025-11-02T23:16:40.796705Z",
     "shell.execute_reply.started": "2025-11-02T23:16:39.230258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !uv pip install -q 'datatrove[io, processing, s3, cli, ray]' tiktoken wandb transformers\n",
    "!uv pip install -q wandb transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T23:17:08.063179Z",
     "iopub.status.busy": "2025-11-02T23:17:08.062509Z",
     "iopub.status.idle": "2025-11-02T23:17:34.537265Z",
     "shell.execute_reply": "2025-11-02T23:17:34.536648Z",
     "shell.execute_reply.started": "2025-11-02T23:17:08.063146Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2225: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "928ba58dfbbc4f149180aa97ccdd8766",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3bf39e718740269693af56488498cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e0e715e98943f9b26f10739a52f6ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd54516fae374086a0c50d10dc53f11a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c20e61e0d6f45b6ba6d598e3a8d8066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtusharmishra802\u001b[0m (\u001b[33mtusharmishra802-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, random, os, math, time, wandb\n",
    "import numpy as np\n",
    "# import tiktoken\n",
    "# from datatrove.pipeline.readers import ParquetReader\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "# from itertools import cycle\n",
    "from torch.utils.data import DataLoader, IterableDataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F  # For scaled_dot_product_attention\n",
    "import torch.optim as optim\n",
    "from torch.amp import autocast, GradScaler\n",
    "import matplotlib.pyplot as plt\n",
    "# from torch.profiler import profile, record_function, ProfilerActivity\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "wandb.login(key=wandb_token) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T23:17:43.531895Z",
     "iopub.status.busy": "2025-11-02T23:17:43.531369Z",
     "iopub.status.idle": "2025-11-02T23:17:43.541083Z",
     "shell.execute_reply": "2025-11-02T23:17:43.540422Z",
     "shell.execute_reply.started": "2025-11-02T23:17:43.531872Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T23:19:15.858962Z",
     "iopub.status.busy": "2025-11-02T23:19:15.858229Z",
     "iopub.status.idle": "2025-11-02T23:19:15.862647Z",
     "shell.execute_reply": "2025-11-02T23:19:15.861874Z",
     "shell.execute_reply.started": "2025-11-02T23:19:15.858935Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "GPT_CONFIG_124M = {\n",
    "    \"vocab_size\": 50257,     # Vocabulary size\n",
    "    \"context_length\": 256,  # Context length - not 1024\n",
    "    \"emb_dim\": 768,          # Embedding dimension\n",
    "    \"n_heads\": 12,           # Number of attention heads\n",
    "    \"n_layers\": 12,          # Number of layers\n",
    "    \"drop_rate\": 0.1,        # Dropout rate\n",
    "    \"qkv_bias\": False        # Query-Key-Value bias\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T23:20:34.540278Z",
     "iopub.status.busy": "2025-11-02T23:20:34.539981Z",
     "iopub.status.idle": "2025-11-02T23:20:34.605189Z",
     "shell.execute_reply": "2025-11-02T23:20:34.604407Z",
     "shell.execute_reply.started": "2025-11-02T23:20:34.540258Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    \"seed\": 42,\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "\n",
    "    # Model\n",
    "    \"vocab_size\": 50257,          # GPT-2 vocab\n",
    "    \"emb_dim\": 768,\n",
    "    \"context_length\": 256,        # keep small for speed\n",
    "    \"n_heads\": 12,\n",
    "    \"n_layers\": 12,\n",
    "    \"drop_rate\": 0.1,\n",
    "    \"qkv_bias\": False,\n",
    "\n",
    "    # Data\n",
    "    \"max_tokens\": 500_000,        # STOP after this many tokens\n",
    "    \"warmup_tokens\": 10_000,      # linear warm-up\n",
    "    \"batch_size\": 32,\n",
    "    \"shuffle_buffer\": 5_000,\n",
    "\n",
    "    # Optimiser\n",
    "    \"optimizer\": \"adamw\",\n",
    "    \"lr\": 3e-4,\n",
    "    \"final_lr\": 3e-5,\n",
    "    \"weight_decay\": 0.1,\n",
    "    \"betas\": (0.9, 0.95),\n",
    "\n",
    "    # Misc\n",
    "    \"log_interval\": 20,           # steps\n",
    "    \"wandb_project\": \"gpt-fineweb-demo\",\n",
    "    \"wandb_run_name\": None,       # auto-generated\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T23:20:57.475612Z",
     "iopub.status.busy": "2025-11-02T23:20:57.475338Z",
     "iopub.status.idle": "2025-11-02T23:21:05.062149Z",
     "shell.execute_reply": "2025-11-02T23:21:05.061389Z",
     "shell.execute_reply.started": "2025-11-02T23:20:57.475591Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20251102_232057-vfzpxlgf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/tusharmishra802-/gpt-fineweb-demo/runs/vfzpxlgf' target=\"_blank\">fragrant-sea-1</a></strong> to <a href='https://wandb.ai/tusharmishra802-/gpt-fineweb-demo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/tusharmishra802-/gpt-fineweb-demo' target=\"_blank\">https://wandb.ai/tusharmishra802-/gpt-fineweb-demo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/tusharmishra802-/gpt-fineweb-demo/runs/vfzpxlgf' target=\"_blank\">https://wandb.ai/tusharmishra802-/gpt-fineweb-demo/runs/vfzpxlgf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/tusharmishra802-/gpt-fineweb-demo/runs/vfzpxlgf?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7d00adf27f50>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(CFG[\"seed\"])\n",
    "if CFG[\"device\"] == \"cuda\":\n",
    "    torch.cuda.manual_seed_all(CFG[\"seed\"])\n",
    "\n",
    "# ------------------- 2. WANDB INIT -------------------\n",
    "wandb.init(\n",
    "    project=CFG[\"wandb_project\"],\n",
    "    name=CFG[\"wandb_run_name\"],\n",
    "    config=CFG,\n",
    "    mode=\"online\",   # set \"offline\" if you have no internet\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T23:21:18.230471Z",
     "iopub.status.busy": "2025-11-02T23:21:18.230174Z",
     "iopub.status.idle": "2025-11-02T23:21:18.238432Z",
     "shell.execute_reply": "2025-11-02T23:21:18.237879Z",
     "shell.execute_reply.started": "2025-11-02T23:21:18.230451Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x-mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return 0.5*x*(1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715*torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg['emb_dim'], 4*cfg['emb_dim']),\n",
    "            GELU(),\n",
    "            nn.Linear(4*cfg['emb_dim'], cfg['emb_dim'])\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TARGET_TOKENS = 100_000          # stop after ~100 k tokens\n",
    "CONTEXT_LEN   = 256\n",
    "STRIDE        = 128\n",
    "SHUFFLE_BUF   = 5_000            # small shuffle buffer (RAM-friendly)\n",
    "BATCH_SIZE    = 32               # DataLoader batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T23:23:01.897570Z",
     "iopub.status.busy": "2025-11-02T23:23:01.896639Z",
     "iopub.status.idle": "2025-11-02T23:23:17.058629Z",
     "shell.execute_reply": "2025-11-02T23:23:17.058062Z",
     "shell.execute_reply.started": "2025-11-02T23:23:01.897535Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "946cc7abc7604af4babb4fdce5c91990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/27468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6881eb3dea2437ebc11a4617f0643ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/27468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset(\"HuggingFaceFW/fineweb\", split=\"train\", streaming=True)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    texts = examples['text']\n",
    "    tokenized = tokenizer(texts, truncation=False, add_special_tokens=False)  # Batched for speed\n",
    "    tokenized['input_ids'] = [ids + [tokenizer.eos_token_id] for ids in tokenized['input_ids']]\n",
    "    return tokenized\n",
    "\n",
    "ds = ds.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['text'])  # Drops raw text, keeps input_ids\n",
    "ds = ds.shuffle(buffer_size=CFG['shuffle_buffer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T23:27:25.536637Z",
     "iopub.status.busy": "2025-11-02T23:27:25.535902Z",
     "iopub.status.idle": "2025-11-02T23:27:25.546282Z",
     "shell.execute_reply": "2025-11-02T23:27:25.545383Z",
     "shell.execute_reply.started": "2025-11-02T23:27:25.536609Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SlidingWindowDataset(IterableDataset):\n",
    "    def __init__(self, ds, tokenizer, context_len, stride, target_tokens):\n",
    "        self.ds = ds  # HF streaming iterable\n",
    "        self.tokenizer = tokenizer\n",
    "        self.context_len = context_len\n",
    "        self.stride = stride\n",
    "        self.max_tokens = target_tokens\n",
    "        self.pad_id = tokenizer.pad_token_id\n",
    "\n",
    "    def __iter__(self):\n",
    "        buffer = []\n",
    "        token_count = 0\n",
    "        for example in self.ds:  # Streams tokenized input_ids\n",
    "            toks = example['input_ids']\n",
    "            if not toks:\n",
    "                continue\n",
    "            buffer.extend(toks)\n",
    "\n",
    "            # Fixed buffer logic\n",
    "            while len(buffer) > self.context_len:\n",
    "                x = buffer[:self.context_len]\n",
    "                y = buffer[1:self.context_len + 1]\n",
    "                # Pad if needed (rare post-fix)\n",
    "                if len(y) < self.context_len:\n",
    "                    y += [self.pad_id] * (self.context_len - len(y))\n",
    "                yield {'input_ids': torch.tensor(x, dtype=torch.long),\n",
    "                       'labels': torch.tensor(y, dtype=torch.long)}  # Dict for HF Trainer\n",
    "                buffer = buffer[self.stride:]\n",
    "                token_count += self.context_len\n",
    "                if token_count >= self.max_tokens:\n",
    "                    return\n",
    "\n",
    "            # Cap buffer to prevent OOM\n",
    "            if len(buffer) > 2 * self.context_len:\n",
    "                buffer = buffer[-self.context_len:]\n",
    "\n",
    "        # Remnant with padding\n",
    "        if len(buffer) >= 128:  # Min threshold\n",
    "            x = buffer[:self.context_len]\n",
    "            y = buffer[1:min(self.context_len + 1, len(buffer) + 1)]\n",
    "            if len(y) < self.context_len:\n",
    "                y += [self.pad_id] * (self.context_len - len(y))\n",
    "            yield {'input_ids': torch.tensor(x, dtype=torch.long),\n",
    "                   'labels': torch.tensor(y, dtype=torch.long)}\n",
    "\n",
    "# Usage\n",
    "dataset = SlidingWindowDataset(ds, tokenizer, context_len=CFG['context_length'], stride=CFG[\"context_length\"] // 2, target_tokens=CFG['max_tokens'])\n",
    "dataloader = DataLoader(dataset, batch_size=32, num_workers=2, pin_memory=True, prefetch_factor=2, collate_fn=lambda b: {k: torch.stack([d[k] for d in b]) for k in b[0]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T23:27:36.204508Z",
     "iopub.status.busy": "2025-11-02T23:27:36.204025Z",
     "iopub.status.idle": "2025-11-02T23:27:36.216937Z",
     "shell.execute_reply": "2025-11-02T23:27:36.215719Z",
     "shell.execute_reply.started": "2025-11-02T23:27:36.204473Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch SDP kernels enabled (mem_efficient & not math for speed on T4/P100)!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n",
      "  self.gen = func(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=True, enable_math=False)\n",
    "        print(\"PyTorch SDP kernels enabled (mem_efficient & not math for speed on T4/P100)!\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not enable SDP kernels: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T23:27:44.683075Z",
     "iopub.status.busy": "2025-11-02T23:27:44.682787Z",
     "iopub.status.idle": "2025-11-02T23:27:44.692404Z",
     "shell.execute_reply": "2025-11-02T23:27:44.691731Z",
     "shell.execute_reply.started": "2025-11-02T23:27:44.683059Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads  # d_k = d_v = d_out / num_heads\n",
    "\n",
    "        # Linear projections for Q, K, V\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Output projection\n",
    "\n",
    "        self.dropout = dropout  # Store value for SDPA\n",
    "        self.scale = self.head_dim ** -0.5\n",
    "\n",
    "        # Optional: causal mask buffer (not needed with is_causal=True)\n",
    "        # self.register_buffer(\"mask\", torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
    "\n",
    "        print(f\"MHA: {num_heads} heads, head_dim={self.head_dim}, SDPA enabled\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        # (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        keys    = self.W_key(x)\n",
    "        values  = self.W_value(x)\n",
    "\n",
    "        # Split into heads: (b, num_tokens, num_heads, head_dim)\n",
    "        # Use .reshape() instead of .view() for safety\n",
    "        queries = queries.reshape(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        keys    = keys.reshape(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        values  = values.reshape(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_heads, num_tokens, head_dim)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        keys    = keys.transpose(1, 2)\n",
    "        values  = values.transpose(1, 2)\n",
    "\n",
    "        # Use SDPA with causal masking\n",
    "        # IMPORTANT: dropout_p only applies during training\n",
    "        attn_output = F.scaled_dot_product_attention(\n",
    "            query=queries,\n",
    "            key=keys,\n",
    "            value=values,\n",
    "            attn_mask=None,\n",
    "            dropout_p=self.dropout if self.training else 0.0,\n",
    "            is_causal=True  # This creates the causal mask automatically\n",
    "        )\n",
    "\n",
    "        # Merge heads: (b, num_tokens, num_heads, head_dim) → (b, num_tokens, d_out)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()  # (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = attn_output.reshape(b, num_tokens, self.d_out)\n",
    "\n",
    "        # Final linear projection + dropout\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "        context_vec = F.dropout(context_vec, p=self.dropout, training=self.training)\n",
    "\n",
    "        return context_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T23:27:48.875932Z",
     "iopub.status.busy": "2025-11-02T23:27:48.875623Z",
     "iopub.status.idle": "2025-11-02T23:27:48.883135Z",
     "shell.execute_reply": "2025-11-02T23:27:48.882513Z",
     "shell.execute_reply.started": "2025-11-02T23:27:48.875909Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.att = MultiHeadAttention(\n",
    "            d_in = cfg['emb_dim'],\n",
    "            d_out = cfg['emb_dim'],\n",
    "            context_length = cfg['context_length'],\n",
    "            num_heads = cfg['n_heads'],\n",
    "            dropout = cfg['drop_rate'],\n",
    "            qkv_bias = cfg['qkv_bias']        \n",
    "        )\n",
    "        self.ff = FeedForward(cfg)\n",
    "        self.norm1 = LayerNorm(cfg['emb_dim'])\n",
    "        self.norm2 = LayerNorm(cfg['emb_dim'])\n",
    "        self.drop_shortcut = nn.Dropout(cfg['drop_rate'])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        shortcut = x\n",
    "        x = self.norm1(x)\n",
    "        x = self.att(x)\n",
    "        x = shortcut + self.drop_shortcut(x)\n",
    "        \n",
    "        shortcut = x\n",
    "        x = self.norm2(x)\n",
    "        x = self.ff(x)\n",
    "        x = shortcut + self.drop_shortcut(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T23:27:51.814396Z",
     "iopub.status.busy": "2025-11-02T23:27:51.814120Z",
     "iopub.status.idle": "2025-11-02T23:27:51.823038Z",
     "shell.execute_reply": "2025-11-02T23:27:51.822284Z",
     "shell.execute_reply.started": "2025-11-02T23:27:51.814375Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GPTModel(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_emb = nn.Embedding(cfg['vocab_size'], cfg['emb_dim'])\n",
    "        self.pos_emb = nn.Embedding(cfg['context_length'], cfg['emb_dim'])\n",
    "        self.drop_emb = nn.Dropout(cfg['drop_rate'])\n",
    "        self.trf_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg['n_layers'])]\n",
    "        )\n",
    "        self.final_norm = LayerNorm(cfg['emb_dim'])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg['emb_dim'], cfg['vocab_size'], bias=False\n",
    "        )\n",
    "        self.out_head.weight = self.tok_emb.weight # weight tying\n",
    "        # === Weight Initialization (CRITICAL) ===\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        \n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_emb(in_idx)\n",
    "        pos_embeds = self.pos_emb(\n",
    "            torch.arange(seq_len, device=in_idx.device)\n",
    "        )\n",
    "        x = tok_embeds + pos_embeds\n",
    "        x = self.drop_emb(x)\n",
    "        x = self.trf_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T23:28:09.943565Z",
     "iopub.status.busy": "2025-11-02T23:28:09.943260Z",
     "iopub.status.idle": "2025-11-02T23:28:12.646640Z",
     "shell.execute_reply": "2025-11-02T23:28:12.645995Z",
     "shell.execute_reply.started": "2025-11-02T23:28:09.943540Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MHA: 12 heads, head_dim=64, SDPA enabled\n",
      "MHA: 12 heads, head_dim=64, SDPA enabled\n",
      "MHA: 12 heads, head_dim=64, SDPA enabled\n",
      "MHA: 12 heads, head_dim=64, SDPA enabled\n",
      "MHA: 12 heads, head_dim=64, SDPA enabled\n",
      "MHA: 12 heads, head_dim=64, SDPA enabled\n",
      "MHA: 12 heads, head_dim=64, SDPA enabled\n",
      "MHA: 12 heads, head_dim=64, SDPA enabled\n",
      "MHA: 12 heads, head_dim=64, SDPA enabled\n",
      "MHA: 12 heads, head_dim=64, SDPA enabled\n",
      "MHA: 12 heads, head_dim=64, SDPA enabled\n",
      "MHA: 12 heads, head_dim=64, SDPA enabled\n"
     ]
    }
   ],
   "source": [
    "model_cfg = {\n",
    "    \"vocab_size\": CFG[\"vocab_size\"],\n",
    "    \"context_length\": CFG[\"context_length\"],\n",
    "    \"emb_dim\": CFG[\"emb_dim\"],\n",
    "    \"n_heads\": CFG[\"n_heads\"],\n",
    "    \"n_layers\": CFG[\"n_layers\"],\n",
    "    \"drop_rate\": CFG[\"drop_rate\"],\n",
    "    \"qkv_bias\": CFG[\"qkv_bias\"],\n",
    "}\n",
    "model = GPTModel(model_cfg).to(CFG[\"device\"])\n",
    "model = model.half()                     # FP16 for P100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T23:28:37.164610Z",
     "iopub.status.busy": "2025-11-02T23:28:37.164322Z",
     "iopub.status.idle": "2025-11-02T23:28:37.174513Z",
     "shell.execute_reply": "2025-11-02T23:28:37.173756Z",
     "shell.execute_reply.started": "2025-11-02T23:28:37.164586Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Scheduler: Warmup 10,000 → Cosine decay to 3.0e-05\n"
     ]
    }
   ],
   "source": [
    "# ------------------- 7. OPTIMISER + SCALER -------------------\n",
    "optimizer = torch.optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CFG[\"lr\"],\n",
    "    betas=CFG[\"betas\"],\n",
    "    weight_decay=CFG[\"weight_decay\"],\n",
    "    fused=True,                          # works on P100\n",
    ")\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "# ------------------- 8. LR SCHEDULER -------------------\n",
    "total_train_tokens = CFG[\"max_tokens\"]\n",
    "warmup_tokens      = CFG[\"warmup_tokens\"]\n",
    "base_lr            = CFG[\"lr\"]\n",
    "final_lr           = CFG[\"final_lr\"]\n",
    "\n",
    "def lr_lambda(tokens_seen):\n",
    "    if tokens_seen <= warmup_tokens:\n",
    "        return tokens_seen / max(1, warmup_tokens)               # linear warm-up\n",
    "    progress = (tokens_seen - warmup_tokens) / max(1, total_train_tokens - warmup_tokens)\n",
    "    cosine = 0.5 * (1.0 + math.cos(math.pi * min(1.0, progress)))\n",
    "    return (final_lr / base_lr) + (1.0 - final_lr / base_lr) * cosine\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "print(f\"LR Scheduler: Warmup {warmup_tokens:,} → Cosine decay to {final_lr:.1e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T23:29:28.221820Z",
     "iopub.status.busy": "2025-11-02T23:29:28.221505Z",
     "iopub.status.idle": "2025-11-02T23:30:53.437048Z",
     "shell.execute_reply": "2025-11-02T23:30:53.436266Z",
     "shell.execute_reply.started": "2025-11-02T23:29:28.221779Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1363 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2335 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step    20 | Loss 10.7418 | LR 6.00e-07 | Tokens 163,840/500,000 | Speed 4,188 t/s\n",
      "Step    40 | Loss 10.0954 | LR 1.20e-06 | Tokens 327,680/500,000 | Speed 5,440 t/s\n",
      "Step    60 | Loss 9.8620 | LR 1.80e-06 | Tokens 491,520/500,000 | Speed 6,038 t/s\n",
      "\n",
      "Reached target token count → stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>gpu_mem_gb</td><td>▁▁▁</td></tr><tr><td>loss</td><td>█▃▁</td></tr><tr><td>lr</td><td>▁▅█</td></tr><tr><td>step</td><td>▁▅█</td></tr><tr><td>tokens_per_sec</td><td>▁▆█</td></tr><tr><td>tokens_seen</td><td>▁▅█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>gpu_mem_gb</td><td>12.67055</td></tr><tr><td>loss</td><td>9.86198</td></tr><tr><td>lr</td><td>0.0</td></tr><tr><td>step</td><td>60</td></tr><tr><td>tokens_per_sec</td><td>6038.45291</td></tr><tr><td>tokens_seen</td><td>491520</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fragrant-sea-1</strong> at: <a href='https://wandb.ai/tusharmishra802-/gpt-fineweb-demo/runs/vfzpxlgf' target=\"_blank\">https://wandb.ai/tusharmishra802-/gpt-fineweb-demo/runs/vfzpxlgf</a><br> View project at: <a href='https://wandb.ai/tusharmishra802-/gpt-fineweb-demo' target=\"_blank\">https://wandb.ai/tusharmishra802-/gpt-fineweb-demo</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251102_232057-vfzpxlgf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training finished!\n"
     ]
    }
   ],
   "source": [
    "# ------------------- 9. TRAINING LOOP -------------------\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=-100)\n",
    "\n",
    "tokens_seen = 0\n",
    "step = 0\n",
    "start_time = time.time()\n",
    "\n",
    "print(\"Starting training …\")\n",
    "for batch in dataloader:\n",
    "    step += 1\n",
    "    batch = {k: v.to(CFG[\"device\"], non_blocking=True) for k, v in batch.items()}\n",
    "\n",
    "    # ---- forward + loss -------------------------------------------------\n",
    "    with autocast(device_type=\"cuda\", dtype=torch.float16):\n",
    "        logits = model(batch[\"input_ids\"])\n",
    "        loss = criterion(\n",
    "            logits.view(-1, logits.size(-1)),\n",
    "            batch[\"labels\"].view(-1),\n",
    "        )\n",
    "\n",
    "    # ---- backward -------------------------------------------------------\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    # ---- LR step (per token, not per step) -------------------------------\n",
    "    tokens_seen += batch[\"input_ids\"].numel()\n",
    "    scheduler.step()                     # LambdaLR uses the *current* token count\n",
    "\n",
    "    # ---- logging ---------------------------------------------------------\n",
    "    if step % CFG[\"log_interval\"] == 0:\n",
    "        elapsed = time.time() - start_time\n",
    "        tokens_per_sec = tokens_seen / elapsed\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "\n",
    "        wandb.log({\n",
    "            \"step\": step,\n",
    "            \"loss\": loss.item(),\n",
    "            \"lr\": lr,\n",
    "            \"tokens_seen\": tokens_seen,\n",
    "            \"tokens_per_sec\": tokens_per_sec,\n",
    "            \"gpu_mem_gb\": torch.cuda.max_memory_allocated() / 1e9,\n",
    "        }, step=step)\n",
    "        print(\n",
    "            f\"Step {step:5d} | \"\n",
    "            f\"Loss {loss.item():.4f} | \"\n",
    "            f\"LR {lr:.2e} | \"\n",
    "            f\"Tokens {tokens_seen:,}/{CFG['max_tokens']:,} | \"\n",
    "            f\"Speed {tokens_per_sec:,.0f} t/s\"\n",
    "        )\n",
    "\n",
    "    # ---- early stop ------------------------------------------------------\n",
    "    if tokens_seen >= CFG[\"max_tokens\"]:\n",
    "        print(\"\\nReached target token count → stopping.\")\n",
    "        break\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "wandb.finish()\n",
    "print(\"Training finished!\")\n",
    "# --------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing purpose - speed check**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T22:04:38.853860Z",
     "iopub.status.busy": "2025-11-02T22:04:38.853641Z",
     "iopub.status.idle": "2025-11-02T22:05:02.274810Z",
     "shell.execute_reply": "2025-11-02T22:05:02.274177Z",
     "shell.execute_reply.started": "2025-11-02T22:04:38.853833Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!uv pip install -q datasets transformers torch\n",
    "\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-02T22:10:05.360330Z",
     "iopub.status.busy": "2025-11-02T22:10:05.359638Z",
     "iopub.status.idle": "2025-11-02T22:10:36.910496Z",
     "shell.execute_reply": "2025-11-02T22:10:36.909515Z",
     "shell.execute_reply.started": "2025-11-02T22:10:05.360300Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb7e1053d474bb59bba2bd3ec1290a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/27468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dd750e924cb4e89b80bb93d84fa6317",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/27468 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to stream up to 200,000 tokens …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4704 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1241 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RESULT ===\n",
      "Time elapsed : 16.92 s\n",
      "Tokens yielded: 200,000\n",
      "Speed        : 11,820 tokens / sec\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "#  test_fineweb_stream_speed.py\n",
    "# --------------------------------------------------------------\n",
    "import time\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "# ------------------- 1. Config -------------------\n",
    "TARGET_TOKENS = 200_000          # stop after ~100 k tokens\n",
    "CONTEXT_LEN   = 256\n",
    "STRIDE        = 128\n",
    "SHUFFLE_BUF   = 5_000            # small shuffle buffer (RAM-friendly)\n",
    "BATCH_SIZE    = 32               # DataLoader batch size (does NOT affect speed here)\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# ------------------- 2. Tokenizer -------------------\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# ------------------- 3. Tokenize function -------------------\n",
    "def tokenize_function(examples):\n",
    "    texts = examples[\"text\"]\n",
    "    tokenized = tokenizer(\n",
    "        texts,\n",
    "        truncation=False,\n",
    "        add_special_tokens=False,\n",
    "    )\n",
    "    # append EOS to every document\n",
    "    tokenized[\"input_ids\"] = [\n",
    "        ids + [tokenizer.eos_token_id] for ids in tokenized[\"input_ids\"]\n",
    "    ]\n",
    "    return tokenized\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# ------------------- 4. Load streaming dataset -------------------\n",
    "ds = load_dataset(\n",
    "    \"HuggingFaceFW/fineweb\",\n",
    "    split=\"train\",\n",
    "    streaming=True,\n",
    "    # trust_remote_code=True,\n",
    ")\n",
    "\n",
    "ds = ds.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    batch_size=1000,             # batched tokenization → faster\n",
    "    remove_columns=[\"text\"],\n",
    ")\n",
    "\n",
    "ds = ds.shuffle(buffer_size=SHUFFLE_BUF)   # optional but realistic\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# ------------------- 5. Sliding-window IterableDataset -------------------\n",
    "class SlidingWindowDataset(IterableDataset):\n",
    "    def __init__(self, hf_ds, tokenizer, context_len, stride, target_tokens):\n",
    "        self.hf_ds        = hf_ds\n",
    "        self.tokenizer    = tokenizer\n",
    "        self.context_len  = context_len\n",
    "        self.stride       = stride\n",
    "        self.target       = target_tokens\n",
    "        self.pad_id       = tokenizer.pad_token_id\n",
    "\n",
    "    def __iter__(self):\n",
    "        buffer       = []\n",
    "        yielded      = 0\n",
    "\n",
    "        for example in self.hf_ds:\n",
    "            toks = example[\"input_ids\"]\n",
    "            if not toks:\n",
    "                continue\n",
    "            buffer.extend(toks)\n",
    "\n",
    "            # ---- yield as many windows as possible ----\n",
    "            while len(buffer) > self.context_len:\n",
    "                x = buffer[: self.context_len]\n",
    "                y = buffer[1: self.context_len + 1]\n",
    "\n",
    "                # pad y if (very rarely) needed\n",
    "                if len(y) < self.context_len:\n",
    "                    y += [self.pad_id] * (self.context_len - len(y))\n",
    "\n",
    "                yield {\n",
    "                    \"input_ids\": torch.tensor(x, dtype=torch.long),\n",
    "                    \"labels\":    torch.tensor(y, dtype=torch.long),\n",
    "                }\n",
    "\n",
    "                buffer = buffer[self.stride:]\n",
    "                yielded += self.context_len\n",
    "                if yielded >= self.target:\n",
    "                    return\n",
    "\n",
    "            # ---- keep buffer bounded (prevents OOM) ----\n",
    "            if len(buffer) > 2 * self.context_len:\n",
    "                buffer = buffer[-self.context_len:]\n",
    "\n",
    "        # ---- final remnant (optional) ----\n",
    "        if len(buffer) >= 128:                     # min-threshold\n",
    "            x = buffer[: self.context_len]\n",
    "            y = buffer[1: self.context_len + 1]\n",
    "            if len(y) < self.context_len:\n",
    "                y += [self.pad_id] * (self.context_len - len(y))\n",
    "\n",
    "            yield {\n",
    "                \"input_ids\": torch.tensor(x, dtype=torch.long),\n",
    "                \"labels\":    torch.tensor(y, dtype=torch.long),\n",
    "            }\n",
    "\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# ------------------- 6. Build dataset + DataLoader -------------------\n",
    "dataset = SlidingWindowDataset(\n",
    "    hf_ds=ds,\n",
    "    tokenizer=tokenizer,\n",
    "    context_len=CONTEXT_LEN,\n",
    "    stride=STRIDE,\n",
    "    target_tokens=TARGET_TOKENS,\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    collate_fn=lambda batch: {\n",
    "        k: torch.stack([b[k] for b in batch]) for k in batch[0]\n",
    "    },\n",
    "    num_workers=2,\n",
    "    pin_memory=True,\n",
    "    prefetch_factor=2,\n",
    ")\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "# ------------------- 7. Benchmark loop -------------------\n",
    "start_time = time.time()\n",
    "total_yielded = 0\n",
    "\n",
    "print(f\"Starting to stream up to {TARGET_TOKENS:,} tokens …\")\n",
    "for i, batch in enumerate(dataloader):\n",
    "    # batch is already on CPU (pin_memory=True helps later .to('cuda'))\n",
    "    tokens_in_batch = batch[\"input_ids\"].numel()   # == batch_size * context_len\n",
    "    total_yielded += tokens_in_batch\n",
    "\n",
    "    if total_yielded >= TARGET_TOKENS:\n",
    "        # truncate last (partial) batch\n",
    "        excess = total_yielded - TARGET_TOKENS\n",
    "        if excess > 0:\n",
    "            total_yielded -= excess\n",
    "        break\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "speed = total_yielded / elapsed\n",
    "\n",
    "print(\"\\n=== RESULT ===\")\n",
    "print(f\"Time elapsed : {elapsed:.2f} s\")\n",
    "print(f\"Tokens yielded: {total_yielded:,}\")\n",
    "print(f\"Speed        : {speed:,.0f} tokens / sec\")\n",
    "# --------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
