{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HF_TOKEN\")\nwandb_token = user_secrets.get_secret(\"wandb\")\n\nimport os\nos.environ[\"HF_TOKEN\"] = hf_token\nos.environ[\"WANDB_API_KEY\"] = wandb_token\nprint(\"HF token loaded—rate limit bypassed!\")  # Optional confirm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T09:46:47.324750Z","iopub.execute_input":"2025-11-13T09:46:47.325483Z","iopub.status.idle":"2025-11-13T09:46:47.773245Z","shell.execute_reply.started":"2025-11-13T09:46:47.325454Z","shell.execute_reply":"2025-11-13T09:46:47.772367Z"}},"outputs":[{"name":"stdout","text":"HF token loaded—rate limit bypassed!\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!uv pip install -q wandb transformers","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T09:46:49.593227Z","iopub.execute_input":"2025-11-13T09:46:49.593737Z","iopub.status.idle":"2025-11-13T09:46:50.029530Z","shell.execute_reply.started":"2025-11-13T09:46:49.593712Z","shell.execute_reply":"2025-11-13T09:46:50.028555Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch, random, os, math, time, wandb\nimport numpy as np\n# import tiktoken\n# from datatrove.pipeline.readers import ParquetReader\nfrom transformers import AutoTokenizer\nfrom datasets import load_dataset\n# from itertools import cycle\nfrom torch.utils.data import DataLoader, IterableDataset\nimport torch.nn as nn\nimport torch.nn.functional as F  # For scaled_dot_product_attention\nimport torch.optim as optim\nfrom torch.amp import autocast, GradScaler\nimport matplotlib.pyplot as plt\n# from torch.profiler import profile, record_function, ProfilerActivity\n\nwandb.login(key=wandb_token) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T09:47:00.388969Z","iopub.execute_input":"2025-11-13T09:47:00.389755Z","iopub.status.idle":"2025-11-13T09:47:21.473545Z","shell.execute_reply.started":"2025-11-13T09:47:00.389723Z","shell.execute_reply":"2025-11-13T09:47:21.472747Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n  warnings.warn(\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtusharmishra802\u001b[0m (\u001b[33mtusharmishra802-\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('huggyllama/llama-30b')\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T09:47:21.474768Z","iopub.execute_input":"2025-11-13T09:47:21.475288Z","iopub.status.idle":"2025-11-13T09:47:27.802161Z","shell.execute_reply.started":"2025-11-13T09:47:21.475268Z","shell.execute_reply":"2025-11-13T09:47:27.801328Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/700 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15fdf84e6fbc4fd1a6a606134c44cc03"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f763c1b64c894c24bd6f1e0253810688"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1e1b802de174ccf8967bf9501b0f062"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc83ef38ac18410da52a55a87a93dd37"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def set_seed(seed=42):\n    random.seed(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T09:47:27.803390Z","iopub.execute_input":"2025-11-13T09:47:27.804179Z","iopub.status.idle":"2025-11-13T09:47:27.813921Z","shell.execute_reply.started":"2025-11-13T09:47:27.804150Z","shell.execute_reply":"2025-11-13T09:47:27.813198Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"CFG = {\n    \"seed\": 42,\n    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\",\n\n    # Model\n    \"vocab_size\": 32000,          # llama vocab\n    \"emb_dim\": 128,               # 4096 \n    \"context_length\": 8,          # 4096\n    \"n_heads\": 4,                 # 32\n    \"num_kv_heads\": 2,\n    \"n_layers\": 1,                # 32\n    \"drop_rate\": 0.1,\n    \"qkv_bias\": False,\n    'base': 10000,\n    \"intermediate_size\": int(8/3 * 4096),  # ~11008 for full\n\n    # Data\n    \"max_tokens\": 500_000,        # STOP after this many tokens\n    \"warmup_tokens\": 10_000,      # linear warm-up\n    \"batch_size\": 32,\n    \"shuffle_buffer\": 5_000,\n\n    # Optimiser\n    \"optimizer\": \"adamw\",\n    \"lr\": 3e-4,\n    \"final_lr\": 3e-5,\n    \"weight_decay\": 0.1,\n    \"betas\": (0.9, 0.95),\n\n    # SwiGLU\n    'beta' : 1,\n\n    # Misc\n    \"log_interval\": 20,           # steps\n    \"wandb_project\": \"llama-demo\",\n    \"wandb_run_name\": None,       # auto-generated\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T09:47:40.400857Z","iopub.execute_input":"2025-11-13T09:47:40.401733Z","iopub.status.idle":"2025-11-13T09:47:40.459786Z","shell.execute_reply.started":"2025-11-13T09:47:40.401699Z","shell.execute_reply":"2025-11-13T09:47:40.458867Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"torch.manual_seed(CFG[\"seed\"])\nif CFG[\"device\"] == \"cuda\":\n    torch.cuda.manual_seed_all(CFG[\"seed\"])\n\n# ------------------- 2. WANDB INIT -------------------\nwandb.init(\n    project=CFG[\"wandb_project\"],\n    name=CFG[\"wandb_run_name\"],\n    config=CFG,\n    mode=\"online\",   # set \"offline\" if you have no internet\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T09:47:48.372488Z","iopub.execute_input":"2025-11-13T09:47:48.372765Z","iopub.status.idle":"2025-11-13T09:47:55.667130Z","shell.execute_reply.started":"2025-11-13T09:47:48.372744Z","shell.execute_reply":"2025-11-13T09:47:55.666357Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.0"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20251113_094748-kbdmu5zu</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/tusharmishra802-/llama-demo/runs/kbdmu5zu' target=\"_blank\">atomic-spaceship-11</a></strong> to <a href='https://wandb.ai/tusharmishra802-/llama-demo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/tusharmishra802-/llama-demo' target=\"_blank\">https://wandb.ai/tusharmishra802-/llama-demo</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/tusharmishra802-/llama-demo/runs/kbdmu5zu' target=\"_blank\">https://wandb.ai/tusharmishra802-/llama-demo/runs/kbdmu5zu</a>"},"metadata":{}},{"execution_count":9,"output_type":"execute_result","data":{"text/html":"<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/tusharmishra802-/llama-demo/runs/kbdmu5zu?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>","text/plain":"<wandb.sdk.wandb_run.Run at 0x7c99f6ecff50>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"class RMSNorm(nn.Module):\n    def __init__(self, emb_dim, eps=1e-6):\n        super().__init__()\n        self.eps = eps\n        self.weight = nn.Parameter(torch.ones(emb_dim))\n\n    def forward(self, x):\n        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps)\n        x_norm = x / rms\n        return x_norm * self.weight\n\nclass SiLU(nn.Module):\n    def __init__(self):\n        super().__init__()\n\n    def forward(self, x):\n        return x * F.sigmoid(CFG[\"beta\"] * x)\n\nclass FeedForward(nn.Module):\n    def __init__(self, cfg):\n        super().__init__()\n        intermediate_size = cfg.get(\"intermediate_size\", int(8/3 * cfg[\"emb_dim\"]))\n        self.gate_proj = nn.Linear(cfg[\"emb_dim\"], intermediate_size, bias=False)\n        self.up_proj = nn.Linear(cfg[\"emb_dim\"], intermediate_size, bias=False)\n        self.down_proj = nn.Linear(intermediate_size, cfg[\"emb_dim\"], bias=False)\n        self.act_fn = SiLU()\n\n    def forward(self, x):\n        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T09:47:55.668296Z","iopub.execute_input":"2025-11-13T09:47:55.668580Z","iopub.status.idle":"2025-11-13T09:47:55.675907Z","shell.execute_reply.started":"2025-11-13T09:47:55.668557Z","shell.execute_reply":"2025-11-13T09:47:55.675157Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class RotaryEmbeddings(nn.Module):\n    def __init__(self, dim: int, max_position_embeddings: int = 2048, base: int = 10000, device=CFG[\"device\"]):\n        super().__init__()\n        self.dim = dim\n        self.max_position_embeddings = max_position_embeddings\n        self.base = base\n        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.float32, device=device) / self.dim))\n        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n        \n        t = torch.arange(self.max_position_embeddings, dtype=torch.float32, device=device)\n        freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)  # (T, D/2)\n        self.register_buffer(\"cos_cached\", freqs.cos(), persistent=False)\n        self.register_buffer(\"sin_cached\", freqs.sin(), persistent=False)\n\n    def forward(self, x, seq_len=None):\n        if seq_len is None:\n            seq_len = x.shape[-2]\n        cos = self.cos_cached[:seq_len, ...].unsqueeze(0).unsqueeze(0)  # (1,1,T,D//2)\n        sin = self.sin_cached[:seq_len, ...].unsqueeze(0).unsqueeze(0)\n        \n        x1 = x[..., : self.dim : 2]  # (..., D//2)\n        x2 = x[..., 1 : self.dim : 2]\n        \n        rotated_x1 = x1 * cos - x2 * sin\n        rotated_x2 = x1 * sin + x2 * cos\n        \n        return torch.cat((rotated_x1, rotated_x2), dim=-1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T09:48:02.280873Z","iopub.execute_input":"2025-11-13T09:48:02.281721Z","iopub.status.idle":"2025-11-13T09:48:02.292292Z","shell.execute_reply.started":"2025-11-13T09:48:02.281686Z","shell.execute_reply":"2025-11-13T09:48:02.291499Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"ds = load_dataset(\"HuggingFaceFW/fineweb\", split=\"train\", streaming=True)\n\ndef tokenize_function(examples):\n    texts = examples['text']\n    tokenized = tokenizer(texts, truncation=False, add_special_tokens=False)  # Batched for speed\n    tokenized['input_ids'] = [ids + [tokenizer.eos_token_id] for ids in tokenized['input_ids']]\n    return tokenized\n\nds = ds.map(tokenize_function, batched=True, batch_size=1000, remove_columns=['text'])  # Drops raw text, keeps input_ids\nds = ds.shuffle(buffer_size=CFG['shuffle_buffer'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T09:48:08.476644Z","iopub.execute_input":"2025-11-13T09:48:08.476930Z","iopub.status.idle":"2025-11-13T09:48:31.270288Z","shell.execute_reply.started":"2025-11-13T09:48:08.476907Z","shell.execute_reply":"2025-11-13T09:48:31.269470Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f7fb5eae32004bc49739ff8c14f15d25"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/27468 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"20105ed5de4d4f659aab5195eac6dd5a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Resolving data files:   0%|          | 0/27468 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c6adf11ebccb49a881c07f73f9288e70"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"class SlidingWindowDataset(IterableDataset):\n    def __init__(self, ds, tokenizer, context_len, stride, target_tokens):\n        self.ds = ds  # HF streaming iterable\n        self.tokenizer = tokenizer\n        self.context_len = context_len\n        self.stride = stride\n        self.max_tokens = target_tokens\n        self.pad_id = tokenizer.pad_token_id\n\n    def __iter__(self):\n        buffer = []\n        token_count = 0\n        for example in self.ds:  # Streams tokenized input_ids\n            toks = example['input_ids']\n            if not toks:\n                continue\n            buffer.extend(toks)\n\n            # Fixed buffer logic\n            while len(buffer) > self.context_len:\n                x = buffer[:self.context_len]\n                y = buffer[1:self.context_len + 1]\n                # Pad if needed (rare post-fix)\n                if len(y) < self.context_len:\n                    y += [self.pad_id] * (self.context_len - len(y))\n                yield {'input_ids': torch.tensor(x, dtype=torch.long),\n                       'labels': torch.tensor(y, dtype=torch.long)}  # Dict for HF Trainer\n                buffer = buffer[self.stride:]\n                token_count += self.context_len\n                if token_count >= self.max_tokens:\n                    return\n\n            # Cap buffer to prevent OOM\n            if len(buffer) > 2 * self.context_len:\n                buffer = buffer[-self.context_len:]\n\n        # Remnant with padding\n        if len(buffer) >= 128:  # Min threshold\n            x = buffer[:self.context_len]\n            y = buffer[1:min(self.context_len + 1, len(buffer) + 1)]\n            if len(y) < self.context_len:\n                y += [self.pad_id] * (self.context_len - len(y))\n            yield {'input_ids': torch.tensor(x, dtype=torch.long),\n                   'labels': torch.tensor(y, dtype=torch.long)}\n\n# Usage\ndataset = SlidingWindowDataset(ds, tokenizer, context_len=CFG['context_length'], stride=CFG[\"context_length\"] // 2, target_tokens=CFG['max_tokens'])\ndataloader = DataLoader(dataset, batch_size=32, num_workers=2, pin_memory=True, prefetch_factor=2, collate_fn=lambda b: {k: torch.stack([d[k] for d in b]) for k in b[0]})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T09:48:31.271387Z","iopub.execute_input":"2025-11-13T09:48:31.271748Z","iopub.status.idle":"2025-11-13T09:48:31.281343Z","shell.execute_reply.started":"2025-11-13T09:48:31.271729Z","shell.execute_reply":"2025-11-13T09:48:31.280663Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"if torch.cuda.is_available():\n    try:\n        torch.backends.cuda.sdp_kernel(enable_flash=False, enable_mem_efficient=True, enable_math=False)\n        print(\"PyTorch SDP kernels enabled (mem_efficient & not math for speed on T4/P100)!\")\n    except Exception as e:\n        print(f\"Could not enable SDP kernels: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T09:48:31.283938Z","iopub.execute_input":"2025-11-13T09:48:31.284158Z","iopub.status.idle":"2025-11-13T09:48:31.302834Z","shell.execute_reply.started":"2025-11-13T09:48:31.284143Z","shell.execute_reply":"2025-11-13T09:48:31.301927Z"}},"outputs":[{"name":"stdout","text":"PyTorch SDP kernels enabled (mem_efficient & not math for speed on T4/P100)!\n","output_type":"stream"},{"name":"stderr","text":"/usr/lib/python3.11/contextlib.py:105: FutureWarning: `torch.backends.cuda.sdp_kernel()` is deprecated. In the future, this context manager will be removed. Please see `torch.nn.attention.sdpa_kernel()` for the new context manager, with updated signature.\n  self.gen = func(*args, **kwds)\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_in, d_out, dropout, num_heads, qkv_bias=False, device=CFG[\"device\"]):\n        super().__init__()\n        assert d_out % num_heads == 0\n        self.d_out = d_out\n        self.num_heads = num_heads\n        self.head_dim = d_out // num_heads\n        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n        self.out_proj = nn.Linear(d_out, d_out, bias=False)\n        self.rope = RotaryEmbeddings(self.head_dim, device=device)\n        self.dropout = dropout\n        print(f\"MHA: {num_heads} heads, head_dim={self.head_dim}\")\n\n    def forward(self, x):\n        b, t, _ = x.shape\n        q = self.W_query(x).reshape(b, t, self.num_heads, self.head_dim).transpose(1, 2)\n        k = self.W_key(x).reshape(b, t, self.num_heads, self.head_dim).transpose(1, 2)\n        v = self.W_value(x).reshape(b, t, self.num_heads, self.head_dim).transpose(1, 2)\n        q = self.rope(q)\n        k = self.rope(k)\n        attn = F.scaled_dot_product_attention(q, k, v, dropout_p=self.dropout if self.training else 0.0, is_causal=True)\n        attn = attn.transpose(1, 2).reshape(b, t, self.d_out)\n        return self.out_proj(attn)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T09:48:45.126993Z","iopub.execute_input":"2025-11-13T09:48:45.127586Z","iopub.status.idle":"2025-11-13T09:48:45.135111Z","shell.execute_reply.started":"2025-11-13T09:48:45.127562Z","shell.execute_reply":"2025-11-13T09:48:45.134256Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"class GroupedQueryAttention(nn.Module):\n    def __init__(self, d_in, d_out, dropout, num_heads, num_kv_heads=None, qkv_bias=False, device=CFG[\"device\"]):\n        super().__init__()\n        assert d_out % num_heads == 0\n        self.d_out = d_out\n        self.num_heads = num_heads\n        self.head_dim = d_out // num_heads\n        self.num_kv_heads = num_kv_heads or num_heads\n        assert self.num_kv_heads <= self.num_heads, \"num_kv_heads must <= num_heads\"\n        assert d_out % self.num_kv_heads == 0 or self.num_kv_heads == num_heads, \"Inconsistent head dims for GQA\"\n        \n        # Projections\n        self.W_query = nn.Linear(d_in, self.num_heads * self.head_dim, bias=qkv_bias)\n        self.W_key = nn.Linear(d_in, self.num_kv_heads * self.head_dim, bias=qkv_bias)  # Renamed for clarity\n        self.W_value = nn.Linear(d_in, self.num_kv_heads * self.head_dim, bias=qkv_bias)\n        self.out_proj = nn.Linear(self.num_heads * self.head_dim, d_out, bias=False)\n        \n        # RoPE with device\n        self.rope = RotaryEmbeddings(self.head_dim, device=device)\n        self.dropout = dropout\n\n    def forward(self, x):\n        b, t, d_in = x.shape\n        \n        # Project Q/K/V\n        q = self.W_query(x).reshape(b, t, self.num_heads, self.head_dim).transpose(1, 2)\n        k = self.W_key(x).reshape(b, t, self.num_kv_heads, self.head_dim).transpose(1, 2)\n        v = self.W_value(x).reshape(b, t, self.num_kv_heads, self.head_dim).transpose(1, 2)\n\n        # GQA Repeat\n        if self.num_kv_heads != self.num_heads:\n            repeat_factor = self.num_heads // self.num_kv_heads\n            k = k.repeat_interleave(repeat_factor, dim=1)\n            v = v.repeat_interleave(repeat_factor, dim=1)\n\n        q = self.rope(q)\n        k = self.rope(k)\n\n        # SDPA (use full k/v seq_len for mask/attn)\n        attn_output = F.scaled_dot_product_attention(\n            q, k, v, dropout_p=self.dropout if self.training else 0.0, is_causal=True, attn_mask=None\n        )\n        \n        # Merge\n        attn_output = attn_output.transpose(1, 2).contiguous().reshape(b, t, self.d_out)\n        output = self.out_proj(attn_output)\n                \n        return output","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T09:48:51.864845Z","iopub.execute_input":"2025-11-13T09:48:51.865558Z","iopub.status.idle":"2025-11-13T09:48:51.874193Z","shell.execute_reply.started":"2025-11-13T09:48:51.865530Z","shell.execute_reply":"2025-11-13T09:48:51.873368Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class TransformerBlock(nn.Module):\n    def __init__(self, cfg, device=CFG[\"device\"]):\n        super().__init__()\n        self.att = GroupedQueryAttention(cfg[\"emb_dim\"], cfg[\"emb_dim\"], cfg[\"drop_rate\"], cfg[\"n_heads\"], cfg['num_kv_heads'], cfg[\"qkv_bias\"], device)\n        self.ff = FeedForward(cfg)\n        self.norm1 = RMSNorm(cfg[\"emb_dim\"])\n        self.norm2 = RMSNorm(cfg[\"emb_dim\"])\n\n    def forward(self, x):\n        x = x + self.att(self.norm1(x))\n        x = x + self.ff(self.norm2(x))\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T09:49:17.640609Z","iopub.execute_input":"2025-11-13T09:49:17.640883Z","iopub.status.idle":"2025-11-13T09:49:17.646929Z","shell.execute_reply.started":"2025-11-13T09:49:17.640862Z","shell.execute_reply":"2025-11-13T09:49:17.646139Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class LlamaModel(nn.Module):\n    def __init__(self, cfg, device=CFG[\"device\"]):\n        super().__init__()\n        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n        self.trf_blocks = nn.Sequential(*[TransformerBlock(cfg, device) for _ in range(cfg[\"n_layers\"])])\n        self.final_norm = RMSNorm(cfg[\"emb_dim\"])\n        self.out_head = nn.Linear(cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False)\n        self.out_head.weight = self.tok_emb.weight\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n        elif isinstance(module, nn.Embedding):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n\n    def forward(self, idx):\n        x = self.tok_emb(idx)\n        x = self.trf_blocks(x)\n        x = self.final_norm(x)\n        return self.out_head(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T09:49:27.117228Z","iopub.execute_input":"2025-11-13T09:49:27.117835Z","iopub.status.idle":"2025-11-13T09:49:27.124883Z","shell.execute_reply.started":"2025-11-13T09:49:27.117813Z","shell.execute_reply":"2025-11-13T09:49:27.124110Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"model_cfg = {\n    \"vocab_size\": CFG[\"vocab_size\"],\n    \"context_length\": CFG[\"context_length\"],\n    \"emb_dim\": CFG[\"emb_dim\"],\n    \"n_heads\": CFG[\"n_heads\"],\n    \"num_kv_heads\": CFG[\"num_kv_heads\"],\n    \"n_layers\": CFG[\"n_layers\"],\n    \"drop_rate\": CFG[\"drop_rate\"],\n    \"qkv_bias\": CFG[\"qkv_bias\"],\n}\nmodel = LlamaModel(model_cfg, device=CFG[\"device\"])  # Pass device here!\n\n# 2. Move to GPU + FP16\nmodel = model.to(CFG[\"device\"]).half()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T09:49:35.047698Z","iopub.execute_input":"2025-11-13T09:49:35.048308Z","iopub.status.idle":"2025-11-13T09:49:35.569128Z","shell.execute_reply.started":"2025-11-13T09:49:35.048283Z","shell.execute_reply":"2025-11-13T09:49:35.568516Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def count_params(model):\n    return sum(p.numel() for p in model.parameters())\n\nprint(f\"Total params: {count_params(model):,}\")\nprint(\"Data equal:\", torch.equal(model.out_head.weight, model.tok_emb.weight))\n\nmodel.tok_emb.weight.data[0, 0] = 999.0  # Modify embedding\nprint(\"Shared? out_head[0,0] after change:\", model.out_head.weight.data[0, 0])  # Should be 999.0\nprint(\"Total params after mod:\", count_params(model))  # Still ~124M—no extra","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T09:49:42.048061Z","iopub.execute_input":"2025-11-13T09:49:42.048623Z","iopub.status.idle":"2025-11-13T09:49:42.256384Z","shell.execute_reply.started":"2025-11-13T09:49:42.048602Z","shell.execute_reply":"2025-11-13T09:49:42.255720Z"}},"outputs":[{"name":"stdout","text":"Total params: 4,276,480\nData equal: True\nShared? out_head[0,0] after change: tensor(999., device='cuda:0', dtype=torch.float16)\nTotal params after mod: 4276480\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"# ------------------- 7. OPTIMISER + SCALER -------------------\noptimizer = torch.optim.AdamW(\n    model.parameters(),\n    lr=CFG[\"lr\"],\n    betas=CFG[\"betas\"],\n    weight_decay=CFG[\"weight_decay\"],\n    fused=True,                          # works on P100\n)\n\nscaler = GradScaler()\n\n# ------------------- 8. LR SCHEDULER -------------------\ntotal_train_tokens = CFG[\"max_tokens\"]\nwarmup_tokens      = CFG[\"warmup_tokens\"]\nbase_lr            = CFG[\"lr\"]\nfinal_lr           = CFG[\"final_lr\"]\n\ndef lr_lambda(tokens_seen):\n    if tokens_seen <= warmup_tokens:\n        return tokens_seen / max(1, warmup_tokens)               # linear warm-up\n    progress = (tokens_seen - warmup_tokens) / max(1, total_train_tokens - warmup_tokens)\n    cosine = 0.5 * (1.0 + math.cos(math.pi * min(1.0, progress)))\n    return (final_lr / base_lr) + (1.0 - final_lr / base_lr) * cosine\n\nscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n\nprint(f\"LR Scheduler: Warmup {warmup_tokens:,} → Cosine decay to {final_lr:.1e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T09:49:47.828666Z","iopub.execute_input":"2025-11-13T09:49:47.829368Z","iopub.status.idle":"2025-11-13T09:49:47.839017Z","shell.execute_reply.started":"2025-11-13T09:49:47.829335Z","shell.execute_reply":"2025-11-13T09:49:47.838247Z"}},"outputs":[{"name":"stdout","text":"LR Scheduler: Warmup 10,000 → Cosine decay to 3.0e-05\n","output_type":"stream"}],"execution_count":21},{"cell_type":"code","source":"def validate(model, loader):\n    model.eval()\n    total_loss = 0.0\n    total_tokens = 0\n    with torch.no_grad():\n        for batch in loader:\n            batch = {k: v.to(CFG[\"device\"], non_blocking=True) for k, v in batch.items()}\n            with autocast(device_type=CFG['device'], dtype=torch.bfloat16, enabled=True):\n                logits = model(batch[\"input_ids\"])\n                loss = criterion(\n                    logits.view(-1, logits.size(-1)),\n                    batch[\"labels\"].view(-1),\n                )\n            num_tokens = (batch[\"labels\"] != -100).sum().item()\n            total_loss += loss.item() * num_tokens\n            total_tokens += num_tokens\n    model.train()\n    return total_loss / total_tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T09:49:55.036324Z","iopub.execute_input":"2025-11-13T09:49:55.037022Z","iopub.status.idle":"2025-11-13T09:49:55.043137Z","shell.execute_reply.started":"2025-11-13T09:49:55.036999Z","shell.execute_reply":"2025-11-13T09:49:55.042471Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"# ------------------- 9. TRAINING LOOP -------------------\ncriterion = nn.CrossEntropyLoss(ignore_index=-100)\n\ntokens_seen = 0\nstep = 0\nstart_time = time.time()\n\nprint(\"Starting training …\")\nfor batch in dataloader:\n    step += 1\n    batch = {k: v.to(CFG[\"device\"], non_blocking=True) for k, v in batch.items()}\n\n    # ---- forward + loss -------------------------------------------------\n    with autocast(device_type=CFG[\"device\"], dtype=torch.float16):\n        logits = model(batch[\"input_ids\"])\n        loss = criterion(\n            logits.view(-1, logits.size(-1)),\n            batch[\"labels\"].view(-1),\n        )\n\n    # ---- backward -------------------------------------------------------\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n    optimizer.zero_grad(set_to_none=True)\n\n    # ---- LR step (per token, not per step) -------------------------------\n    tokens_seen += batch[\"input_ids\"].numel()\n    scheduler.step()                     # LambdaLR uses the *current* token count\n\n    # ---- logging ---------------------------------------------------------\n    if step % CFG[\"log_interval\"] == 0:\n        elapsed = time.time() - start_time\n        tokens_per_sec = tokens_seen / elapsed\n        lr = optimizer.param_groups[0][\"lr\"]\n\n        wandb.log({\n            \"step\": step,\n            \"loss\": loss.item(),\n            \"lr\": lr,\n            \"tokens_seen\": tokens_seen,\n            \"tokens_per_sec\": tokens_per_sec,\n            \"gpu_mem_gb\": torch.cuda.max_memory_allocated() / 1e9,\n        }, step=step)\n        print(\n            f\"Step {step:5d} | \"\n            f\"Loss {loss.item():.4f} | \"\n            f\"LR {lr:.2e} | \"\n            f\"Tokens {tokens_seen:,}/{CFG['max_tokens']:,} | \"\n            f\"Speed {tokens_per_sec:,.0f} t/s\"\n        )\n\n    # ---- early stop ------------------------------------------------------\n    if tokens_seen >= CFG[\"max_tokens\"]:\n        print(\"\\nReached target token count → stopping.\")\n        break\n\n# --------------------------------------------------------------\nwandb.finish()\nprint(\"Training finished!\")\n# --------------------------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T09:50:01.014105Z","iopub.execute_input":"2025-11-13T09:50:01.014412Z","iopub.status.idle":"2025-11-13T09:50:33.057596Z","shell.execute_reply.started":"2025-11-13T09:50:01.014391Z","shell.execute_reply":"2025-11-13T09:50:33.056725Z"}},"outputs":[{"name":"stdout","text":"Starting training …\n","output_type":"stream"},{"name":"stderr","text":"Token indices sequence length is longer than the specified maximum sequence length for this model (2092 > 2048). Running this sequence through the model will result in indexing errors\nToken indices sequence length is longer than the specified maximum sequence length for this model (3506 > 2048). Running this sequence through the model will result in indexing errors\n","output_type":"stream"},{"name":"stdout","text":"Step    20 | Loss 610.6035 | LR 6.00e-07 | Tokens 5,120/500,000 | Speed 324 t/s\nStep    40 | Loss 565.7599 | LR 1.20e-06 | Tokens 10,240/500,000 | Speed 641 t/s\nStep    60 | Loss 422.3762 | LR 1.80e-06 | Tokens 15,360/500,000 | Speed 950 t/s\nStep    80 | Loss 445.9661 | LR 2.40e-06 | Tokens 20,480/500,000 | Speed 1,251 t/s\nStep   100 | Loss 323.7099 | LR 3.00e-06 | Tokens 25,600/500,000 | Speed 1,550 t/s\nStep   120 | Loss 501.3358 | LR 3.60e-06 | Tokens 30,720/500,000 | Speed 1,845 t/s\nStep   140 | Loss 483.0356 | LR 4.20e-06 | Tokens 35,840/500,000 | Speed 2,136 t/s\nStep   160 | Loss 275.6973 | LR 4.80e-06 | Tokens 40,960/500,000 | Speed 2,421 t/s\nStep   180 | Loss 296.7721 | LR 5.40e-06 | Tokens 46,080/500,000 | Speed 2,701 t/s\nStep   200 | Loss 502.7380 | LR 6.00e-06 | Tokens 51,200/500,000 | Speed 2,974 t/s\nStep   220 | Loss 500.6068 | LR 6.60e-06 | Tokens 56,320/500,000 | Speed 3,243 t/s\nStep   240 | Loss 151.7938 | LR 7.20e-06 | Tokens 61,440/500,000 | Speed 3,510 t/s\nStep   260 | Loss 243.0830 | LR 7.80e-06 | Tokens 66,560/500,000 | Speed 3,773 t/s\nStep   280 | Loss 218.4943 | LR 8.40e-06 | Tokens 71,680/500,000 | Speed 4,031 t/s\nStep   300 | Loss 160.3073 | LR 9.00e-06 | Tokens 76,800/500,000 | Speed 4,286 t/s\nStep   320 | Loss 292.8426 | LR 9.60e-06 | Tokens 81,920/500,000 | Speed 4,535 t/s\nStep   340 | Loss 155.0575 | LR 1.02e-05 | Tokens 87,040/500,000 | Speed 4,781 t/s\nStep   360 | Loss 118.0578 | LR 1.08e-05 | Tokens 92,160/500,000 | Speed 5,006 t/s\nStep   380 | Loss 71.1630 | LR 1.14e-05 | Tokens 97,280/500,000 | Speed 5,240 t/s\nStep   400 | Loss 123.8921 | LR 1.20e-05 | Tokens 102,400/500,000 | Speed 5,476 t/s\nStep   420 | Loss 50.8722 | LR 1.26e-05 | Tokens 107,520/500,000 | Speed 5,698 t/s\nStep   440 | Loss 46.5584 | LR 1.32e-05 | Tokens 112,640/500,000 | Speed 5,922 t/s\nStep   460 | Loss 50.7087 | LR 1.38e-05 | Tokens 117,760/500,000 | Speed 6,144 t/s\nStep   480 | Loss 45.4216 | LR 1.44e-05 | Tokens 122,880/500,000 | Speed 6,359 t/s\nStep   500 | Loss 44.7659 | LR 1.50e-05 | Tokens 128,000/500,000 | Speed 6,578 t/s\nStep   520 | Loss 50.1081 | LR 1.56e-05 | Tokens 133,120/500,000 | Speed 6,793 t/s\nStep   540 | Loss 17.5381 | LR 1.62e-05 | Tokens 138,240/500,000 | Speed 7,005 t/s\nStep   560 | Loss 23.0331 | LR 1.68e-05 | Tokens 143,360/500,000 | Speed 7,214 t/s\nStep   580 | Loss 19.5208 | LR 1.74e-05 | Tokens 148,480/500,000 | Speed 7,419 t/s\nStep   600 | Loss 14.5000 | LR 1.80e-05 | Tokens 153,600/500,000 | Speed 7,617 t/s\nStep   620 | Loss 11.6957 | LR 1.86e-05 | Tokens 158,720/500,000 | Speed 7,817 t/s\nStep   640 | Loss 19.5701 | LR 1.92e-05 | Tokens 163,840/500,000 | Speed 8,009 t/s\nStep   660 | Loss 19.6864 | LR 1.98e-05 | Tokens 168,960/500,000 | Speed 8,197 t/s\nStep   680 | Loss 13.9153 | LR 2.04e-05 | Tokens 174,080/500,000 | Speed 8,379 t/s\nStep   700 | Loss 17.3592 | LR 2.10e-05 | Tokens 179,200/500,000 | Speed 8,561 t/s\nStep   720 | Loss 10.1272 | LR 2.16e-05 | Tokens 184,320/500,000 | Speed 8,741 t/s\nStep   740 | Loss 10.1002 | LR 2.22e-05 | Tokens 189,440/500,000 | Speed 8,925 t/s\nStep   760 | Loss 10.7011 | LR 2.28e-05 | Tokens 194,560/500,000 | Speed 9,109 t/s\nStep   780 | Loss 10.3175 | LR 2.34e-05 | Tokens 199,680/500,000 | Speed 9,286 t/s\nStep   800 | Loss 9.9552 | LR 2.40e-05 | Tokens 204,800/500,000 | Speed 9,460 t/s\nStep   820 | Loss 9.8138 | LR 2.46e-05 | Tokens 209,920/500,000 | Speed 9,622 t/s\nStep   840 | Loss 11.3691 | LR 2.52e-05 | Tokens 215,040/500,000 | Speed 9,786 t/s\nStep   860 | Loss 9.7920 | LR 2.58e-05 | Tokens 220,160/500,000 | Speed 9,955 t/s\nStep   880 | Loss 11.0237 | LR 2.64e-05 | Tokens 225,280/500,000 | Speed 10,125 t/s\nStep   900 | Loss 9.4828 | LR 2.70e-05 | Tokens 230,400/500,000 | Speed 10,293 t/s\nStep   920 | Loss 14.2370 | LR 2.76e-05 | Tokens 235,520/500,000 | Speed 10,454 t/s\nStep   940 | Loss 9.6618 | LR 2.82e-05 | Tokens 240,640/500,000 | Speed 10,616 t/s\nStep   960 | Loss 9.5457 | LR 2.88e-05 | Tokens 245,760/500,000 | Speed 10,772 t/s\nStep   980 | Loss 11.4330 | LR 2.94e-05 | Tokens 250,880/500,000 | Speed 10,925 t/s\nStep  1000 | Loss 10.8444 | LR 3.00e-05 | Tokens 256,000/500,000 | Speed 11,083 t/s\nStep  1020 | Loss 9.5632 | LR 3.06e-05 | Tokens 261,120/500,000 | Speed 11,218 t/s\nStep  1040 | Loss 9.4732 | LR 3.12e-05 | Tokens 266,240/500,000 | Speed 11,356 t/s\nStep  1060 | Loss 9.3991 | LR 3.18e-05 | Tokens 271,360/500,000 | Speed 11,502 t/s\nStep  1080 | Loss 9.3160 | LR 3.24e-05 | Tokens 276,480/500,000 | Speed 11,653 t/s\nStep  1100 | Loss 9.1953 | LR 3.30e-05 | Tokens 281,600/500,000 | Speed 11,799 t/s\nStep  1120 | Loss 9.0851 | LR 3.36e-05 | Tokens 286,720/500,000 | Speed 11,946 t/s\nStep  1140 | Loss 9.2298 | LR 3.42e-05 | Tokens 291,840/500,000 | Speed 12,080 t/s\nStep  1160 | Loss 9.0098 | LR 3.48e-05 | Tokens 296,960/500,000 | Speed 12,207 t/s\nStep  1180 | Loss 10.1852 | LR 3.54e-05 | Tokens 302,080/500,000 | Speed 12,349 t/s\nStep  1200 | Loss 8.9706 | LR 3.60e-05 | Tokens 307,200/500,000 | Speed 12,490 t/s\nStep  1220 | Loss 9.5315 | LR 3.66e-05 | Tokens 312,320/500,000 | Speed 12,624 t/s\nStep  1240 | Loss 9.3559 | LR 3.72e-05 | Tokens 317,440/500,000 | Speed 12,759 t/s\nStep  1260 | Loss 8.9338 | LR 3.78e-05 | Tokens 322,560/500,000 | Speed 12,898 t/s\nStep  1280 | Loss 9.4568 | LR 3.84e-05 | Tokens 327,680/500,000 | Speed 13,026 t/s\nStep  1300 | Loss 9.2211 | LR 3.90e-05 | Tokens 332,800/500,000 | Speed 13,147 t/s\nStep  1320 | Loss 9.3739 | LR 3.96e-05 | Tokens 337,920/500,000 | Speed 13,263 t/s\nStep  1340 | Loss 9.1570 | LR 4.02e-05 | Tokens 343,040/500,000 | Speed 13,384 t/s\nStep  1360 | Loss 9.4494 | LR 4.08e-05 | Tokens 348,160/500,000 | Speed 13,513 t/s\nStep  1380 | Loss 8.8649 | LR 4.14e-05 | Tokens 353,280/500,000 | Speed 13,638 t/s\nStep  1400 | Loss 9.3029 | LR 4.20e-05 | Tokens 358,400/500,000 | Speed 13,765 t/s\nStep  1420 | Loss 10.8429 | LR 4.26e-05 | Tokens 363,520/500,000 | Speed 13,890 t/s\nStep  1440 | Loss 9.0667 | LR 4.32e-05 | Tokens 368,640/500,000 | Speed 13,999 t/s\nStep  1460 | Loss 8.8774 | LR 4.38e-05 | Tokens 373,760/500,000 | Speed 14,107 t/s\nStep  1480 | Loss 8.6562 | LR 4.44e-05 | Tokens 378,880/500,000 | Speed 14,219 t/s\nStep  1500 | Loss 9.0944 | LR 4.50e-05 | Tokens 384,000/500,000 | Speed 14,337 t/s\nStep  1520 | Loss 9.0275 | LR 4.56e-05 | Tokens 389,120/500,000 | Speed 14,454 t/s\nStep  1540 | Loss 8.6673 | LR 4.62e-05 | Tokens 394,240/500,000 | Speed 14,565 t/s\nStep  1560 | Loss 8.6959 | LR 4.68e-05 | Tokens 399,360/500,000 | Speed 14,677 t/s\nStep  1580 | Loss 9.2668 | LR 4.74e-05 | Tokens 404,480/500,000 | Speed 14,797 t/s\nStep  1600 | Loss 8.5442 | LR 4.80e-05 | Tokens 409,600/500,000 | Speed 14,897 t/s\nStep  1620 | Loss 8.4220 | LR 4.86e-05 | Tokens 414,720/500,000 | Speed 14,997 t/s\nStep  1640 | Loss 8.5302 | LR 4.92e-05 | Tokens 419,840/500,000 | Speed 15,103 t/s\nStep  1660 | Loss 8.4461 | LR 4.98e-05 | Tokens 424,960/500,000 | Speed 15,209 t/s\nStep  1680 | Loss 8.9048 | LR 5.04e-05 | Tokens 430,080/500,000 | Speed 15,309 t/s\nStep  1700 | Loss 8.5492 | LR 5.10e-05 | Tokens 435,200/500,000 | Speed 15,419 t/s\nStep  1720 | Loss 8.5037 | LR 5.16e-05 | Tokens 440,320/500,000 | Speed 15,520 t/s\nStep  1740 | Loss 9.2802 | LR 5.22e-05 | Tokens 445,440/500,000 | Speed 15,622 t/s\nStep  1760 | Loss 10.8504 | LR 5.28e-05 | Tokens 450,560/500,000 | Speed 15,727 t/s\nStep  1780 | Loss 8.8151 | LR 5.34e-05 | Tokens 455,680/500,000 | Speed 15,825 t/s\nStep  1800 | Loss 8.4759 | LR 5.40e-05 | Tokens 460,800/500,000 | Speed 15,915 t/s\nStep  1820 | Loss 8.6480 | LR 5.46e-05 | Tokens 465,920/500,000 | Speed 16,003 t/s\nStep  1840 | Loss 8.1423 | LR 5.52e-05 | Tokens 471,040/500,000 | Speed 16,080 t/s\nStep  1860 | Loss 10.3518 | LR 5.58e-05 | Tokens 476,160/500,000 | Speed 16,176 t/s\nStep  1880 | Loss 10.4331 | LR 5.64e-05 | Tokens 481,280/500,000 | Speed 16,274 t/s\nStep  1900 | Loss 8.3962 | LR 5.70e-05 | Tokens 486,400/500,000 | Speed 16,374 t/s\nStep  1920 | Loss 9.4842 | LR 5.76e-05 | Tokens 491,520/500,000 | Speed 16,462 t/s\nStep  1940 | Loss 10.4626 | LR 5.82e-05 | Tokens 496,640/500,000 | Speed 16,549 t/s\n\nReached target token count → stopping.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>gpu_mem_gb</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>loss</td><td>███▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>step</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇██</td></tr><tr><td>tokens_per_sec</td><td>▁▁▁▁▂▂▂▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇█████</td></tr><tr><td>tokens_seen</td><td>▁▁▁▂▂▂▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇██</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>gpu_mem_gb</td><td>0.14571</td></tr><tr><td>loss</td><td>10.4626</td></tr><tr><td>lr</td><td>6e-05</td></tr><tr><td>step</td><td>1940</td></tr><tr><td>tokens_per_sec</td><td>16549.27979</td></tr><tr><td>tokens_seen</td><td>496640</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">atomic-spaceship-11</strong> at: <a href='https://wandb.ai/tusharmishra802-/llama-demo/runs/kbdmu5zu' target=\"_blank\">https://wandb.ai/tusharmishra802-/llama-demo/runs/kbdmu5zu</a><br> View project at: <a href='https://wandb.ai/tusharmishra802-/llama-demo' target=\"_blank\">https://wandb.ai/tusharmishra802-/llama-demo</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20251113_094748-kbdmu5zu/logs</code>"},"metadata":{}},{"name":"stdout","text":"Training finished!\n","output_type":"stream"}],"execution_count":23},{"cell_type":"code","source":"# Fixed generation function (top-k sampling)\ndef generate(model, tokenizer, prompt, max_new_tokens=50, top_k=50, temperature=0.8, pad_token_id=None):\n    if pad_token_id is None:\n        pad_token_id = tokenizer.eos_token_id\n    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(CFG[\"device\"])\n    \n    with torch.no_grad():\n        for _ in range(max_new_tokens):\n            with autocast(device_type=\"cuda\" if torch.cuda.is_available() else \"cpu\", dtype=torch.bfloat16, enabled=True):\n                logits = model(input_ids)[:, -1, :]  # Last position logits\n                logits = logits / temperature\n                # Top-k filtering\n                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n                probs = torch.softmax(logits, dim=-1)\n                mask = logits[0] < v[0].min()  # 1D boolean [vocab]\n                probs[0][mask] = 0  # Set low-prob to 0\n                next_token = torch.multinomial(probs, num_samples=1)\n                input_ids = torch.cat([input_ids, next_token], dim=-1)\n                if next_token.item() == pad_token_id:\n                    break  # Stop at EOS\n    return tokenizer.decode(input_ids[0], skip_special_tokens=True)\n\n# Test prompts (web/edu themed)\nprompts = [\n    \"Web hosting is essential for\",\n    \"Machine learning models train on datasets like\",\n    \"A good developer should know\",\n    \"FineWeb-Edu is a filtered version of\",\n    \"The future of AI in education involves\",\n    \"Why do you think people follow me\"\n]\n\nprint(\"=== Generation Tests (Final Train Loss: 4.19 | PPL: {:.0f}) ===\".format(math.exp(4.19)))\nfor prompt in prompts:\n    with torch.no_grad():  # Per-prompt for safety\n        generated = generate(model, tokenizer, prompt, max_new_tokens=50, top_k=50, temperature=0.8)\n        continuation = generated[len(prompt):].strip()  # Continuation only\n        print(f\"\\nPrompt: {prompt}\")\n        print(f\"Output: {continuation}\")\n        print(\"-\" * 80)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-13T09:51:01.928725Z","iopub.execute_input":"2025-11-13T09:51:01.929479Z","iopub.status.idle":"2025-11-13T09:51:02.851710Z","shell.execute_reply.started":"2025-11-13T09:51:01.929449Z","shell.execute_reply":"2025-11-13T09:51:02.851104Z"}},"outputs":[{"name":"stdout","text":"=== Generation Tests (Final Train Loss: 4.19 | PPL: 66) ===\n\nPrompt: Web hosting is essential for\nOutput: 0, they- in is, it have will who of is that,' at , “ the\n it in when of not I the one't an but0. we. for you, they to of to to with\n that0\n--------------------------------------------------------------------------------\n\nPrompt: Machine learning models train on datasets like\nOutput: the of and have as. his of I to is be it\n to haveed of to1 we is,\n you of aed\n for who in as1 is of\n in in be and'’  to\nation be\n--------------------------------------------------------------------------------\n\nPrompt: A good developer should know\nOutput: 5 and the for the that,\n to' of it in1 we that your have I to\n to a for I bes them the one ofation when is them with is, his in1. their not15 it 0\n--------------------------------------------------------------------------------\n\nPrompt: FineWeb-Edu is a filtered version of\nOutput: . of it1 for can of in that one is  of so, of the to with we. to can their your I ands\ns in that ,'1 is can thats “,. be at-ations and to\n--------------------------------------------------------------------------------\n\nPrompt: The future of AI in education involves\nOutput: of to a.’ in have to to a your that andation we is when you a is, but for of is of it the of you ofation is you a be- an one,  their a an of be as ( in at\n--------------------------------------------------------------------------------\n\nPrompt: Why do you think people follow me\nOutput: ,, at it. is’ “ be of’ an5 to\n\n to and. of to0 and their can,' isation an their bet they and in at you at out s1 to “ to to,- (\n--------------------------------------------------------------------------------\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}